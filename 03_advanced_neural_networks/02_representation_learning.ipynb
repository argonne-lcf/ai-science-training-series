{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0442b2-193e-4cf0-a5c8-1e192b04f130",
   "metadata": {},
   "source": [
    "# Foundation Models\n",
    "\n",
    "# This notebook is not officially part of the course.\n",
    "\n",
    "But you are welcome to look through it anyways, you can send questions on slack, and we are happy to talk about it.  \n",
    "\n",
    "Author: Corey Adams\n",
    "\n",
    "The previous notebook trained a classifier network which did ok.  But what if we didn't have a lot of data?  In this notebook, we'll apply that model in a new way with _representation learning_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69d87a-de6c-4fba-a6e2-6de36b165b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "batch_size = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551b0f7-675c-4452-97f8-36f3a269c6de",
   "metadata": {},
   "source": [
    "Here's the Convolutional Neural Network Again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8068c4-c85b-46d0-af2c-c665dbb6a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Downsampler(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=2):\n",
    "        super(Downsampler, self).__init__()\n",
    "\n",
    "        self.norm = nn.InstanceNorm2d(in_channels)\n",
    "\n",
    "        self.downsample = nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            kernel_size = stride,\n",
    "            stride = stride,\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        return self.downsample(self.norm(inputs))\n",
    "        \n",
    "        \n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"This block of operations is loosely based on this paper:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvNextBlock, self).__init__()\n",
    "\n",
    "        # Depthwise, seperable convolution with a large number of output filters:\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=in_channels, \n",
    "                                     groups=in_channels,\n",
    "                                     kernel_size=[7,7],\n",
    "                                     padding='same' )\n",
    "\n",
    "        self.norm = nn.InstanceNorm2d(in_channels)\n",
    "\n",
    "        # Two more convolutions:\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=4*in_channels,\n",
    "                                     kernel_size=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=4*in_channels, \n",
    "                                     out_channels=in_channels,\n",
    "                                     kernel_size=1\n",
    "                                     )\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "\n",
    "        # The normalization layer:\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # The non-linear activation layer:\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # This makes it a residual network:\n",
    "        return x + inputs\n",
    "    \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n_initial_filters, n_stages, blocks_per_stage, n_outputs):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # This is a downsampling convolution that will produce patches of output.\n",
    "\n",
    "        # This is similar to what vision transformers do to tokenize the images.\n",
    "        self.stem = nn.Conv2d(in_channels=3,\n",
    "                                    out_channels=n_initial_filters,\n",
    "                                    kernel_size=1,\n",
    "                                    stride=1)\n",
    "        \n",
    "        self.norm1 = nn.InstanceNorm2d(n_initial_filters)\n",
    "\n",
    "        current_n_filters = n_initial_filters\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        for n_blocks in range(n_stages):\n",
    "            # Add a convnext block series:\n",
    "            for _ in range(blocks_per_stage):\n",
    "                self.layers.append(ConvNextBlock(in_channels=current_n_filters))\n",
    "            # Add a downsampling layer:\n",
    "            self.layers.append(Downsampler(in_channels=current_n_filters, out_channels=2*current_n_filters))\n",
    "            # Double the number of filters:\n",
    "            current_n_filters = 2*current_n_filters\n",
    "\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(current_n_filters),\n",
    "            nn.Linear(current_n_filters, n_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.stem(inputs)\n",
    "        # Apply a normalization after the initial patching:\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Apply the main chunk of the network:\n",
    "        x = self.layers(x)\n",
    "\n",
    "        # Normalize and readout:\n",
    "        x = nn.functional.avg_pool2d(x, x.shape[2:])\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbbcfa-843e-4bcc-ba12-00d18b9e55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representation_model(n_features, rank, size):\n",
    "\n",
    "    model = Classifier(32, 2, 2, n_features)\n",
    "\n",
    "\n",
    "    model.to(f\"cuda:{rank}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_representation_model(256, 0, 1)\n",
    "\n",
    "head = torch.nn.Sequential(\n",
    "    nn.Linear(256,128),\n",
    ")\n",
    "\n",
    "head.to(f\"cuda:0\")\n",
    "\n",
    "from torchinfo import summary\n",
    "    \n",
    "print(summary(model, input_size=(batch_size, 3, 32, 32)))\n",
    "print(summary(head, input_size=(batch_size, 256)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691caee3-bb58-451b-b3d7-fc394393b95f",
   "metadata": {},
   "source": [
    "This will download the data if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c5c32-2d59-47b4-9e69-249451f3f410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eeb11a6-e248-4223-bc06-ef995f658ec8",
   "metadata": {},
   "source": [
    "\n",
    "We're going to train this on Polaris nodes which have 4 A100s (But only using one node at a time).  So, the following helper functions will automatically distribute the code and model to use all 4 GPUs at once:\n",
    "\n",
    "(They are all from the [DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b9ac3-52cf-4ee4-8e36-cd7c4b29c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2ebd9-47f7-4534-b67a-7ca43b9bc98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(transforms, batch_size, rank, seed):\n",
    "    # Start up the data loader:\n",
    "    dev = torch.device(\n",
    "        f\"cuda:{rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    training_data = torchvision.datasets.CIFAR10(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms\n",
    "    )\n",
    "    \n",
    "    training_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))\n",
    "    \n",
    "    # The dataloader makes our dataset iterable \n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True, \n",
    "                                                   num_workers=8)\n",
    "    \n",
    "    val_dataloader = torch.utils.data.DataLoader(validation_data, \n",
    "                                                 batch_size=batch_size, \n",
    "                                                 shuffle=True, \n",
    "                                                 num_workers=8)\n",
    "    \n",
    "\n",
    "    def preprocess(x, y):\n",
    "        # CIFAR-10 is *color* images so 3 layers!\n",
    "        return x.view(-1, 3, 32, 32).to(dev), y.to(dev)\n",
    "    \n",
    "    \n",
    "    class WrappedDataLoader:\n",
    "        def __init__(self, dl, func):\n",
    "            self.dl = dl\n",
    "            self.func = func\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "    \n",
    "        def __iter__(self):\n",
    "            for b in self.dl:\n",
    "                yield (self.func(*b))\n",
    "\n",
    "\n",
    "    train_dataloader = WrappedDataLoader(train_dataloader, preprocess)\n",
    "    val_dataloader = WrappedDataLoader(val_dataloader, preprocess)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fcdae-fbe6-4e0b-b3d7-e8b771e9ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def demo_basic(rank, world_size, n_epochs):\n",
    "#     print(f\"Running basic DDP example on rank {rank}.\")\n",
    "#     setup(rank, world_size)\n",
    "\n",
    "    \n",
    "#     # create model and move it to GPU with id rank\n",
    "#     model = ToyModel().to(rank)\n",
    "#     ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "#     loss_fn = nn.MSELoss()\n",
    "#     optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = ddp_model(torch.randn(20, 10))\n",
    "#     labels = torch.randn(20, 5).to(rank)\n",
    "#     loss_fn(outputs, labels).backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     cleanup()\n",
    "\n",
    "\n",
    "# def run_demo(demo_fn, world_size):\n",
    "#     mp.spawn(demo_fn,\n",
    "#              args=(world_size,5),\n",
    "#              nprocs=world_size,\n",
    "#              join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544004df-e1f3-4ee3-b37e-0e3dce527da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11354136-05f8-4297-908c-fc35ff0a828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, os\n",
    "# from multiprocessing import Pool\n",
    "# from multiprocessing.reduction import ForkingPickler\n",
    "# from types import FunctionType\n",
    "# import cloudpickle\n",
    "\n",
    "# assert sys.version_info >= (3, 8), 'python3.8 or greater required to use reducer_override'\n",
    "\n",
    "# def reducer_override(obj):\n",
    "#     if type(obj) is FunctionType:\n",
    "#         return (cloudpickle.loads, (cloudpickle.dumps(obj),))\n",
    "#     else:\n",
    "#         return NotImplemented\n",
    "\n",
    "# # Monkeypatch our function reducer into the pickler for multiprocessing.\n",
    "# # Without this line, the main block will not work on windows or macOS.\n",
    "# # Alterntively, moving the defintionn of foo outside of the if statement\n",
    "# # would make the main block work on windows or macOS (when run from\n",
    "# # the command line).\n",
    "# ForkingPickler.reducer_override = staticmethod(reducer_override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9716224-0de3-4e52-85e8-7f6e26763c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d1b3a-f159-430d-81c8-05957ca3824f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda19fe1-3e37-45b6-8bb6-574bee4b5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This method is from the pytorch implementation of SimCLR:\n",
    "# https://github.com/sthalles/SimCLR/blob/master/data_aug/contrastive_learning_dataset.py\n",
    "\n",
    "def get_simclr_pipeline_transform(size, s=1):\n",
    "    \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\n",
    "    color_jitter = v2.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "    data_transforms = v2.Compose([v2.RandomResizedCrop(size=size, scale=[0.85,1.0]),\n",
    "                                          v2.RandomHorizontalFlip(),\n",
    "                                          v2.RandomApply([color_jitter], p=0.8),\n",
    "                                          v2.RandomGrayscale(p=0.2),\n",
    "                                          v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "                                          # v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                          # v2.ToTensor()\n",
    "                                        ])\n",
    "    return data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67638fcb-43c7-4c45-a8e6-284998ea6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms1 = get_simclr_pipeline_transform((32,32))\n",
    "transforms2 = get_simclr_pipeline_transform((32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279ed33-e4e3-4f33-96f9-05971cc75551",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = create_data_loaders(v2.ToTensor(), batch_size, 0, seed = 1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d388c-2226-49f4-b185-825c9048f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, (X, Y) = next(enumerate(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bc89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = transforms1(X); X2 = transforms2(X)\n",
    "\n",
    "print(type(X1))\n",
    "print(type(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b8dd8-10d2-4a7a-a321-1456eb27c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085f655-7e32-449c-801c-3a73a74815b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0].cpu().permute((1,2,0))); plt.show()\n",
    "plt.imshow(X1[0].cpu().permute((1,2,0))); plt.show()\n",
    "plt.imshow(X2[0].cpu().permute((1,2,0))); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec2a9e-08d0-4569-8175-a4f3062a1228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f813d20-e3f7-47c3-b4c5-7831f3c4ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(first_images, second_images, rank, world_size = 1, temperature=0.1):\n",
    "        # Each image is represented with k parameters,\n",
    "        # Assume the batch size is N, so the\n",
    "        # inputs have shape (N, k)\n",
    "\n",
    "        # These are pre-distributed shapes:\n",
    "        N = first_images.shape[0]\n",
    "        k = first_images.shape[1]\n",
    "\n",
    "\n",
    "        first_images = first_images / torch.norm(first_images,dim=1).reshape((-1,1))\n",
    "        second_images = second_images / torch.norm(second_images,dim=1).reshape((-1,1))\n",
    "\n",
    "        # Take the two tuples, and concatenate them.\n",
    "        # Then, reshape into Y = (1, 2N, k) and Z = (2N, 1, k)\n",
    "\n",
    "        c = torch.concat([first_images, second_images], dim=0)\n",
    "\n",
    "        # Gather all the c up if the world size > 1:\n",
    "        if world_size > 1:\n",
    "            gathered_c = torch.distributed.all_gather(tensor=c)\n",
    "            gathered_c = gathered_c.reshape((-1, first_images.shape[-1]))\n",
    "        else:\n",
    "            gathered_c = c\n",
    "\n",
    "        # Each rank computes only a slice of the global loss matrix, or\n",
    "        # the memory usage gets out of control.\n",
    "\n",
    "        # We calculate the dot product between the local and global tensors:\n",
    "        local_reps = c.reshape((c.shape[0], 1, c.shape[1]))\n",
    "        all_reps   = gathered_c.reshape((1, gathered_c.shape[0], gathered_c.shape[1]))\n",
    "\n",
    "\n",
    "        # Assume we have n images per rank, for N global images with N = n * world_size\n",
    "        # Compute the product of these tensors, which gives shape\n",
    "        # (2n, 2N, k)\n",
    "        mat =  local_reps*all_reps\n",
    "\n",
    "        # We need to compute the function (sim(x,y)) for each element in the 2N sequent.\n",
    "        # Since the are normalized, we're computing x^T . Y / (||x||*||y||),\n",
    "        # but the norms are equal to 1.\n",
    "        # So, summing the matrix over the dim = 0 and dim = 1 computes this for each pair.\n",
    "\n",
    "        sim = torch.sum(mat, dim=-1) / temperature\n",
    "\n",
    "\n",
    "\n",
    "        # Now, sim is of shape [2*n, 2*N]\n",
    "\n",
    "        # This yields a symmetric matrix, diagonal entries equal 1.  Off diagonal are symmetrics and < 1.\n",
    "\n",
    "        # sim = torch.exp(sim / temperature)\n",
    "        # Now, for every entry i in C (concat of both batches), the sum of sim[i] - sim[i][i] is the denominator\n",
    "\n",
    "        device = sim.device\n",
    "\n",
    "        # Since we have a non-symmetric matrix, need to build a non-symmetric index:\n",
    "        positive = torch.zeros(sim.shape, device=device)\n",
    "\n",
    "        # We concatenated all the local examples, and compute symmetric positive pairs\n",
    "        # So for the first N entries, the index of the positive pair is i + N  (locally)\n",
    "        # For the second N entries, the index of the positive pair is i - N (locally)\n",
    "        # with a distributed run, we've squashed all the similarity scores together.\n",
    "        # to a shape of [2*N, 2*N*Size]\n",
    "        # Each 2*N by 2*N block is the local positive indexes, all others are negative.\n",
    "        # That means that the index is shifted by global_rank*2*N\n",
    "\n",
    "        access_index_x = torch.arange(2*N)\n",
    "        # For the first N, the y-index is equal to x + 2*N\n",
    "        # For the second N\n",
    "        access_index_y = torch.arange(2*N)\n",
    "        # Shift by +/- N:\n",
    "        access_index_y[0:N] = access_index_y[0:N] + N\n",
    "        access_index_y[N:]  = access_index_y[N:] - N\n",
    "\n",
    "        access_index_y +=  rank * 2*N\n",
    "\n",
    "        # print(\"access_index_y: \", access_index_y, flush=True)\n",
    "\n",
    "        positive[access_index_x, access_index_y] = 1\n",
    "\n",
    "        # For the negative, we invert the positive and have to 0 out the self-index entries\n",
    "        negative = 1 - positive\n",
    "\n",
    "        # THESE WORK IF IT'S NOT DISTRIBUTED\n",
    "        # positive = torch.tile(torch.eye(N, device=device), (2,2))\n",
    "        # # Unsure if this line is needed?\n",
    "        # positive = positive - torch.eye(2*N, device=device)\n",
    "        #\n",
    "        # negative = - (torch.eye(2*N, device=device) - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Here, we can compute the top-k metrics for this batch, since we have the global state:\n",
    "            # We want the top 5 entries but the self-sim is obviously perfect.\n",
    "            # So take the top 6 and reject the first.\n",
    "            topk = torch.topk(sim, k=6, dim=-1, sorted=True)\n",
    "\n",
    "            # Top 1 is just an equality check:\n",
    "            top1_acc = topk.indices[:,1] == access_index_y.to(topk.indices.device)\n",
    "            top1_acc = torch.mean(top1_acc.to(torch.float))\n",
    "          \n",
    "            # Top 5 is a little more complicated:\n",
    "            # Compute the index distance to the correct index, abs value:\n",
    "            top5_acc_dist = torch.abs(topk.indices[:,1:] - access_index_y.to(topk.indices.device).reshape(-1,1))\n",
    "            # Get the minumum value, and see if it is less than 5:\n",
    "            min_values, _ = torch.min(top5_acc_dist, dim=-1)\n",
    "            top5_acc =  min_values < 5.\n",
    "            # Average over the batch dimension:\n",
    "            top5_acc = torch.mean(top5_acc.to(torch.float))\n",
    "\n",
    "\n",
    "        negative_examples = sim * negative\n",
    "        positive_examples = sim * positive\n",
    "\n",
    "        # Now, positive/negative examples is the temperature normalized similarity.\n",
    "        # we need to sum across the whole batch dimension to compute it per-example:\n",
    "\n",
    "\n",
    "        # Compute the alignment, summed over the entire global batch:\n",
    "        alignment = torch.sum(positive_examples, dim=-1)\n",
    "\n",
    "        # Compute the exp, which we'll eventually sum and log:\n",
    "        exp = torch.sum(torch.exp(negative_examples), dim=-1)\n",
    "\n",
    "        # print(\"Alignment: \", alignment, flush=True)\n",
    "        # print(\"exp: \",       exp, flush=True)\n",
    "\n",
    "\n",
    "        # And compute the logsumexp of the negative examples:\n",
    "        log_sum_exp = torch.log(exp )\n",
    "\n",
    "\n",
    "        # Additionally, we can compute the \"floor\" of the loss at this batch size:\n",
    "        # floor = torch.log(1.*N) - 1.\n",
    "\n",
    "        loss_metrics = {\n",
    "            \"alignment\"   : torch.mean(alignment),\n",
    "            \"log_sum_exp\" : torch.mean(log_sum_exp),\n",
    "            \"top1\"        : top1_acc,\n",
    "            \"top5\"        : top5_acc,\n",
    "            # \"floor\"       : floor,\n",
    "        }\n",
    "\n",
    "        loss = torch.mean( - alignment + log_sum_exp)\n",
    "        return loss, loss_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135ea73-85b0-431d-b942-61e9bfbd340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, t1, t2, model, head, loss_fn, optimizer, rank, size, progress_bar):\n",
    "    model.train()\n",
    "    head.train()\n",
    "    for (batch, (X, _)) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        X1 = t1(X); X2 = t2(X)\n",
    "        pred1 = head(model(X1))\n",
    "        pred2 = head(model(X2))\n",
    "        loss, metrics = loss_fn(pred1, pred2, rank, size)\n",
    "\n",
    "        # print(metrics)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # progress_bar.refresh()\n",
    "        cpu_metrics = { key : f\"{metrics[key].detach().cpu().numpy():.2f}\" for key in metrics.keys()}\n",
    "        cpu_metrics[\"loss\"] = f\"{loss.detach().cpu().numpy():.2f}\"\n",
    "        progress_bar.update()\n",
    "        progress_bar.set_postfix(cpu_metrics)\n",
    "        # progress_bar.description = f\"Train loss: {loss.cpu():.2f} top5: {metrics['top5'].cpu():.2f}\"\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29465e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(dataloader, t1, t2, model, head, loss_fn, rank, size, progress_bar):\n",
    "    model.train()\n",
    "    head.train()\n",
    "    n = 0.\n",
    "    sum_metrics = None\n",
    "    for (batch, (X, _)) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        X1 = t1(X); X2 = t2(X)\n",
    "        pred1 = head(model(X1))\n",
    "        pred2 = head(model(X2))\n",
    "        loss, metrics = loss_fn(pred1, pred2, rank, size)\n",
    "\n",
    "        # print(metrics)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # progress_bar.refresh()\n",
    "        cpu_metrics = { key : metrics[key].detach().cpu().numpy() for key in metrics.keys()}\n",
    "        if sum_metrics is None: \n",
    "            sum_metrics = cpu_metrics\n",
    "        else:\n",
    "            for key in sum_metrics.keys():\n",
    "                sum_metrics[key] += cpu_metrics[key]\n",
    "        progress_bar.update()\n",
    "        n += 1.\n",
    "        # progress_bar.description = f\"Train loss: {loss.cpu():.2f} top5: {sum_metrics['top5'].cpu():.2f}\"\n",
    "        # break\n",
    "    \n",
    "    for key in sum_metrics:\n",
    "        sum_metrics[key] = sum_metrics[key] / n\n",
    "    return sum_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e321f3-7cb3-43cd-bd0e-416234443a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(list(model.parameters()) + list(head.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb095e0b-e965-4d66-90bc-23b279f32034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72478d32-f057-4808-92b7-3a6da64fa9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "for j in range(5):\n",
    "    # with tqdm(total=len(train), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar1:\n",
    "    \n",
    "    #     train_one_epoch(train, transforms1, transforms2, model, head, contrastive_loss, optimizer, 0, 1, train_bar1)\n",
    "\n",
    "    with tqdm(total=len(val), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n",
    "        metrics = validate_one_epoch(val, transforms1, transforms2, model, head, contrastive_loss, 0, 1, val_bar)\n",
    "        print_metrics = {\n",
    "            key : f\"{key}={metrics[key]:.2f}\" for key in metrics.keys()\n",
    "        }\n",
    "        print_metrics = \"; \".join(print_metrics.values())\n",
    "        print(f\"Validate epoch {j}: \", print_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f602adb-f56a-48af-ba08-fad3b3020236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we retrain the classification head without touching the representation. This is called fine tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e03fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afda987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, head, loss_fn, val_bar):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            pred = head(model(X))\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            val_bar.update()\n",
    "            \n",
    "    loss /= num_batches\n",
    "    correct /= (size*batch_size)\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(dataloader, rep_model, head, loss_fn, optimizer, progress_bar):\n",
    "    head.train()\n",
    "    model.eval()\n",
    "    for batch1, (X, Y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        # Calling detach blocks all gradients into the representation model!\n",
    "        rep = rep_model(X).detach()\n",
    "        pred = head(rep)\n",
    "        loss = loss_fn(pred, Y)\n",
    "        \n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        correct = (pred.argmax(1) == Y).type(torch.float).mean().item()\n",
    "                    \n",
    "        # progress_bar.refresh()\n",
    "        cpu_metrics = {}\n",
    "        cpu_metrics[\"acc\"] = f\"{correct:.2f}\"\n",
    "        cpu_metrics[\"loss\"] = f\"{loss.detach().cpu().numpy():.2f}\"\n",
    "        progress_bar.update()\n",
    "        progress_bar.set_postfix(cpu_metrics)\n",
    "        # progress_bar.description = f\"Train loss: {loss.cpu():.2f} top5: {metrics['top5'].cpu():.2f}\"\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head = nn.Linear(256, 10).cuda()\n",
    "classification_loss = loss_fn = nn.CrossEntropyLoss()\n",
    "fine_tune_optimizer = torch.optim.AdamW(classification_head.parameters(), lr=0.01)\n",
    "print(fine_tune_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in range(5):\n",
    "    with tqdm(total=len(train), position=0, leave=True, desc=f\"Fine Tune Epoch {j}\") as train_bar1:\n",
    "    \n",
    "        fine_tune(train, model, classification_head, classification_loss, fine_tune_optimizer, train_bar1)\n",
    "    with tqdm(total=len(val), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n",
    "        acc, loss = evaluate(val, model, classification_head, classification_loss, val_bar)\n",
    "        print(f\"Epoch {j}: validation loss: {loss:.3f}, accuracy: {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398afdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
