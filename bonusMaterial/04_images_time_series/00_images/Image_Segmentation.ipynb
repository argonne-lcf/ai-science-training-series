{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This tutorial is based on__:\n",
    "\n",
    "1. [Image segmentation with a U-Net-like architecture, Francois Chollet](https://keras.io/examples/vision/oxford_pets_image_segmentation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the Penn-Fudan dataset\n",
    "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
    "# extract it in the current folder\n",
    "!unzip -q PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Looking at Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a random Image and its corresponding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'PennFudanPed/PNGImages/FudanPed00021.png'\n",
    "mask_path = 'PennFudanPed/PedMasks/FudanPed00021_mask.png'\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "# Image\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "image = load_img(img_path)\n",
    "image = np.array(image)\n",
    "ax1.imshow(image)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Mask\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "mask = np.array( load_img(mask_path, grayscale=True) )\n",
    "ax2.imshow(mask)\n",
    "ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, each pedestrian is assigned a unique id in the mask:\n",
    "\n",
    "- Background id : 0\n",
    "- First Pedestrian : 1\n",
    "- Second Pedestrian : 2, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we will reduce this to a simpler binary problem of segementing out pedestrian class from the background class. This is often referred to as **'Semantic Segmentation'** where one mask is assigned to a whole class, as opposed to **'Instance Segmentation'** where each member of a class is given a unique mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (mask != 0)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Data-Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pedestrian_Datagenerator(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            mask = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            mask = np.array(mask)\n",
    "            mask = mask!=0\n",
    "            y[j] = np.expand_dims(mask, 2)\n",
    "        return x/255, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Data into Train / Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all paths\n",
    "\n",
    "input_dir = \"PennFudanPed/PNGImages/\"\n",
    "target_dir = \"PennFudanPed/PedMasks/\"\n",
    "\n",
    "\n",
    "input_img_paths = sorted(glob.glob(input_dir + '*.png'))\n",
    "target_img_paths = sorted(glob.glob(target_dir + '*.png'))\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "    print(input_path, \"|\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train / Validation\n",
    "val_samples = 20\n",
    "random.Random(42).shuffle(input_img_paths)\n",
    "random.Random(42).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "print(f'Train set size: {len(train_input_img_paths)} \\nValidation set size: {len(val_input_img_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate data generators\n",
    "\n",
    "img_size = (160, 160)\n",
    "batch_size = 4\n",
    "\n",
    "train_gen = Pedestrian_Datagenerator(batch_size, img_size, train_input_img_paths, train_target_img_paths)\n",
    "val_gen = Pedestrian_Datagenerator(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define U-Net like Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-net is a popular architecture for image segmentation. The name comes from its U like structure as shown in the image below. The architecture broadly consists of two parts; a downsampling part consisting of a series of convolutions and max-pooling layers (like a regular CNN), followed by an upsampling part consisting of a series of up-convolutions and/or upsampling operations. For more detail on the U-Net architecture refer to the original [U-Net paper](https://arxiv.org/abs/1505.04597).  \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2b/Example_architecture_of_U-Net_for_producing_k_256-by-256_image_masks_for_a_256-by-256_RGB_image.png\" alt=\"U-net Image\" width=\"600\"/>\n",
    "\n",
    "[src: [Wikipedia](https://en.wikipedia.org/wiki/U-Net)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(img_size):\n",
    "    inputs = Input(shape=img_size + (3,))\n",
    "    \n",
    "    # First half of the network: downsampling \n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    \n",
    "    # Second half of the network: upsampling \n",
    "    up4 = Conv2D(128, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (4,4))(pool3))\n",
    "    merge4 = concatenate([conv2,up4], axis = 3)\n",
    "    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same')(merge4)\n",
    "    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv4)\n",
    "    up5 = Conv2D(64, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv4))\n",
    "    merge5 = concatenate([conv1,up5], axis = 3)\n",
    "    conv5 = Conv2D(64, 3, activation = 'relu', padding = 'same')(merge5)\n",
    "    conv5 = Conv2D(64, 3, activation = 'relu', padding = 'same')(conv5)\n",
    "    conv5 = Conv2D(2, 3, activation = 'relu', padding = 'same')(conv5)\n",
    "    conv6 = Conv2D(1, 1, activation = 'sigmoid')(conv5)\n",
    "\n",
    "    model = keras.Model(inputs = inputs, outputs = conv6)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = unet(img_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compile and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"pedestrian_segmentation.h5\", save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and Train \n",
    "epochs = 40\n",
    "history = model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) On Train Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = Pedestrian_Datagenerator(1, img_size, train_input_img_paths, train_target_img_paths)\n",
    "train_preds = model.predict(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 4\n",
    "idxs = np.arange(0,20); random.Random(42).shuffle(idxs); idxs = idxs[:4]\n",
    "\n",
    "\n",
    "print('\\t\\tImage \\t\\t\\t\\t Ground Truth \\t\\t\\t\\t Prediction')\n",
    "for i in np.arange(samples):\n",
    "    idx = idxs[i]\n",
    "    x, y = train_gen[idx]\n",
    "    pred = train_preds[idx]\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    \n",
    "    # Image\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    ax1.imshow(x[0])\n",
    "    ax1.axis('off')\n",
    "    #ax1.set_title('Image')\n",
    "    \n",
    "    # Mask\n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    ax2.imshow(y.squeeze())\n",
    "    ax2.axis('off')\n",
    "    #ax2.set_title('Ground Truth')\n",
    "    \n",
    "    # Pred\n",
    "    ax3 = plt.subplot(1, 3, 3)\n",
    "    ax3.imshow(pred.squeeze())\n",
    "    ax3.axis('off')\n",
    "    #ax3.set_title('Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) On Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = Pedestrian_Datagenerator(1, img_size, val_input_img_paths, val_target_img_paths)\n",
    "val_preds = model.predict(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 4\n",
    "idxs = np.arange(0,20); random.Random(42).shuffle(idxs); idxs = idxs[:4]\n",
    "\n",
    "\n",
    "print('\\t\\tImage \\t\\t\\t\\t Ground Truth \\t\\t\\t\\t Prediction')\n",
    "for i in np.arange(samples):\n",
    "    idx = idxs[i]\n",
    "    x, y = val_gen[idx]\n",
    "    pred = val_preds[idx]\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    \n",
    "    # Image\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    ax1.imshow(x[0])\n",
    "    ax1.axis('off')\n",
    "    #ax1.set_title('Image')\n",
    "    \n",
    "    # Mask\n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    ax2.imshow(y.squeeze())\n",
    "    ax2.axis('off')\n",
    "    #ax2.set_title('Ground Truth')\n",
    "    \n",
    "    # Pred\n",
    "    ax3 = plt.subplot(1, 3, 3)\n",
    "    ax3.imshow(pred.squeeze())\n",
    "    ax3.axis('off')\n",
    "    #ax3.set_title('Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2021-09-22",
   "language": "python",
   "name": "conda-2021-09-22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
