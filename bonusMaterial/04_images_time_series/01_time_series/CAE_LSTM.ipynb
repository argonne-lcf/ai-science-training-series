{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional autoencoders (CAE) and LSTMs for PDE surrogates\n",
    "\n",
    "*Original author*: Romit Maulik\n",
    "\n",
    "*Additional edits*: Kyle Felker\n",
    "\n",
    "Maulik, R., Lusch, B., & Balaprakash, P. (2021). Reduced-order modeling of advection-dominated systems with recurrent neural networks and convolutional autoencoders. Physics of Fluids, 33(3), 037106.\n",
    "https://aip.scitation.org/doi/abs/10.1063/5.0039986\n",
    "\n",
    "## Shallow water equations (SWE)\n",
    "\n",
    "![Shallow water equations](media/Shallow_water_waves.gif \"Shallow water equations\")\n",
    "<center><a href=\"https://en.wikipedia.org/wiki/Shallow_water_equations\"><b>Wikipedia</b></a>: Output from a shallow-water equation model of water in a bathtub. The water experiences five splashes which generate surface gravity waves that propagate away from the splash locations and reflect off the bathtub walls. </center>\n",
    "\n",
    "### Conservative form of SWE:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial (\\rho \\eta) }{\\partial t} &+ \\frac{\\partial (\\rho \\eta u)}{\\partial x} + \\frac{\\partial (\\rho \\eta v)}{\\partial y} = 0,\\\\[3pt]\n",
    "\\frac{\\partial (\\rho \\eta u)}{\\partial t} &+ \\frac{\\partial}{\\partial x}\\left( \\rho \\eta u^2 + \\frac{1}{2}\\rho g \\eta^2 \\right) + \\frac{\\partial (\\rho \\eta u v)}{\\partial y} = 0,\\\\[3pt]\n",
    "\\frac{\\partial (\\rho \\eta v)}{\\partial t} &+ \\frac{\\partial (\\rho \\eta uv)}{\\partial x} + \\frac{\\partial}{\\partial y}\\left(\\rho \\eta v^2 + \\frac{1}{2}\\rho g \\eta ^2\\right) = 0.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\eta(x,y,t)$ is the fluid column height, $(u,v)$ is the horizontal flow velocity. Inviscid, incompressible fluid with the horizontal length scale >> vertical length scale. Used in atmospheric and oceanic modeling, when you include Coriolis forces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Lambda, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras import optimizers, models, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.ticker as mtick\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SWE snapshot data\n",
    "\n",
    "Generated using a finite volume simulation:\n",
    "> Our full-order model uses a fourth-order accurate Runge–Kutta temporal integration scheme and a ﬁfth-order accurate weighted essentially nonoscillatory scheme (WENO) 57 for computing state reconstructions at cell faces. The Rusanov Reimann solver is utilized for ﬂux reconstruction after cell-face quantities are calculated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single initial condition (simulation) in each dataset\n",
    "swe_data = np.transpose(np.load('datasets/train.npy'))\n",
    "swe_data_v = np.transpose(np.load('datasets/validation.npy'))\n",
    "print(swe_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep, y, x, variable (channel)\n",
    "swe_train_data = np.zeros(shape=(400,64,64,3)) # Channels last\n",
    "swe_valid_data = np.zeros(shape=(400,64,64,3))\n",
    "\n",
    "for i in range(np.shape(swe_data)[0]):\n",
    "    temp_1 = swe_data[i,0:64*64].reshape(64,64)\n",
    "    temp_2 = swe_data[i,64*64:2*64*64].reshape(64,64)\n",
    "    temp_3 = swe_data[i,2*64*64:3*64*64].reshape(64,64)\n",
    "    swe_train_data[i,:,:,0] = np.transpose(temp_1[:,:])\n",
    "    swe_train_data[i,:,:,1] = np.transpose(temp_2[:,:])\n",
    "    swe_train_data[i,:,:,2] = np.transpose(temp_3[:,:])\n",
    "    \n",
    "for i in range(np.shape(swe_data_v)[0]):\n",
    "    temp_1 = swe_data_v[i,0:64*64].reshape(64,64)\n",
    "    temp_2 = swe_data_v[i,64*64:2*64*64].reshape(64,64)\n",
    "    temp_3 = swe_data_v[i,2*64*64:3*64*64].reshape(64,64)\n",
    "    swe_valid_data[i,:,:,0] = np.transpose(temp_1[:,:])\n",
    "    swe_valid_data[i,:,:,1] = np.transpose(temp_2[:,:])\n",
    "    swe_valid_data[i,:,:,2] = np.transpose(temp_3[:,:])\n",
    "    \n",
    "\n",
    "# Just to keep things simple, cut datasets in half by number of timesteps\n",
    "# swe_train_data = swe_train_data[0:200,:,:,:]\n",
    "# swe_valid_data = swe_valid_data[0:200,:,:,:]\n",
    "\n",
    "# Normalize inputs ([0,1] for \\rho\\eta, arbitrary for other two vars)\n",
    "for j in range(3):\n",
    "    swe_train_data[:,:,:,j] = (swe_train_data[:,:,:,j] - np.min(swe_train_data[:,:,:,0]))/(np.max(swe_train_data[:,:,:,0])-np.min(swe_train_data[:,:,:,0]))\n",
    "    swe_valid_data[:,:,:,j] = (swe_valid_data[:,:,:,j] - np.min(swe_valid_data[:,:,:,0]))/(np.max(swe_valid_data[:,:,:,0])-np.min(swe_valid_data[:,:,:,0]))\n",
    "\n",
    "# Visualize one time instance\n",
    "time = 0\n",
    "fig, ax = plt.subplots(nrows=1,ncols=3, figsize=(15,15))\n",
    "\n",
    "ax[0].imshow(swe_valid_data[time,:,:,0])\n",
    "ax[1].imshow(swe_valid_data[time,:,:,1])\n",
    "ax[2].imshow(swe_valid_data[time,:,:,2])\n",
    "\n",
    "ax[0].set_title(r'$q_1 =\\rho \\eta$')\n",
    "ax[1].set_title(r'$q_2 = \\rho\\eta u$')\n",
    "ax[2].set_title(r'$q_3 = \\rho\\eta v$')\n",
    "ax[0].set_ylabel(f\"$t = {time}$\",rotation=0, labelpad=30, fontsize=16) # fontweight='bold')\n",
    "# fig.suptitle(f\"$t = {time}$\")\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(top=0.85)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple time instances\n",
    "for time in range(0,200,10):\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=3)\n",
    "    ax[0].imshow(swe_train_data[time,:,:,0])\n",
    "    ax[1].imshow(swe_train_data[time,:,:,1])\n",
    "    ax[2].imshow(swe_train_data[time,:,:,2])\n",
    "    ax[0].set_title(r'$q_1 = \\rho\\eta$')\n",
    "    ax[1].set_title(r'$q_2 = \\rho\\eta u$')\n",
    "    ax[2].set_title(r'$q_3 = \\rho\\eta v$')\n",
    "    ax[0].set_ylabel(f\"$t = {time}$\",rotation=0, labelpad=30, fontsize=16) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Presets and Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'  # 'train'\n",
    "lrate = 0.001\n",
    "\n",
    "def mean_absolute_error(y_pred,y_true):\n",
    "    return K.mean(K.abs(y_true-y_pred))\n",
    "\n",
    "def max_absolute_error(y_pred,y_true):\n",
    "    return K.max(K.abs(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define recursive model architecture\n",
    "weights_filepath = 'saved_models/SWE_CAE_Weights.h5'\n",
    "## Encoder\n",
    "encoder_inputs = Input(shape=(64,64,3),name='Field')\n",
    "# Encode   \n",
    "x = Conv2D(30,kernel_size=(3,3),activation='relu',padding='same')(encoder_inputs)\n",
    "enc_l2 = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "x = Conv2D(20,kernel_size=(3,3),activation='relu',padding='same')(enc_l2)\n",
    "enc_l3 = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "x = Conv2D(10,kernel_size=(3,3),activation='relu',padding='same')(enc_l3)\n",
    "enc_l4 = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "x = Conv2D(15,kernel_size=(3,3),activation='relu',padding='same')(enc_l4)\n",
    "enc_l5 = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "x = Conv2D(1,kernel_size=(3,3),activation=None,padding='same')(enc_l5)\n",
    "encoded = MaxPooling2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "encoder = Model(inputs=encoder_inputs,outputs=encoded)\n",
    "    \n",
    "## Decoder\n",
    "decoder_inputs = Input(shape=(2,2,1),name='decoded')\n",
    "\n",
    "x = Conv2D(1,kernel_size=(3,3),activation='relu',padding='same')(decoder_inputs)\n",
    "dec_l1 = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(5,kernel_size=(3,3),activation='relu',padding='same')(dec_l1)\n",
    "dec_l2 = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(10,kernel_size=(3,3),activation='relu',padding='same')(dec_l2)\n",
    "dec_l3 = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(20,kernel_size=(3,3),activation='relu',padding='same')(dec_l3)\n",
    "dec_l4 = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(30,kernel_size=(3,3),activation='relu',padding='same')(dec_l4)\n",
    "dec_l5 = UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "decoded = Conv2D(3,kernel_size=(3,3),activation=None,padding='same')(dec_l5)\n",
    "    \n",
    "decoder = Model(inputs=decoder_inputs,outputs=decoded)\n",
    "\n",
    "## Autoencoder\n",
    "ae_outputs = decoder(encoder(encoder_inputs))\n",
    "  \n",
    "model = Model(inputs=encoder_inputs,outputs=ae_outputs,name='CAE')\n",
    "   \n",
    "# design network\n",
    "my_adam = optimizers.Adam(learning_rate=lrate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='loss', verbose=1, save_best_only=True, mode='min',save_weights_only=True)\n",
    "earlystopping = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit network\n",
    "model.compile(optimizer=my_adam,loss='mean_squared_error',metrics=[mean_absolute_error,max_absolute_error])    \n",
    "model.summary()\n",
    "\n",
    "num_epochs = 5000\n",
    "batch_size = 4\n",
    "\n",
    "if mode == 'train':\n",
    "    train_history = model.fit(x=swe_train_data, y=swe_train_data, epochs=num_epochs, batch_size=batch_size, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(encoder, show_shapes=True, show_layer_names=True, to_file='encoder-model.png')\n",
    "Image('encoder-model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(decoder, show_shapes=True, show_layer_names=True, to_file='decoder-model.png')\n",
    "Image('decoder-model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 100\n",
    "\n",
    "model.load_weights(weights_filepath)\n",
    "from scipy.ndimage import gaussian_filter\n",
    "recoded_1 = model.predict(swe_train_data[time:time+1,:,:,:])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2,ncols=3,figsize=(14,12))\n",
    "\n",
    "cs1 = ax[0,0].imshow(swe_train_data[time,:,:,0],label='input')\n",
    "ax[1,0].imshow(gaussian_filter(recoded_1[0,:,:,0],sigma=2),label='decoded')\n",
    "\n",
    "cs2 = ax[0,1].imshow(swe_train_data[time,:,:,1],label='input')\n",
    "ax[1,1].imshow(gaussian_filter(recoded_1[0,:,:,1],sigma=2),label='decoded')\n",
    "\n",
    "cs3 = ax[0,2].imshow(swe_train_data[time,:,:,2],label='input')\n",
    "ax[1,2].imshow(gaussian_filter(recoded_1[0,:,:,2],sigma=2),label='decoded')\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax[i,j].set_xlabel('x')\n",
    "        ax[i,j].set_ylabel('y')\n",
    "        \n",
    "fig.colorbar(cs1,ax=ax[0,0],fraction=0.046, pad=0.04)\n",
    "fig.colorbar(cs1,ax=ax[1,0],fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.colorbar(cs2,ax=ax[0,1],fraction=0.046, pad=0.04)\n",
    "fig.colorbar(cs2,ax=ax[1,1],fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.colorbar(cs3,ax=ax[0,2],fraction=0.046, pad=0.04)\n",
    "fig.colorbar(cs3,ax=ax[1,2],fraction=0.046, pad=0.04)\n",
    "\n",
    "\n",
    "ax[0,0].set_title(r'True $q_1$')\n",
    "ax[0,1].set_title(r'True $q_2$')\n",
    "ax[0,2].set_title(r'True $q_3$')\n",
    "\n",
    "ax[1,0].set_title(r'Reconstructed $q_1$')\n",
    "ax[1,1].set_title(r'Reconstructed $q_2$')\n",
    "ax[1,2].set_title(r'Reconstructed $q_3$')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5,hspace=-0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate encoded data for LSTM learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = K.eval(encoder(swe_train_data[:,:,:,:].astype('float32')))\n",
    "# Visualize latent space\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(7,6))\n",
    "time = 98\n",
    "cs = ax[0].imshow(encoded[time,:,:,0])\n",
    "fig.colorbar(cs,ax=ax[0],fraction=0.046, pad=0.04)\n",
    "\n",
    "time = 198\n",
    "ax[1].imshow(encoded[time,:,:,0])\n",
    "fig.colorbar(cs,ax=ax[1],fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoded.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(encoded[:,0],label='Mode 1')\n",
    "plt.plot(encoded[:,1],label='Mode 2')\n",
    "plt.plot(encoded[:,2],label='Mode 3')\n",
    "plt.plot(encoded[:,3],label='Mode 4')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 15 # The sliding window size of the LSTM, for truncated backpropagation through time (BPTT)\n",
    "lstm_training_data = np.copy(encoded)\n",
    "num_train_snapshots = 1\n",
    "total_size = np.shape(lstm_training_data)[0]\n",
    "\n",
    "# Shape the inputs and outputs\n",
    "input_seq = np.zeros(shape=(total_size-time_window,time_window,4))\n",
    "output_seq = np.zeros(shape=(total_size-time_window,4))\n",
    "\n",
    "# Setting up inputs\n",
    "sample = 0\n",
    "for t in range(time_window,total_size):\n",
    "    input_seq[sample,:,:] = lstm_training_data[t-time_window:t,:]\n",
    "    output_seq[sample,:] = lstm_training_data[t,:]\n",
    "    sample = sample + 1\n",
    "print(f\"Total samples = {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "lstm_model = models.Sequential()\n",
    "\n",
    "# LSTM is always \"stateful\" when time_window > 1. Keras \"stateful\" option refers to inter-batch states\n",
    "lstm_model.add(LSTM(20,input_shape=(time_window, 4),return_sequences=True, stateful=False)) \n",
    "lstm_model.add(LSTM(20,input_shape=(time_window, 4),return_sequences=False))  #\n",
    "lstm_model.add(Dense(4, activation=None))\n",
    "\n",
    "# lstm_model = models.Sequential(\n",
    "# [\n",
    "#     LSTM(20,input_shape=(time_window, 4),return_sequences=True),\n",
    "#     LSTM(20,input_shape=(time_window, 4),return_sequences=False),\n",
    "#     Dense(4, activation=None)\n",
    "# ])\n",
    "\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 3000\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# design network\n",
    "lstm_filepath = './saved_models/SWE_LSTM_Weights.h5'\n",
    "lstm_adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "checkpoint = ModelCheckpoint(lstm_filepath, monitor='loss', verbose=0, save_best_only=True, mode='min',save_weights_only=True)\n",
    "earlystopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "lstm_callbacks_list = [checkpoint]\n",
    "\n",
    "# fit network\n",
    "lstm_model.compile(optimizer=lstm_adam,loss='mean_squared_error',metrics=[mean_absolute_error,max_absolute_error])\n",
    "\n",
    "#mode = 'train'\n",
    "if mode == 'train':\n",
    "    lstm_train_history = lstm_model.fit(input_seq, output_seq, epochs=num_epochs, batch_size=batch_size, callbacks=lstm_callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN sequence input and output](media/rnn_sequences.png)\n",
    "Adapted from [Andrej Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "See https://stackoverflow.com/questions/38714959/understanding-keras-lstms for a helpful summary of the Keras API for `LSTM()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lstm_model, show_shapes=True, show_layer_names=True, to_file='lstm-model.png')\n",
    "Image('lstm-model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LSTM with parameter information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode='test'\n",
    "if mode == 'test':\n",
    "    lstm_model.load_weights(lstm_filepath)\n",
    "\n",
    "encoded_valid = K.eval(encoder(swe_valid_data[:,:,:,:].astype('float32')))\n",
    "encoded_valid = encoded_valid.reshape(-1,4)\n",
    "lstm_testing_data = np.copy(encoded_valid)\n",
    "\n",
    "# Shape the inputs and outputs\n",
    "input_seq = np.zeros(shape=(1,time_window,4))\n",
    "output_seq_pred = np.zeros(shape=(total_size,4))\n",
    "\n",
    "# Setting up inputs\n",
    "sample = 0\n",
    "for t in range(time_window,total_size):\n",
    "    input_seq[0,:,:] = lstm_testing_data[t-time_window:t,:]\n",
    "    output_seq_pred[t,:] = lstm_model.predict(input_seq[0:1,:,:])[0,:]\n",
    "    input_seq[0,0:time_window-1,:] = input_seq[0,1:,:] \n",
    "    input_seq[0,time_window-1,:] = output_seq_pred[t,:]\n",
    "    sample = sample + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check quality in latent space for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.plot(lstm_testing_data[time_window:,i],'r',label='True',linewidth=3)\n",
    "    plt.plot(output_seq_pred[time_window:,i],'b--',label='Predicted',linewidth=3)\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.xlim([0, 200])    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution in physical space\n",
    "\n",
    "Apply decoder to LSTM-evolved latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for decoding\n",
    "output_seq_pred = np.reshape(output_seq_pred,newshape=(-1,2,2,1))\n",
    "# Feed it through decoder\n",
    "decoded_valid = K.eval(decoder(output_seq_pred.astype('float32')))\n",
    "\n",
    "# Check evolution through spot checks\n",
    "time = 50\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2,ncols=3,figsize=(14,12))\n",
    "cs1 = ax[0,0].imshow(swe_valid_data[time,:,:,0],label='Truth')\n",
    "ax[1,0].imshow(gaussian_filter(decoded_valid[time,:,:,0],sigma=2),label='Prediction')\n",
    "\n",
    "cs2 = ax[0,1].imshow(swe_valid_data[time,:,:,1],label='Truth')\n",
    "ax[1,1].imshow(gaussian_filter(decoded_valid[time,:,:,1],sigma=2),label='Prediction')\n",
    "\n",
    "cs3 = ax[0,2].imshow(swe_valid_data[time,:,:,2],label='Truth')\n",
    "ax[1,2].imshow(gaussian_filter(decoded_valid[time,:,:,2],sigma=2),label='Prediction')\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax[i,j].set_xlabel('x')\n",
    "        ax[i,j].set_ylabel('y')\n",
    "        \n",
    "fig.colorbar(cs1,ax=ax[0,0],fraction=0.046, pad=0.04)\n",
    "fig.colorbar(cs1,ax=ax[1,0],fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.colorbar(cs2,ax=ax[0,1],fraction=0.046, pad=0.04)\n",
    "fig.colorbar(cs2,ax=ax[1,1],fraction=0.046, pad=0.04)\n",
    "\n",
    "fig.colorbar(cs3,ax=ax[0,2],fraction=0.046, pad=0.04)\n",
    "fig.colorbar(cs3,ax=ax[1,2],fraction=0.046, pad=0.04)\n",
    "\n",
    "\n",
    "ax[0,0].set_title(r'True $q_1$')\n",
    "ax[0,1].set_title(r'True $q_2$')\n",
    "ax[0,2].set_title(r'True $q_3$')\n",
    "\n",
    "ax[1,0].set_title(r'Reconstructed $q_1$')\n",
    "ax[1,1].set_title(r'Reconstructed $q_2$')\n",
    "ax[1,2].set_title(r'Reconstructed $q_3$')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5,hspace=-0.3)\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A posteriori analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1/2, 1/2, 64)  # Array with x-points\n",
    "y = np.linspace(-1/2, 1/2, 64)  # Array with x-points\n",
    "\n",
    "# Meshgrid for plotting\n",
    "X, Y = np.meshgrid(x, y)\n",
    "time = 199\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize = (11, 7))\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_surface(X, Y, swe_valid_data[time,:,:,0], rstride = 1, cstride = 1,\n",
    "    cmap = plt.cm.jet, linewidth = 0, antialiased = True)\n",
    "\n",
    "# ax.set_title('Visualization', fontname = \"serif\", fontsize = 17)\n",
    "ax.set_xlabel('x [m]', fontsize = 16)\n",
    "ax.set_ylabel('y [m]', fontsize = 16)\n",
    "\n",
    "ax.xaxis.labelpad=30\n",
    "ax.yaxis.labelpad=30\n",
    "\n",
    "ax.xaxis.labelpad=30\n",
    "ax.yaxis.labelpad=30\n",
    "\n",
    "ax.tick_params(axis='both', which='major', pad=15)\n",
    "\n",
    "ax.set_zticks([0.1, 0.15, 0.20, 0.25, 0.3])\n",
    "ax.set_zlim((0.1,0.3))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize = (11, 7))\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_surface(X, Y, gaussian_filter(decoded_valid[time,:,:,0],sigma=2), rstride = 1, cstride = 1,\n",
    "    cmap = plt.cm.jet, linewidth = 0, antialiased = True)\n",
    "\n",
    "# ax.set_title('Visualization', fontname = \"serif\", fontsize = 17)\n",
    "ax.set_xlabel('x [m]', fontsize = 16)\n",
    "ax.set_ylabel('y [m]', fontsize = 16)\n",
    "\n",
    "ax.xaxis.labelpad=30\n",
    "ax.yaxis.labelpad=30\n",
    "\n",
    "ax.set_zticks([0.1, 0.15, 0.20, 0.25, 0.3])\n",
    "ax.set_zlim((0.1,0.3))\n",
    "\n",
    "ax.tick_params(axis='both', which='major', pad=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX portability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_proto, external_tensor_storage = tf2onnx.convert.from_keras(lstm_model, \n",
    "                                                                  input_signature=[tf.TensorSpec((None, time_window, 4) )],\n",
    "                                                                  opset=10, output_path='./lstm.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2021-11-30",
   "language": "python",
   "name": "conda-2021-11-30"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
