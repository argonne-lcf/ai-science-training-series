{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argonne-lcf/ai-science-training-series/blob/main/05_llm_part2/RAGTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGpjFGh2qX2O"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACA8jfiCDuOU"
      },
      "source": [
        "## Overview\n",
        "*   Motivation for RAG\n",
        "*   Idea behind RAG\n",
        "*   Advantages and Disadvantages\n",
        "*   Implementation to augment question + answer\n",
        "*   Advanced applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5YjAA14ih8R"
      },
      "source": [
        "#### Imagine you went to live under a rock on August 2006. When you come out in 2024, you are asked how many planets revolve around the sun. What would you say?...\n",
        "![pluto](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/pluto_planets.jpeg?raw=1)\n",
        "\n",
        "This is similar to LLMs which are trained with data until a certain point and then asked questions on data they are not trained on. Understandably, LLMs will either be unable to answer or simply hallucinate a probably wrong answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3HmhY-rl29V"
      },
      "source": [
        "###What can be done?\n",
        "\n",
        "Have the LLM go to the library using **Retrieval Augmented Generation (RAG)**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXtqMcK1nAv_"
      },
      "source": [
        "RAG involves adding your own data (via a retrieval tool) to the prompt that you pass into a large language model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAGt5yQVrz0C"
      },
      "source": [
        "![rag architecture](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/rag-overview.original.png?raw=1)\n",
        "Image credit: https://scriv.ai/guides/retrieval-augmented-generation-overview/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6N4A87lt3Wq"
      },
      "source": [
        "RAG has been shown to improve LLM prediction accuracy without needing to increase parameter size.\n",
        "\n",
        "![rag architecture](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/rag_acc_v_size.png?raw=1)\n",
        "\n",
        "*Image credit: Yu, Wenhao. \"Retrieval-augmented generation across heterogeneous knowledge.\" Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop. 2022.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDcwB4BMyUQ5"
      },
      "source": [
        "RAG also increases explainability by giving the source for information.\n",
        "\n",
        "![rag architecture](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/rag_source_locator.png?raw=1)\n",
        "\n",
        "Image credit: https://ai.stanford.edu/blog/retrieval-based-NLP/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRUJ2eQvJIAg"
      },
      "source": [
        "## Advantages and Disadvantages\n",
        "\n",
        "### Advantages\n",
        "\n",
        "*   Provides domain specific context\n",
        "*   Improves predictive performance and reduces hallucinations\n",
        "*   Does not increase model parameters\n",
        "*   Less labor intensive than fine-tuning LLMs\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "*   May introduce latency since we are adding a relatively costly search step\n",
        "*   If your dataset includes private information, you may inadvertently expose another user with this information.\n",
        "*   The data you want to use needs to be curated and you should decide how the data should be accessed. This adds time for the initial set-up.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42Iqfmjy-1E"
      },
      "source": [
        "#Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVkTgnU80jjV"
      },
      "source": [
        "### 1. Install + load relevant modules:\n",
        "*   langchain\n",
        "*   torch\n",
        "*   transformers\n",
        "*   sentence-transformers\n",
        "*   datasets\n",
        "*   faiss-cpu  \n",
        "*   pypdf\n",
        "*  unstructure[pdf]\n",
        "*  huggingface_hub (add hf_token)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1uDuyDLd1Xi",
        "outputId": "f409cf5c-a38a-4b14-e8c4-555fd830cd34",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.1.5\n",
            "  Using cached langchain-0.1.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.17 (from langchain==0.1.5)\n",
            "  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain-core<0.2,>=0.1.16 (from langchain==0.1.5)\n",
            "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain==0.1.5)\n",
            "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.16.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (3.23.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.5) (3.0.0)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community<0.1,>=0.0.17 (from langchain==0.1.5)\n",
            "  Using cached langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n",
            "  Using cached langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Using cached langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Using cached langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
            "  Using cached langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n",
            "  Using cached langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Using cached langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Using cached langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
            "  Using cached langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n",
            "  Using cached langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-core<0.2,>=0.1.16 (from langchain==0.1.5)\n",
            "  Using cached langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.5) (3.7.1)\n",
            "  Using cached langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
            "  Using cached langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain==0.1.5)\n",
            "  Using cached langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.5) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.5) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.5) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.5) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.5) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.5) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.5) (3.1.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (1.2.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (0.2.0)\n",
            "Using cached langchain-0.1.5-py3-none-any.whl (806 kB)\n",
            "Using cached langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
            "Using cached langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "Using cached langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "Installing collected packages: langsmith, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.1.137\n",
            "    Uninstalling langsmith-0.1.137:\n",
            "      Successfully uninstalled langsmith-0.1.137\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.3.3\n",
            "    Uninstalling langchain-community-0.3.3:\n",
            "      Successfully uninstalled langchain-community-0.3.3\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.4\n",
            "    Uninstalling langchain-0.3.4:\n",
            "      Successfully uninstalled langchain-0.3.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-experimental 0.3.2 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.0.20 which is incompatible.\n",
            "langchain-experimental 0.3.2 requires langchain-core<0.4.0,>=0.3.6, but you have langchain-core 0.1.23 which is incompatible.\n",
            "langchain-text-splitters 0.3.0 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.1.5 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (23.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: unstructured==0.12.3 in /usr/local/lib/python3.10/dist-packages (0.12.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (2.14.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (2024.10.22)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (1.26.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (3.10.1)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (0.26.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (1.16.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (43.0.3)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (0.2.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (0.27.2)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (1.0.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (1.6.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (2.9.2)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (5.1.0)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (1.0.0)\n",
            "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (0.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2->unstructured-client>=0.15.1->unstructured==0.12.3) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured==0.12.3) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured==0.12.3) (3.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.12.3) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.12.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.12.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.12.3) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client>=0.15.1->unstructured==0.12.3) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured==0.12.3) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.0->unstructured-client>=0.15.1->unstructured==0.12.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.0->unstructured-client>=0.15.1->unstructured==0.12.3) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<0.10.0,>=0.9.0->unstructured-client>=0.15.1->unstructured==0.12.3) (1.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client>=0.15.1->unstructured==0.12.3) (2.22)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (1.2.2)\n",
            "Requirement already satisfied: unstructured==0.12.3 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (0.12.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (2.14.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (2024.10.22)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.26.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (3.10.1)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (0.26.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.16.0)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (1.17.0)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (1.17.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (20231228)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (9.4.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (5.1.0)\n",
            "Requirement already satisfied: unstructured-inference==0.7.23 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (0.7.23)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (0.3.13)\n",
            "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.0.16)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.24.7)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.10.0.84)\n",
            "Requirement already satisfied: onnxruntime<1.16 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.15.1)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.44.2)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (43.0.3)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (0.2.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (0.27.2)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.0.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.6.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (2.9.2)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.0.0)\n",
            "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (0.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.16.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]==0.12.3) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]==0.12.3) (10.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured==0.12.3->unstructured[pdf]==0.12.3) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured==0.12.3->unstructured[pdf]==0.12.3) (3.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3->unstructured[pdf]==0.12.3) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3->unstructured[pdf]==0.12.3) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3->unstructured[pdf]==0.12.3) (4.66.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->unstructured[pdf]==0.12.3) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]==0.12.3) (3.4.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[pdf]==0.12.3) (1.2.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.12.3->unstructured[pdf]==0.12.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.12.3->unstructured[pdf]==0.12.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.12.3->unstructured[pdf]==0.12.3) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (0.14.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.0->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.9.0->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (2.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2024.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<0.10.0,>=0.9.0->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.2.2)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.11.4)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.3.13)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.20.0+cu121)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (2.22)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3->unstructured[pdf]==0.12.3) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (10.0)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.0.11)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.0.8)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.3.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.10.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2024.2)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.30.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.9.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.1.5\n",
        "!pip install --quiet langchain_experimental\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers\n",
        "!pip install unstructured==0.12.3\n",
        "!pip install unstructured[pdf]==0.12.3\n",
        "!pip install tiktoken\n",
        "!pip install huggingface_hub\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = \"hf_yourtoken\"\n",
        "login(token=hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "id": "Wii5r_8yAlvv",
        "outputId": "fa9c47d0-669b-408e-e31c-cf69d9e28c26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.13)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.6.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5agp6iRm-7jD"
      },
      "source": [
        "### 2. Choose a dataset to use and then load it into your code\n",
        "Here we are using the pdfs loaded in pdfs/. We load this using langchain DirectoryLoader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBx0cQJ9Jmtv"
      },
      "source": [
        "We can load multiple types of datasets into this example though the most commonly used are PDFs and websites.\n",
        "\n",
        "To load websites, we could also use `langchain WebBaseLoader`\n",
        "\n",
        "In this example, we will consider PDFs and load them in using `langchain DirectoryLoader`.\n",
        "\n",
        "We host all PDFs at the PDFs directory `ai-science-training-series/05_llm_part2/PDFs`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8YpJK93pNsZ",
        "outputId": "af002684-7fbc-495f-e781-90116a6bb1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai-science-training-series'...\n",
            "remote: Enumerating objects: 3684, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 3684 (delta 7), reused 21 (delta 2), pack-reused 3653 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3684/3684), 391.62 MiB | 25.10 MiB/s, done.\n",
            "Resolving deltas: 100% (1878/1878), done.\n",
            "Updating files: 100% (417/417), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/argonne-lcf/ai-science-training-series.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDkvdEwLAlvv"
      },
      "source": [
        "The below code is the langchain based document loader; however, recently this showed failure. We are working to fix it but we have an alternative code to run using bash commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YYoEp4FJz6oK",
        "scrolled": true,
        "outputId": "f06b1135-fe8a-4108-ecb5-ac41a58ef535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s][nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "100%|██████████| 7/7 [00:36<00:00,  5.27s/it]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "loader = DirectoryLoader('ai-science-training-series/05_llm_part2/PDFs', glob=\"**/*.pdf\", show_progress=True)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd4ZlinFAlvv"
      },
      "outputs": [],
      "source": [
        "# from pypdf import PdfReader\n",
        "# x = !ls llm-workshop/tutorials/04-rag/PDFs/*.pdf\n",
        "# print(x)\n",
        "# # creating a pdf reader object\n",
        "# documents = []\n",
        "# for f in x:\n",
        "#   reader = PdfReader('llm-workshop/tutorials/04-rag/PDFs/1703.07076.pdf')\n",
        "\n",
        "#   # printing number of pages in pdf file\n",
        "#   print(len(reader.pages))\n",
        "\n",
        "#   # creating a page object\n",
        "#   page = reader.pages[0]\n",
        "\n",
        "#   # extracting text from page\n",
        "#   # print(page.extract_text())\n",
        "#   documents.append(page.extract_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjvyEOMMBB7G"
      },
      "source": [
        "### 3. Now, we need to split our documents into chunks.\n",
        "We want the embedding to be greater than 1 word but much less than an entire page. This is essential for the similarity search between the query and the document. Essentially, the query will be searched for greatest similarity to embedded chunks in the dataset. Then those chunks with greatest similarity are augmented to the query.\n",
        "\n",
        "It is essential to choose the chunking method according to your data type.\n",
        "There are different ways to do this:\n",
        "\n",
        "Fixed size\n",
        "*   Token: Splits text on tokens. Can chunk tokens together\n",
        "*   Character: Splits based on some user defined character.\n",
        "\n",
        "Recursive\n",
        "*  Recursively splits text. Useful for keeping related pieces of text next to each other.\n",
        "\n",
        "Document based\n",
        "*   HTML: Splits text based on HTML-specific characters.\n",
        "*   Markdown: Splits on Markdown-specific characters\n",
        "*   Code: Splits text based on characters specific to coding languages.\n",
        "\n",
        "Semantic chunking\n",
        "*   Extract semantic meaning from embeddings and then assess the semantic relationship between these chunks. Essentially splits into sentences, then groups into groups of 3 sentences, and then merges one that are similar in the embedding space.\n",
        "\n",
        "Here we use recursive where the dataset is split using a set of characters. The default characters provided to it are [\"\\n\\n\", \"\\n\", \" \", \"\"].  A large text is split by the first character \\n\\n. If the first split by \\n\\n is still large then it moves to the next character which is \\n and tries to split by it. This continues until the chunk size is reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W9q3y9FIAlvv"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "#documents = text_splitter.create_documents(documents)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "id": "a3Mlw-FgAlvv",
        "outputId": "c15e29da-a3ae-4360-dd9e-c9c4d2368617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='De novo design of protein structure and function with RFdiffusion\\n\\nhttps://doi.org/10.1038/s41586-023-06415-8\\n\\nReceived: 14 December 2022\\n\\nAccepted: 7 July 2023\\n\\nPublished online: 11 July 2023\\n\\nOpen access\\n\\nJoseph L. Watson1,2,15, David Juergens1,2,3,15, Nathaniel R. Bennett1,2,3,15, Brian L. Trippe2,4,5,15, Jason Yim2,6,15, Helen E. Eisenach1,2,15, Woody Ahern1,2,7,15, Andrew J. Borst1,2, Robert J. Ragotte1,2, Lukas F. Milles1,2, Basile I. M. Wicky1,2, Nikita Hanikel1,2, Samuel J. Pellock1,2, Alexis Courbet1,2,8, William Sheffler1,2, Jue Wang1,2, Preetham Venkatesh1,2,9, Isaac Sappington1,2,9, Susana Vázquez Torres1,2,9, Anna Lauko1,2,9, Valentin De Bortoli8, Emile Mathieu10, Sergey Ovchinnikov11,12, Regina Barzilay6, Tommi S. Jaakkola6, Frank DiMaio1,2, Minkyung Baek13 & David Baker1,2,14\\u2009✉\\n\\nCheck for updates'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='There has been considerable recent progress in designing new proteins using deep- learning methods1–9. Despite this progress, a general deep-learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher-order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of protein backbone geometry and sequence–structure relationships. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology- constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal- binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='De novo protein design seeks to generate proteins with specified structural and/or functional properties, for example, making a bind- ing interaction with a given target12, folding into a particular topology13 or containing a catalytic site4. Denoising diffusion probabilistic models (DDPMs), a powerful class of machine learning models recently dem- onstrated to generate new photorealistic images in response to text prompts14,15, have several properties well suited to protein design. First, DDPMs generate highly diverse outputs, as they are trained to denoise data (for instance, images or text) that have been corrupted with Gauss- ian noise. By learning to stochastically reverse this corruption, diverse outputs closely resembling the training data are generated. Second, DDPMs can be guided at each step of the iterative generation process towards specific design objectives through provision of conditioning'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='information. Third, for almost all protein design applications it is neces- sary to explicitly model three-dimensional (3D) structures; rotation- ally equivariant DDPMs can do this in a global representation frame independent manner. Recent work has adapted DDPMs for protein monomer design by conditioning on small protein ‘motifs’5,9 or on sec- ondary structure and block-adjacency (‘fold’) information8. Although promising, these attempts have shown limited success in generating sequences that fold to the intended structures in silico5,16, probably due to the limited ability of the denoising networks to generate realistic protein backbones, and have not been tested experimentally.\\n\\nWe reasoned that improved diffusion models for protein design could be developed by taking advantage of the deep understanding of protein structure implicit in powerful structure prediction methods'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='1Department of Biochemistry, University of Washington, Seattle, WA, USA. 2Institute for Protein Design, University of Washington, Seattle, WA, USA. 3Graduate Program in Molecular Engineering, University of Washington, Seattle, WA, USA. 4Columbia University, Department of Statistics, New York, NY, USA. 5Irving Institute for Cancer Dynamics, Columbia University, New York, NY, USA. 6Massachusetts Institute of Technology, Cambridge, MA, USA. 7Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, USA. 8National Centre for Scientific Research, École Normale Supérieure rue d’Ulm, Paris, France. 9Graduate Program in Biological Physics, Structure and Design, University of Washington, Seattle, WA, USA. 10Department of Engineering, University of Cambridge, Cambridge, UK. 11Faculty of Applied Sciences, Harvard University, Cambridge, MA, USA. 12John Harvard Distinguished Science Fellowship, Harvard University, Cambridge, MA, USA. 13School of Biological'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='University, Cambridge, MA, USA. 12John Harvard Distinguished Science Fellowship, Harvard University, Cambridge, MA, USA. 13School of Biological Sciences, Seoul National University, Seoul, Republic of Korea. 14Howard Hughes Medical Institute, University of Washington, Seattle, WA, USA. 15These authors contributed equally: Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern. ✉e-mail: dabaker@uw.edu'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Nature | Vol 620 | 31 August 2023 | 1089'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='such as AlphaFold2 (ref. 17) (AF2) and RoseTTAFold18 (RF). RF has prop- erties well suited for use in a protein design DDPM (Fig. 1a): it gener- ates protein structures with high precision, operates on a rigid-frame representation of residues with rotational equivariance and has an architecture enabling conditioning on design specifications at the individual residue, inter-residue distance and orientation, and 3D coordinate levels. In previous work, we fine-tuned RF to complete protein backbones around input functional motifs in a single step (RFjoint Inpainting4). Experimental characterization showed that the method can scaffold a wide range of protein functional motifs with atomic accuracy19, but the approach fails on minimalist site descrip- tions that do not sufficiently constrain the overall fold and, because it is deterministic, can produce only a limited diversity of designs for a given problem. We reasoned that by fine-tuning RF as the denoising net- work in a generative'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='can produce only a limited diversity of designs for a given problem. We reasoned that by fine-tuning RF as the denoising net- work in a generative diffusion model instead, we could overcome both problems: because the starting point is random noise, each denoising trajectory yields a different solution, and because structure is built up progressively through many denoising iterations, little to no starting structural information should be required. In this study, we used an updated version of RF18 as the basis for the denoising network archi- tecture (Supplementary Methods), but other equivariant structure prediction networks (AF2 (ref. 17), OmegaFold20, ESMFold21) could in principle be substituted into an analogous DDPM.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='We construct a RF-based diffusion model, RFdiffusion, using the RF frame representation that comprises a Cα coordinate and N-Cα-C rigid orientation for each residue. We generate training inputs by noising structures sampled from the Protein Data Bank (PDB) for up to 200 steps22. For translations, we perturb Cα coordinates with 3D Gaussian noise. For residue orientations, we use Brownian motion on the mani- fold of rotation matrices (building on refs. 23,24). To enable RFdiffusion to learn to reverse each step of the noising process, we train the model by minimizing a mean-squared error (m.s.e.) loss between frame pre- dictions and the true protein structure (without alignment), averaged across all residues (Supplementary Methods). This loss drives denoising trajectories to match the data distribution at each timestep and hence to converge on structures of designable protein backbones (Extended Data Fig. 2a). The m.s.e. contrasts to the loss used in RF structure predic- tion training'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='on structures of designable protein backbones (Extended Data Fig. 2a). The m.s.e. contrasts to the loss used in RF structure predic- tion training (frame aligned point error or FAPE) in that, unlike FAPE, m.s.e. loss is not invariant to the global reference frame and therefore promotes continuity of the global coordinate frame between timesteps (Supplementary Methods).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='To generate a new protein backbone, we first initialize random resi- due frames and RFdiffusion makes a denoised prediction. Each residue frame is updated by taking a step in the direction of this prediction with some noise added to generate the input to the next step. The nature of the noise added and the size of this reverse step is chosen such that the denoising process matches the distribution of the noising process (Supplementary Methods and Extended Data Fig. 2a). RFdiffusion initially seeks to match the full breadth of possible protein structures compatible with the purely random frames with which it is initialized, and hence the denoised structures do not initially seem protein-like (Fig. 1c, left). However, through many such steps, the breadth of pos- sible protein structures from which the input could have arisen narrows and RFdiffusion predictions come to closely resemble protein struc- tures (Fig. 1c, right). We use the ProteinMPNN network1 to subsequently design sequences'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='predictions come to closely resemble protein struc- tures (Fig. 1c, right). We use the ProteinMPNN network1 to subsequently design sequences encoding these structures, typically sampling eight sequences per design in line with previous work5,16 (but see Supplemen- tary Fig. 2a). We also considered simultaneously designing structure and sequence within RFdiffusion, but given the excellent performance of combining ProteinMPNN with the diffusion of structure alone, we did not extensively explore this possibility.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Figure 1a highlights the similarities between RF structure predic- tion and an RFdiffusion denoising step: in both cases, the networks transform coordinates into a predicted structure, conditioned on inputs to the model. In RF, sequence is the primary input, with extra\\n\\nstructural information provided as templates and initial coordinates to the model. In RFdiffusion, the primary input is the noised coordinates from the previous step. For specific design tasks, a range of auxiliary conditioning information, including partial sequence, fold informa- tion or fixed functional-motif coordinates can be provided (Fig. 1b and Supplementary Methods).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='We explored two different strategies for training RFdiffusion: (1) in a manner akin to ‘canonical’ diffusion models, with predictions at each timestep independent of predictions at previous timesteps (as in previous work5,8,9,16), and (2) with self-conditioning25, in which the model can condition on previous predictions between timesteps (Fig. 1a, bottom row and Supplementary Methods). The latter strategy was inspired by the success of ‘recycling’ in AF2, which is also central to the more recent RF model used here (Supplementary Methods). Self-conditioning within RFdiffusion notably improved performance on in silico benchmarks encompassing both conditional and uncondi- tional protein design tasks (Fig. 2e and Extended Data Fig. 1e). Increased coherence of predictions within self-conditioned trajectories may, at least in part, explain these performance increases (Extended Data Fig. 1h). Fine-tuning RFdiffusion from pretrained RF weights was far more successful than training for an'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='these performance increases (Extended Data Fig. 1h). Fine-tuning RFdiffusion from pretrained RF weights was far more successful than training for an equivalent length of time from untrained weights (Extended Data Fig. 1f,g, also Supplementary Fig. 1) and the m.s.e. loss was also crucial for unconditional generation (Extended Data Fig. 1d). For all in silico benchmarks in this paper, we use the AF2 structure prediction network17 for validation and define an in silico ‘success’ as an RFdiffusion output for which the AF2 structure predicted from a single sequence is (1) of high confidence (mean pre- dicted aligned error (pAE), less than five), (2) globally within a 2\\u2009Å back- bone root mean-squared deviation (r.m.s.d.) of the designed structure and (3) within 1\\u2009Å backbone r.m.s.d. on any scaffolded functional site (Supplementary Methods). This measure of in silico success has been found to correlate with experimental success4,7,26 and is significantly more stringent than template'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='This measure of in silico success has been found to correlate with experimental success4,7,26 and is significantly more stringent than template modelling (TM)-score-based metrics used elsewhere5,16,27–29 (Supplementary Fig. 2c,d).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Unconditional protein monomer generation As shown in Fig. 2a–c and Supplementary Fig. 3c,d, starting from ran- dom noise, RFdiffusion can readily generate elaborate protein struc- tures with little overall structural similarity to structures seen during training, indicating considerable generalization beyond the PDB (see Supplementary Table 1 for a comparison of all designs in the paper to the PDB). The designs are diverse (Supplementary Fig. 3a), spanning a wide range of alpha, beta and mixed alpha–beta topologies, with AF2 and ESMFold (Fig. 2c, Extended Data Fig. 1b,c and Supplemen- tary Fig. 2b) predictions very close to the design structure models for de novo designs with as many as 600 residues. RFdiffusion generates plausible structures for even very large proteins, but these are difficult to validate in silico as they are probably generally beyond the single sequence prediction capabilities of AF2 and ESMFold. The quality and diversity of designs that are sampled are inherent'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='generally beyond the single sequence prediction capabilities of AF2 and ESMFold. The quality and diversity of designs that are sampled are inherent to the model, and do not depend on any auxiliary conditioning input (for example, second- ary structure information8). We experimentally characterized six of the 300 amino acid designs and three of the 200 amino acid designs, and found that they have circular dichroism spectra consistent with the mixed alpha–beta topologies of the designs and are extremely thermostable (Extended Data Fig. 3). Physics-based protein design methodologies have struggled in unconstrained generation of diverse protein monomers because of the difficulty of sampling on the very large and rugged conformational landscape30, and overcoming this limitation has been a primary test of deep-learning based protein design approaches5,6,8,16,27,31. RFdiffusion strongly outperforms (based on the AF2 success metric described above) Hallucination with RF, an experimentally'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='approaches5,6,8,16,27,31. RFdiffusion strongly outperforms (based on the AF2 success metric described above) Hallucination with RF, an experimentally validated method using Monte Carlo search or gradient descent to identify sequences predicted to fold into stable structures'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='1090 | Nature | Vol 620 | 31 August 2023\\n\\na\\n\\nDiffusion model\\n\\nForward (noising) process\\n\\nN(0,1)\\n\\nSingle step\\n\\nGaussian noise\\n\\n...\\n\\nProtein structure\\n\\nXT\\n\\nXt\\n\\nXt–1\\n\\nX0\\n\\nReverse (generative) process\\n\\nRoseTTAFold\\n\\nRFdiffusion\\n\\nInput sequence\\n\\nHomologous templates\\n\\nMADHTI?DTREE\\n\\nRF\\n\\nMasked input sequence ˆX0 (self- conditioning)\\n\\nt+1\\n\\n????????????\\n\\nRF\\n\\nInitial/recycled coordinates\\n\\nPredicted structure\\n\\nXt Diffused coordinates\\n\\nˆ X0\\n\\nSingle RFdiffusion step\\n\\nXt\\n\\nRF\\n\\nˆX0\\n\\nˆ interp(Xt, X0) + ε\\n\\nXt–1\\n\\nSelf-conditioning\\n\\nc\\n\\nt = 200\\n\\nt = 175\\n\\nt = 150\\n\\n) t u p n\\n\\ni (\\n\\nX\\n\\nn o i t c d e r p\\n\\ni\\n\\n(\\n\\n0 X ˆ'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='i\\n\\n(\\n\\n0 X ˆ\\n\\nFig. 1 | Protein design using RFdiffusion. a, Diffusion models for proteins are trained to recover corrupted (noised) protein structures and to generate new structures by reversing the corruption process through iterative denoising of initially random noise XT into a realistic structure X0 (top panel). The RF structure prediction network (middle panel, left side) is fine-tuned with minimal architectural changes into RFdiffusion (middle panel, right side); the denoising network of a DDPM is also shown. In RF, the primary input to the model is the sequence. In RFdiffusion, the primary input is diffused residue frames (coordinates and orientations). In both cases, the model predicts final 3D coordinates (denoted \\uf0b5X0 in RFdiffusion). The bottom panel shows that in RFdiffusion, the model receives its previous prediction as a template input (‘self-conditioning’, Supplementary Methods). At each timestep t of a trajectory (typically 200 steps), RFdiffusion takes \\uf0b5X 0\\n\\nt\\n\\n+1'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='t\\n\\n+1\\n\\nfrom the previous step and Xt and\\n\\n(Fig. 2d). RFdiffusion generation is also more compute efficient than unconstrained Hallucination with RF, and efficiency can be greatly improved by taking larger steps at inference time and by truncating tra- jectories early, which is possible because RF predicts the final structure at each timestep (Extended Data Fig. 2b,c). For example, a 100-residue\\n\\nb\\n\\nXT\\n\\nX0\\n\\nUnconditional\\n\\nSymmetric noise\\n\\nSymmetric oligomers\\n\\nBinding target\\n\\nBinder design\\n\\nFunctional motif\\n\\nMotif scaffolding\\n\\nSymmetric motif\\n\\nSymmetric scaffolding\\n\\nt = 125\\n\\nt = 100\\n\\nt = 1\\n\\nt'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Functional motif\\n\\nMotif scaffolding\\n\\nSymmetric motif\\n\\nSymmetric scaffolding\\n\\nt = 125\\n\\nt = 100\\n\\nt = 1\\n\\nt\\n\\n\\uf0b5 ). The next coordinate input to then predicts an updated X0 structure (X 0 t the model (Xt−1) is generated by a noisy interpolation (interp) towards \\uf0b5X 0. b, RFdiffusion is broadly applicable for protein design. RFdiffusion generates protein structures either without further input (top row) or by conditioning on (top to bottom): symmetry specifications; binding targets; protein functional motifs or symmetric functional motifs. In each case random noise, along with conditioning information, is input to RFdiffusion, which iteratively refines that noise until a final protein structure is designed. c, An example of an unconditional design trajectory for a 300-residue chain, depicting the input to \\uf0b5 prediction. At early timesteps (high t), the model (Xt) and the corresponding X0 \\uf0b5 bears little resemblance to a protein but is gradually refined into a realistic X0 protein structure.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='protein can be generated in as little as 11\\u2009s on an NVIDIA RTX A4000 Graphical Processing Unit, in contrast to RF Hallucination, which takes around 8.5\\u2009min.\\n\\nIt is often desirable to be able to specify a protein fold during design (such as triose-phosphate isomerase (TIM) barrels or cavity-containing\\n\\nNature | Vol 620 | 31 August 2023 | 1091\\n\\na\\n\\n300 amino acids\\n\\n600 amino acids\\n\\nDesign\\n\\nAF2\\n\\nb\\n\\nTop TMAlign match to PDB\\n\\nc\\n\\nr.m.s.d. design vs AlphaFold2\\n\\nd\\n\\nr.m.s.d. AlphaFold2 vs design\\n\\ne\\n\\nAblations reveal the determinants of RFdiffusion performance\\n\\nB D P o t\\n\\ne r o c s M T t s e h g H\\n\\n0.9\\n\\n0.8\\n\\n0.7\\n\\n0.6\\n\\n0.5\\n\\n0.4\\n\\nÅ\\n\\n(\\n\\nd . s . m\\n\\n. r\\n\\nn g s e d s u s r e v\\n\\ni\\n\\n2 F A\\n\\n40\\n\\n35\\n\\n30\\n\\n25\\n\\n20\\n\\n15\\n\\n10\\n\\n5\\n\\n)\\n\\nÅ\\n\\nd . s . m\\n\\n. r\\n\\n5.0\\n\\n2.5\\n\\n0\\n\\nZoom\\n\\n400 300 200 No. of amino acids\\n\\n100\\n\\nÅ\\n\\n(\\n\\nd . s . m\\n\\n. r\\n\\nn g s e d s u s r e v\\n\\ni\\n\\n2 F A\\n\\n30\\n\\n25\\n\\n20\\n\\n15\\n\\n10\\n\\n5\\n\\nZoom\\n\\n)\\n\\n2\\n\\nÅ\\n\\n.\\n\\nd . s . m\\n\\n1\\n\\n. r\\n\\n0\\n\\n70 No. of amino acids\\n\\n100\\n\\nDesign method\\n\\nHallucination RFdiffusion\\n\\ny t i s n e D\\n\\n0.25'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='25\\n\\n20\\n\\n15\\n\\n10\\n\\n5\\n\\nZoom\\n\\n)\\n\\n2\\n\\nÅ\\n\\n.\\n\\nd . s . m\\n\\n1\\n\\n. r\\n\\n0\\n\\n70 No. of amino acids\\n\\n100\\n\\nDesign method\\n\\nHallucination RFdiffusion\\n\\ny t i s n e D\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.10\\n\\n0.05\\n\\nRFdiffusion No pretraining (eq. compute) No fine-tuning (RF weights) No MSE Loss No self-conditioning\\n\\n0.3\\n\\n100 200 300 400 600 800 1,000\\n\\n0\\n\\n100 200 300 400 600 800 1,000\\n\\n0\\n\\n70\\n\\n100\\n\\n200\\n\\n300\\n\\n0\\n\\n0\\n\\n10\\n\\n20\\n\\n30\\n\\nNumber of amino acids\\n\\nNumber of amino acids\\n\\nNumber of amino acids\\n\\nr.m.s.d. AF2 versus design (Å)\\n\\nf\\n\\nCD spectra\\n\\ng\\n\\nTIM barrel\\n\\nCD spectra\\n\\n×104\\n\\n6WVS\\n\\n1.0\\n\\n×104\\n\\n3 _ a a 0 0 3\\n\\n) 1 – l o m d 2\\n\\nm c g e d\\n\\n1\\n\\n0\\n\\n–1\\n\\n300aa_3 300aa_8\\n\\n) 1 – l o m d 2\\n\\nm c g e d\\n\\n0.5\\n\\n0\\n\\n6WVS TIM_barrel_6\\n\\nE R M\\n\\n–2\\n\\nE R M\\n\\n–0.5\\n\\n–1.0\\n\\n200\\n\\n225\\n\\nWavelength (nm)\\n\\n250\\n\\n200\\n\\n225 Wavelength (nm)\\n\\n250\\n\\nTIM_barrel_6\\n\\n×104\\n\\nCD melts\\n\\n1.0\\n\\n×104\\n\\nCD melts\\n\\n8 _ a a 0 0 3\\n\\n) 1 – l o m d 2\\n\\nm c g e d\\n\\n1\\n\\n0\\n\\n–1\\n\\n300aa_3 300aa_8\\n\\n) 1 – l o m d 2\\n\\nm c g e d\\n\\n0.5\\n\\n0\\n\\n–0.5\\n\\n6WVS TIM_barrel_6\\n\\n2 2 2 E R M\\n\\n–2\\n\\nDesign\\n\\n2 2 2 E R M\\n\\n–1.0'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='m c g e d\\n\\n1\\n\\n0\\n\\n–1\\n\\n300aa_3 300aa_8\\n\\n) 1 – l o m d 2\\n\\nm c g e d\\n\\n0.5\\n\\n0\\n\\n–0.5\\n\\n6WVS TIM_barrel_6\\n\\n2 2 2 E R M\\n\\n–2\\n\\nDesign\\n\\n2 2 2 E R M\\n\\n–1.0\\n\\nDesign\\n\\nAF2\\n\\n25 35 45 55 65 75 85 95 Temperature (ºC)\\n\\nAF2\\n\\n–1.5\\n\\n25 35 45 55 65 75 85 95 Temperature (ºC)'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Fig. 2 | Outstanding performance of RFdiffusion for monomer generation. a, RFdiffusion can generate new monomeric proteins of different lengths (left 300, right 600) with no conditioning information. Grey, design model; colours, AF2 prediction. r.m.s.d. AF2 versus design (Å), left to right: 0.90, 0.98, 1.15, 1.67. b, Unconditional designs from RFdiffusion are new and not present in the training set as quantified by highest TM-score to the PDB; the divergence from previously known structures increases with length. c, Unconditional samples are closely repredicted by AF2 up to about 400 amino acids. d, RFdiffusion significantly outperforms Hallucination (with RF) at unconditional monomer generation (two-proportion z-test of in silico success: n\\u2009=\\u2009400 designs per condition, z\\u2009=\\u20099.5, P\\u2009=\\u20091.6\\u2009×\\u200910−21). Although Hallucination successfully generates designs up to 100 amino acids in length, in silico success rates rapidly deteriorate beyond this length. e, Ablating pretraining (by starting'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='designs up to 100 amino acids in length, in silico success rates rapidly deteriorate beyond this length. e, Ablating pretraining (by starting from untrained RF), RFdiffusion fine-tuning (that is, using original RF structure'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='prediction weights as the denoiser), self-conditioning or m.s.e. losses (by training with FAPE) each notably decrease the performance of RFdiffusion. r.m.s.d. between design and AF2 is shown, for the unconditional generation of 300 amino acid proteins (Supplementary Methods). f, Two example 300 amino acid proteins that expressed as soluble monomers. Designs (grey) overlaid with AF2 predictions (colours) are shown on the left, alongside circular dichroism (CD) spectra (top) and melt curves (bottom) on the right. The designs are highly thermostable. g, RFdiffusion can condition on fold information. An example TIM barrel is shown (bottom left), conditioned on the secondary structure and block adjacency of a previously designed TIM barrel, PDB 6WVS (top left). Designs have very similar circular dichroism spectra to PDB 6WVS (top right) and are highly thermostable (bottom right). See also Extended Data Fig. 3 for further traces. Boxplots represent median\\u2009±\\u2009interquartile range; tails are'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='are highly thermostable (bottom right). See also Extended Data Fig. 3 for further traces. Boxplots represent median\\u2009±\\u2009interquartile range; tails are minimum and maximum excluding outliers (±1.5× interquartile range).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='NTF2s for small molecule binder and enzyme design32,33), and thus we further fine-tuned RFdiffusion to condition on secondary structure and/or fold information, enabling rapid and accurate generation of\\n\\ndiverse designs with the desired topologies (Fig. 2g and Extended Data Fig. 4). In silico success rates were 42.5 and 54.1% for TIM barrels and NTF2 folds, respectively (Extended Data Fig. 4d), and experimental\\n\\n1092 | Nature | Vol 620 | 31 August 2023\\n\\ncharacterization of 11 TIM barrel designs indicated that at least eight designs were soluble, thermostable and had circular dichroism spectra consistent with the design model (Fig. 2g and Extended Data Fig. 4e,f).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Design of higher-order oligomers There is considerable interest in designing symmetric oligomers, which can serve as vaccine platforms34, delivery vehicles35 and catalysts36. Cyclic oligomers have been designed using structure prediction net- works with an adaptation of Hallucination that searches for sequences predicted to fold to the desired cyclic symmetry, but this approach fails for higher-order dihedral, tetrahedral, octahedral and icosahedral symmetries, probably in part because of the much lower representation of such structures in the PDB7.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='We set out to generalize RFdiffusion to create symmetric oligomeric structures with any specified point group symmetry. Given a specifica- tion of a point group symmetry for an oligomer with n chains, and the monomer chain length, we generate random starting residue frames for a single monomer subunit as in the unconditional generation case, and then generate n\\u2009−\\u20091 copies of this starting point arranged with the specified point group symmetry. Because RFdiffusion is equivariant (inherited from RF) with respect to rotation and relabelings of chains, symmetry is largely maintained in the denoising predictions; we explic- itly resymmetrize at each step but this changes the structures only slightly (compare grey and coloured chains in Extended Data Fig. 5a and Supplementary Methods). For octahedral and icosahedral archi- tectures, we explicitly model only the smallest subset of monomers required to generate the full assembly (for example, for icosahedra, the subunits at the five-, three-'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='model only the smallest subset of monomers required to generate the full assembly (for example, for icosahedra, the subunits at the five-, three- and twofold symmetry axes) to reduce the computational cost and memory footprint.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Despite not being trained on symmetric inputs, RFdiffusion is able to generate symmetric oligomers with high in silico success rates (Extended Data Fig. 5b), particularly when guided by an auxiliary inter- and intrachain contact potential (Extended Data Fig. 5c). As illustrated in Fig. 3 and Extended Data Fig. 5e, RFdiffusion designs are nearly indis- tinguishable from AF2 predictions of the structures adopted by the designed sequences, and many show little resemblance to previously solved protein structures (Extended Data Fig. 5d and Supplementary Table 1). Several of the oligomeric topologies are not seen in the PDB, including two-layer beta barrels (Fig. 3a, C10 symmetry) and complex mixed alpha/beta topologies (Fig. 3a, C8 symmetry; closest TM align in PDB 6BRP, 0.47, and PDB 6BRO, 0.43, respectively).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='We selected 608 designs for experimental characterization and found using size-exclusion chromatography (SEC) that at least 87 had oligomerization states closely consistent with the design mod- els (within the 95% confidence interval, 126 designs within the 99% confidence interval, as determined by SEC calibration curves; Sup- plementary Figs. 4 and 5). We took advantage of the increased size of these oligomers (compared to the smaller unconditional and fold-conditioned monomers described above) and collected nega- tive stain electron microscopy (nsEM) data on a subset of these designs across different symmetry groups. For most, distinct particles were evident with shapes resembling the design models in both the raw micrographs and subsequent two-dimensional (2D) classifications (Fig. 3 and Extended Data Fig. 5f). nsEM characterization of a C3 design (HE0822) with 350 residue subunits (1,050 residues in total) suggests that the actual structure is very close to the design, both over'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='of a C3 design (HE0822) with 350 residue subunits (1,050 residues in total) suggests that the actual structure is very close to the design, both over the 350 residue subunits and the overall C3 architecture. 2D class averages are clearly consistent with both top and side views of the design model, and a 3D reconstruction of the density has key features consistent with the design, including the distinctive pinwheel shape (Fig. 3b, top row). Electron microscopy 2D class averages of C5 and C6 designs with more than 750 residues (HE0794, HE0789, HE0841) were also consistent with the respective design models (Extended Data Fig. 5f).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='RFdiffusion also generated cyclic oligomers with alpha and/or beta barrel structures that resemble expanded TIM barrels and provide an interesting comparison between innovation during natural evolution and innovation through deep learning. The TIM barrel fold, with eight strands and eight helices, is one of the most abundant folds in nature37. nsEM confirmed the structure of two RFdiffusion designed cyclic oli- gomers, which considerably extend beyond this fold (Fig. 3b, bottom rows). HE0626 is a C6 alpha–beta barrel composed of 18 strands and 18 helices, and HE0675 is a C8 octamer composed of an inner ring of 16 strands and an outer ring of 16 helices arranged locally in a very similar repeating pattern to the TIM barrel (1:1 helix:strand). For both HE0626 and HE0675 we obtained nsEM 3D reconstructions that are in agree- ment with the computational design models. The HE0600 design is also an alpha–beta barrel (Extended Data Fig. 5f), but has two strands for every helix (24 strands'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='computational design models. The HE0600 design is also an alpha–beta barrel (Extended Data Fig. 5f), but has two strands for every helix (24 strands and 12 helices in total) and hence is locally different from a TIM barrel. Whereas natural evolution has extensively explored structural variations of the classic eight-strand or eight-helix TIM barrel fold, RFdiffusion can more readily explore global changes in barrel curvature, enabling discovery of TIM barrel-like structures with many more helices and strands.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='RFdiffusion also readily generated structures with dihedral, tet- rahedral and icosohedral symmetries (Fig. 3c,d and Extended Data Fig. 5e,f). SEC characterization indicated that 38 D2, seven D3 and three D4 designs had the expected molecular weights (these have four, six and eight chains, respectively) (Supplementary Fig. 5). Although the D2 dihedrals are too small for nsEM, 2D class averages—and for some, 3D reconstructions of D3 and D4 designs—were congruent with the overall topologies of the design models (Fig. 3c and Extended Data Fig. 5f). Similarly, 3D reconstruction (Fig. 3c) and cryogenic electron microscopy (cryo-EM) 2D class averages (Extended Data Fig. 5g and Sup- plementary Fig. 6) of the D4 HE0537 closely match the design model, recapitulating the roughly 45° offset between tetramic subunits. 2D nsEM class averages for a 12-chain tetrahedron (HE0964) were consist- ent with the design model (Extended Data Fig. 5f). Forty-eight icosa- hedra were selected for experimental'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='tetrahedron (HE0964) were consist- ent with the design model (Extended Data Fig. 5f). Forty-eight icosa- hedra were selected for experimental validation, and one, HE0902, a 15\\u2009nm (diameter) highly porous assembly (Fig. 3d, left) was observed in nsEM micrographs to form homogeneous particles. 2D class averages and a 3D reconstruction very closely match the design model (Fig. 3d), with triangular hubs arrayed around the empty C5 axes. Designs such as HE0902 (and future similar large assemblies) should be useful as new nanomaterials and vaccine scaffolds, with robust assembly and (in the case of HE0902) the outward facing N and C termini offering many possibilities for antigen display.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Functional-motif scaffolding We next investigated the use of RFdiffusion for scaffolding protein structural motifs that carry out binding and catalytic functions, in which the role of the scaffold is to hold the motif in precisely the 3D geometry needed for optimal function. In RFdiffusion, we input motifs as 3D coordinates (including sequence and sidechains) both during conditional training and inference, and build scaffolds that hold the motif atomic coordinates in place. Many deep-learning methods have been developed recently to address this problem, including RFjoint Inpainting4, constrained Hallucination4 and other DDPMs5,8,29. To rigorously evaluate the performance of these methods in comparison to RFdiffusion across a broad set of design challenges, we established an in silico benchmark test (Supplementary Table 9) comprising 25 motif-scaffolding design problems addressed in six recent publications encompassing several design methodologies4,5,29,38–40. The challenges span a'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='motif-scaffolding design problems addressed in six recent publications encompassing several design methodologies4,5,29,38–40. The challenges span a broad range of motifs, including simple ‘inpainting’ problems, viral epitopes, receptor traps, small molecule binding sites, binding interfaces and enzyme active sites.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='RFdiffusion solves 23 of the 25 benchmark problems, compared to 15 for Hallucination and 19 for RFjoint Inpainting (Fig. 4a,b). For 19 out\\n\\nNature | Vol 620 | 31 August 2023 | 1093\\n\\na\\n\\nb\\n\\nRFdiffusion\\n\\nAF2\\n\\n2D class averages\\n\\n3D reconstruction\\n\\nD2\\n\\n90º\\n\\nHE0822 C3\\n\\nC6\\n\\n90º\\n\\nHE0626 C6\\n\\nC8\\n\\nHE0675 C8\\n\\n90º\\n\\nc\\n\\nC10\\n\\nHE0490 D3\\n\\n90º\\n\\nO\\n\\nHE0537 D4\\n\\n90º\\n\\nd\\n\\nHE0902 HE0902\\n\\nRepresentative Representative micrograph micrograph\\n\\nC3 C3\\n\\nC2 C2\\n\\nC5 C5\\n\\n50 nm\\n\\n90 A\\n\\nm n\\n\\n5 1'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Representative Representative micrograph micrograph\\n\\nC3 C3\\n\\nC2 C2\\n\\nC5 C5\\n\\n50 nm\\n\\n90 A\\n\\nm n\\n\\n5 1\\n\\nFig. 3 | Design and experimental characterization of symmetric oligomers. a, RFdiffusion-generated assemblies overlaid with the AF2 structure predictions based on the designed sequences; in all five cases they are nearly indistinguishable (for the octahedron (bottom), the prediction was for the C3 substructure). Symmetries are indicated to the left of the design models. b,c, Designed assemblies characterized by nsEM. Model symmetries are as follows: cyclic, C3 (HE0822, 350 amino acids (AA) per chain), C6 (HE0626, 100 AA per chain) and C8 (HE0675, 60 AA per chain) (b); dihedral, D3 (HE0490, 80 AA per chain) and D4 (HE0537, 100 AA per chain) (c). From left to right: (1) symmetric design model, (2) AF2 prediction of design following sequence design with ProteinMPNN, (3) 2D class averages showing both top and side views (scale bar, 60\\u2009Å for all class averages) and (4) 3D reconstructions from'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='class averages with the design model fit into the density map. The overall shapes are consistent with the design models, and confirm the intended oligomeric state. As in a, AF2 predictions of each design are nearly indistinguishable from the design model (backbone r.m.s.d.s (Å) for HE0822, HE0626, HE0490, HE0675 and HE0537, are 1.33, 1.03, 0.60, 0.74 and 0.75, respectively). d, nsEM characterization of an icosahedral particle (HE0902, 100 AA per chain). The design model, including the AF2 prediction of the C3 subunit are shown on the left. nsEM data are shown on the right: on top, a representative micrograph is shown alongside 2D class averages along each symmetry axis (C3, C2 and C5, from left to right) with the corresponding 3D reconstruction map views shown directly below overlaid on the design model.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='of 23 of the problems solved by RFdiffusion, the fraction of successful designs is higher than either Hallucination or RFjoint Inpainting. The excellent performance of RFdiffusion required no hyperparameter tun- ing or external potentials; this contrasts with Hallucination, for which\\n\\nproblem-specific optimization can be required. In 17 out of 23 of the problems, RFdiffusion-generated successful solutions with higher in silico success rates when noise was not added during the reverse diffu- sion trajectories (see Extended Data Fig. 1i for further discussion on the\\n\\n1094 | Nature | Vol 620 | 31 August 2023\\n\\na\\n\\nb\\n\\nRFdiffusion outperforms hallucination and RFjoint\\n\\n5TRV long\\n\\n7MRX 128\\n\\n1.0\\n\\nDiffusion, noise = 0\\n\\nDiffusion, noise = 1\\n\\n0.8\\n\\nHallucination, MPNN\\n\\n)\\n\\n%\\n\\ne t a r\\n\\n0.6\\n\\nHallucination, no MPNN\\n\\nRFjoint, MPNN\\n\\nRFjoint, no MPNN\\n\\ns s e c c u s o c\\n\\n0.4\\n\\n6E6R long\\n\\n5TPN\\n\\ni l i\\n\\ns\\n\\nn\\n\\n0.2\\n\\nI\\n\\n0\\n\\nF C B 1\\n\\nd e m _ R 6 E 6\\n\\n8 L K 2\\n\\ng n o _ R 6 E 6\\n\\nl\\n\\ng n o _ Z X E 6\\n\\nl\\n\\nR C Y 1'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='s s e c c u s o c\\n\\n0.4\\n\\n6E6R long\\n\\n5TPN\\n\\ni l i\\n\\ns\\n\\nn\\n\\n0.2\\n\\nI\\n\\n0\\n\\nF C B 1\\n\\nd e m _ R 6 E 6\\n\\n8 L K 2\\n\\ng n o _ R 6 E 6\\n\\nl\\n\\ng n o _ Z X E 6\\n\\nl\\n\\nR C Y 1\\n\\n1 W V 6\\n\\nN P T 5\\n\\nd e m _ Z X E 6\\n\\nP Y Z 4\\n\\nt r o h s _ R 6 E 6\\n\\ng n o _ V R T 5\\n\\nl\\n\\nT X 3\\n\\nI\\n\\nd e m _ V R T 5\\n\\n5 8 _ X R M 7\\n\\n8 2 1 _ X R M 7\\n\\nW R P 1\\n\\nt r o h s _ V R T 5\\n\\n0 6 _ X R M 7\\n\\nt r o h s _ Z X E 6\\n\\nS U 5\\n\\nI\\n\\nI\\n\\nU Y 5\\n\\n9 N W 5\\n\\nW H J 4\\n\\nG J Q 1\\n\\ne\\n\\n0.2\\n\\nc\\n\\nd\\n\\nMdm2 p53\\n\\n)\\n\\nm n\\n\\n(\\n\\ns t i n u\\n\\ne s n o p s e R\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.10\\n\\n0.05\\n\\n)\\n\\nm n\\n\\n(\\n\\ns t i n u\\n\\ne s n o p s e R\\n\\n0.30\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.10\\n\\n0.05\\n\\n[Binder] (nM)\\n\\n333 111 37 12 4 1\\n\\n)\\n\\nm n\\n\\n(\\n\\ne s n o p s e R\\n\\n0.1\\n\\n0\\n\\nKD = 0.7 nM\\n\\n0\\n\\nKD = 0.5 nM\\n\\n0\\n\\n250\\n\\n500\\n\\n750\\n\\nTime (s)\\n\\n1,000\\n\\n0\\n\\n250\\n\\n500 Time (s)\\n\\n750\\n\\n1,000\\n\\n0\\n\\np53/MDM2\\n\\nf\\n\\nOxidoreductase (EC1)\\n\\ng\\n\\n5\\n\\nEnzyme active site scaffolding\\n\\nNative enzyme\\n\\nInput\\n\\nDesign\\n\\nZoom\\n\\n)\\n\\n%\\n\\n4\\n\\n(\\n\\ne t a r\\n\\ns s e c c u s o c\\n\\n3\\n\\n2\\n\\ni l i\\n\\ns\\n\\nn\\n\\n1\\n\\n0\\n\\nEC1\\n\\nEC2\\n\\nEC3\\n\\nEC4\\n\\nEC5'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Fig. 4 | Scaffolding of diverse functional sites with RFdiffusion. a, RFdiffusion outperforms other methods across 25 benchmark motif-scaffolding problems collected from six recent publications (Supplementary Table 9). In silico success is defined as AF2 r.m.s.d. to design model less than 2\\u2009Å, AF2 r.m.s.d. to the native functional motif less than 1\\u2009Å and AF2 pAE less than five. One hundred designs were generated per problem, with no previous optimization on the benchmark set (some optimization was necessary for Hallucination). Supplementary Table 10 presents full results. In silico success rates on the problems are correlated between the methods, and RFdiffusion can still struggle on challenging problems in which all methods have low success. b, Four examples of designs in which RFdiffusion significantly outperforms existing methods. Teal, native motif; colours, AF2 prediction of a design. Metrics (r.m.s.d. AF2 versus design/versus native motif (Å), AF2 pAE): 5TRV long, 1.17/0.57;'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Teal, native motif; colours, AF2 prediction of a design. Metrics (r.m.s.d. AF2 versus design/versus native motif (Å), AF2 pAE): 5TRV long, 1.17/0.57; 4.73; 6E6R long, 0.89/0.27, 4.56; 7MRX long, 0.84/0.82 4.32; 5TPN, 0.59/0.49 3.77. c, RFdiffusion can scaffold the p53 helix that binds MDM2'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='(left) and makes extra contacts with the target (right, average 31% increased surface area. Design was p53_design_89). Designs were generated with an RFdiffusion model fine-tuned on complexes. d, BLI measurements indicate high-affinity binding to MDM2 (p53_design_89, 0.7\\u2009nM; p53_design_53, 0.5\\u2009nM); the native affinity is 600\\u2009nM (ref. 42). e, Out of 95 designs, 55 showed binding to MDM2 (more than 50% of maximum response). Thirty-two of these were monomeric (Supplementary Fig. 10h). f, After fine-tuning (Supplementary Methods), RFdiffusion can scaffold enzyme active sites. An oxidoreductase example (EC1) is shown (PDB 1A4I); catalytic site (teal); RFdiffusion output (grey, model; colours, AF2 prediction); zoom of active site. AF2 versus design backbone r.m.s.d. 0.88\\u2009Å, AF2 versus design motif backbone r.m.s.d. 0.53\\u2009Å, AF2 versus design motif full-atom r.m.s.d. 1.05\\u2009Å, AF2 pAE 4.47. g, In silico success rates on active sites derived from EC1-5 (AF2 Motif r.m.s.d. versus native: backbone'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='full-atom r.m.s.d. 1.05\\u2009Å, AF2 pAE 4.47. g, In silico success rates on active sites derived from EC1-5 (AF2 Motif r.m.s.d. versus native: backbone less than 1\\u2009Å, backbone and sidechain atoms less than 1.5\\u2009Å, r.m.s.d. AF2 versus design less than 2\\u2009Å, AF2 pAE less than 5).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='effect of noise on design quality, and Supplementary Fig. 8 for analysis of design diversity). The ability of RFdiffusion to scaffold functional motifs is not related to their presence in the RFdiffusion training set (Supplementary Fig. 7).\\n\\nOne of the benchmark problems is the scaffolding of the p53 helix that binds MDM2. Inhibiting this interaction through high-affinity competitive inhibition by scaffolding the p53 helix and making further interactions with MDM2 is a promising therapeutic avenue41. In silico\\n\\nNature | Vol 620 | 31 August 2023 | 1095'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Nature | Vol 620 | 31 August 2023 | 1095\\n\\nsuccess has been described elsewhere4, but experimental success has not been reported. We used an RFdiffusion model fine-tuned on protein complexes (Supplementary Methods) to generate 96 designs scaffolding this helix. We scaffolded the p53 helix in the presence of MDM2, so extra interactions could be designed by RFdiffusion and experimentally identified 0.5 and 0.7\\u2009nM binders (Fig. 4c,d), three orders of magnitude higher affinity than the reported 600\\u2009nM affinity of the p53 peptide alone42. The overall success rate was quite high: out of the 96 designs, 55 showed some detectable binding at 10\\u2009μM (Fig. 4e and Supplementary Fig. 10h).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Scaffolding enzyme active sites A grand challenge in protein design is to scaffold minimal descriptions of enzyme active sites comprising a few single amino acids. Whereas some in silico success has been reported previously4, a general solu- tion that can readily produce high-quality, orthogonally validated outputs remains elusive. Following fine-tuning on a task mimicking this problem (Supplementary Methods), RFdiffusion was able to scaf- fold enzyme active sites comprising many sidechain and backbone functional groups with high accuracy and in silico success rates across a range of enzyme classes (Fig. 4f and Extended Data Fig. 6a–d; in silico success required fine tuning). Although RFdiffusion is unable to explicitly model bound small molecules at present (however, see our conclusions), the substrate can be implicitly modelled using an exter- nal potential to guide the generation of ‘pockets’ around the active site. As a demonstration, we scaffold a retroaldolase active site triad'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='exter- nal potential to guide the generation of ‘pockets’ around the active site. As a demonstration, we scaffold a retroaldolase active site triad while implicitly modelling the reaction substrate (Extended Data Fig. 6e–h).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Symmetric functional-motif scaffolding Several important design challenges involve the scaffolding of several copies of a functional motif in symmetric arrangements. For example, many viral glycoproteins are trimeric and symmetry matched arrange- ments of inhibitory domains can be extremely potent43–46. Conversely, symmetric presentation of viral epitopes in an arrangement that mimics the virus could induce new classes of neutralizing antibodies47,48. To explore this general direction, we sought to design trimeric multiva- lent binders to the SARS-CoV-2 spike protein. In previous work, flex- ible linkage of a binder to the ACE2 binding site (on the spike protein receptor binding domain) to a trimerization domain yielded a high-affinity inhibitor that had potent and broadly neutralizing anti- viral activity in animal models43. Ideally, however, symmetric fusions to binders would be rigid, so as to reduce the entropic cost of binding while maintaining the avidity benefits from'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='however, symmetric fusions to binders would be rigid, so as to reduce the entropic cost of binding while maintaining the avidity benefits from multivalency. We used RFdiffusion to design C3-symmetric trimers that rigidly hold three bind- ing domains (the functional motif in this case) such that they exactly match the ACE2 binding sites on the SARS-CoV-2 spike protein trimer. The designs were confidently predicted by AF2 to both assemble as C3-symmetric oligomers, and to scaffold the AHB2 SARS-CoV-2 binder interface with high accuracy (Fig. 5a).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='The ability to scaffold functional sites with any desired symmetry opens up new approaches to designing metal-coordinating protein assemblies49,50. Divalent transition metal ions show distinct prefer- ences for specific coordination geometries (for example, square planar, tetrahedral and octahedral) with ion-specific optimal sidechain–metal bond lengths. RFdiffusion provides a general route to building up sym- metric protein assemblies around such sites, with the symmetry of the assembly matching the symmetry of the coordination geometry. As a first test, we sought to design square-planar Ni2+ binding sites. We designed C4 protein assemblies with four central histidine imida- zoles arranged in an ideal Ni2+-binding site with square-planar coor- dination geometry (Fig. 5b). Diverse designs starting from distinct C4-symmetric histidine square-planar sites had good in silico success'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='with the histidine residues in near ideal geometries for coordinating metal in the AF2-predicted structures (Supplementary Fig. 9).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='We expressed and purified 44 designs in Escherichia coli, and found that 37 had SEC chromatograms consistent with the intended oligo- meric state (Extended Data Fig. 7b). Of the designs, 36 were tested for Ni2+ coordination by isothermal titration calorimetry, and 18 were found to bind Ni2+ with dissociation constants ranging from low nanomolar to low micromolar (Fig. 5c,d and Extended Data Fig. 7a). The inflection points in the wild-type isotherms indicate binding with the designed stoichiometry, a one to four ratio of ion to monomer. Although most of the designed proteins showed exothermic metal coordination, in a few cases binding was endothermic (Fig. 5d, left and Extended Data Fig. 7a: NiB2.9, NiB2.10, NiB2.15 and NiB2.23), suggesting that Ni2+ coordination is entropically driven in these assemblies. To confirm that Ni2+ binding was indeed mediated by the scaffolded histidine 52, we mutated this residue to alanine, which abolished or notably reduced binding in 17 out of 17 cases'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='indeed mediated by the scaffolded histidine 52, we mutated this residue to alanine, which abolished or notably reduced binding in 17 out of 17 cases with successful expression (Extended Data Figs. 7a,c and Fig. 5c,d; one mutant did not express). We structurally charac- terized by nsEM a subset of the designs—NiB1.12, NiB1.15, NiB1.17 and NiB1.20—that showed histidine-dependent binding. All four designs showed clear fourfold symmetry both in the raw micrographs and in 2D class averages (Fig. 5c,d), with design NiB1.17 also clearly showing twofold axis side views with a measured diameter approximating the design model. A 3D reconstruction of NiB1.17 was in close agreement with the design model (Fig. 5c).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Design of protein-binding proteins The design of high-affinity binders to target proteins is a grand chal- lenge in protein design, with numerous therapeutic applications51. A general method for de novo binder design from target structure infor- mation alone using the physically based Rosetta method was recently described12, and subsequently, using ProteinMPNN for sequence design and AF2 for design filtering was found to improve design success rates26. However, experimental success rates were low, still requiring many thousands of designs to be screened for each design campaign12, and the approach relied on prespecifying a particular set of protein scaf- folds as the basis for the designs, inherently limiting the diversity and shape complementarity of possible solutions12. To our knowledge, no deep-learning method has yet demonstrated experimental general success in designing completely de novo binders.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='We reasoned that RFdiffusion might be able to address this chal- lenge by directly generating binding proteins in the context of the target. For many therapeutic applications, for example, blocking a protein–protein interaction, it is desirable to bind to a particular site on a target protein. To enable this, we fine-tuned RFdiffusion on protein complex structures, providing a feature as input indicating a subset of the residues on the target chain (called ‘interface hotspots’) to which the diffused chain binds (Fig. 6a and Extended Data Fig. 8a,b). For design challenges in which a particular binder fold might be especially compatible, we enabled coarse-grained control over binder scaffold topology by fine-tuning an extra model to condition binder diffusion on secondary structure and block-adjacency information, in addition to conditioning on interface hotspots (Extended Data Fig. 8c,d and Supplementary Methods).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='To compare RFdiffusion to previous binder design methods, we performed binder design campaigns against five targets: Influenza A H1 Haemagglutinin (HA)52, Interleukin-7 Receptor-α (IL-7Rα)12, Programmed Death-Ligand 1 (PD-L1)12, Insulin Receptor (InsR) and Tropomyosin Receptor Kinase A (TrkA)12. We designed putative binders to each target, both with and without conditioning on compatible fold information, with high in silico success rates (Extended Data Fig. 8e,f). Designs were filtered by AF2 confidence in the interface and mono- mer structure26, and 95 were selected for each target for experimental characterization.\\n\\n1096 | Nature | Vol 620 | 31 August 2023\\n\\na\\n\\nC3 axis\\n\\nC3 motif + noise\\n\\nSpike trimer\\n\\nb\\n\\nC4 motif\\n\\nC4 motif + noise\\n\\nd\\n\\n) 1 – l o m\\n\\nl\\n\\na c k\\n\\n3\\n\\n1\\n\\nWT H52A KD < 20 nM\\n\\n) 1 – l o m\\n\\nl\\n\\na c k\\n\\n–2\\n\\n–4\\n\\n–6\\n\\n(\\n\\n(\\n\\nH Δ\\n\\nH Δ\\n\\n–8\\n\\n–1\\n\\n–10\\n\\n0\\n\\n0.1\\n\\n0.2 0.3 Molar ratio\\n\\n0.4\\n\\n0.5\\n\\n0\\n\\n0.1'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Fig. 5 | Symmetric motif scaffolding with RFdiffusion. a, Design of symmetric oligomers scaffolding the binding interface of ACE2 mimic AHB2 (left, teal) against the SARS-CoV-2 spike trimer (left, grey). Three AHB2 copies are input to RFdiffusion along with C3 noise (middle); output are C3-symmetric oligomers holding the three AHB2 copies in place to engage all spike subunits. AF2 predictions (right) recapitulate the AHB2 structure with 0.6\\u2009Å r.m.s.d. over the assymetric unit and 2.9\\u2009Å r.m.s.d. over the C3 assembly. b, Design of C4- symmetric oligomers to scaffold a Ni2+ binding motif (left). Starting from square-planar histidine rotamers within helical fragments (Supplementary Methods), RFdiffusion generates a C4 oligomer scaffolding the binding domain (middle). AF2 predictions (colour) agree closely with the design model (grey), with backbone r.m.s.d. less than 1.0\\u2009Å (right). c, nsEM 2D class averages (scale bar, 60\\u2009Å) and 3D reconstruction density are consistent with the symmetry'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='backbone r.m.s.d. less than 1.0\\u2009Å (right). c, nsEM 2D class averages (scale bar, 60\\u2009Å) and 3D reconstruction density are consistent with the symmetry and'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='The designed binders were expressed in E. coli and purified, and binding was assessed through single point biolayer interferometry (BLI) screening at 10\\u2009μM binder concentration (Extended Data Fig. 8g). The overall experimental success rate, defined as binding at or above 50% of the maximal response for the positive control, was 19% (this is a conservative estimate as some designs that showed binding had insufficient material to permit screening at 10\\u2009μM: Extended Data\\n\\nc\\n\\n3D reconstruction\\n\\ns e g a r e v a\\n\\ns s a c D 2\\n\\nl\\n\\n–1\\n\\n) 1 – l o m\\n\\nl\\n\\na c k\\n\\n(\\n\\n–3\\n\\n–5\\n\\n–7\\n\\nWT H52A KD < 20 nM\\n\\nH Δ\\n\\n–9\\n\\n0\\n\\n0.1\\n\\n0.2 0.3 Molar ratio\\n\\n0.4\\n\\n0.5\\n\\n) 1 – l o m\\n\\n0\\n\\nl\\n\\nWT H52A KD < 20 nM\\n\\na c k\\n\\n(\\n\\nH Δ\\n\\n–2\\n\\nWT H52A KD ≈ 77 nM\\n\\n–4\\n\\n0.2 0.3 Molar ratio\\n\\n0.4\\n\\n0.5\\n\\n0\\n\\n0.1\\n\\n0.2 0.3 Molar ratio\\n\\n0.4\\n\\n0.5'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='0.4\\n\\n0.5\\n\\n0\\n\\n0.1\\n\\n0.2 0.3 Molar ratio\\n\\n0.4\\n\\n0.5\\n\\nstructure of the NiB1.17 design model shown superimposed on the density in ribbon representation (top). Isothermal titration calorimetry binding isotherm of design NiB1.17 (blue) indicates a dissociation constant less than 20\\u2009nM at a metal:monomer stoichiometry of 1:4. The H52A mutant isotherm (pink) ablates binding, indicating scaffolded histidine residues are critical for metal binding. d, Additional experimentally characterized Ni2+ binders NiB2.15 (left), NiB1.12 (middle) and NiB1.20 (right). Metal-coordinating sidechains in the design models (top, teal) are closely recapitulated in the AF2 predictions (colours). 2D nsEM class averages (middle; scale bar, 60\\u2009Å) are consistent with design models. Binding isotherms for wild-type (WT) and H52A mutant (bottom) indicate Ni2+ binding mediated directly by the scaffolded histidines at the designed stoichiometry. Note that for ITC plots, points represent single measurements.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Fig. 8g); an increase of roughly two orders of magnitude over our previous Rosetta-based method on the same targets (Fig. 6b). Bind- ers were identified for all five targets, with fewer than 100 designs tested per target compared to thousands in previous studies. Full BLI titrations for a subset of the designs showed nanomolar affini- ties with no further experimental optimization, including HA and IL-7Rα binders with affinities of roughly 30\\u2009nM (Fig. 6c). Binding\\n\\nNature | Vol 620 | 31 August 2023 | 1097\\n\\na\\n\\nInterface hotspots\\n\\nb\\n\\nc\\n\\nIL-7Ra\\n\\nInsR\\n\\nPD-L1\\n\\nTrkA\\n\\nRFdiffusion plus AF2 filtering has orders-of-magnitude higher experimental success rates than previous methods\\n\\n%\\n\\nL B y b e t a r\\n\\ns s e c c u s\\n\\na t n e m\\n\\ni r e p x E\\n\\n40\\n\\n35\\n\\n30\\n\\n25\\n\\n20\\n\\n15\\n\\n10\\n\\n5\\n\\n0\\n\\nRosetta pipeline → RFdiffusion plus AF2 filtering ←\\n\\n% 4 1 . 0\\n\\n% 3 4 . 0\\n\\n% 7 0 . 0\\n\\n40\\n\\n35\\n\\n30\\n\\n25\\n\\n20\\n\\n15\\n\\n10\\n\\n5\\n\\n0\\n\\n%\\n\\nD S Y y b e t a r\\n\\ns s e c c u s\\n\\na t n e m\\n\\ni r e p x E\\n\\nm n\\n\\ns t i n u\\n\\ne s n o p s e R\\n\\n0.12\\n\\n0.10'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='40\\n\\n35\\n\\n30\\n\\n25\\n\\n20\\n\\n15\\n\\n10\\n\\n5\\n\\n0\\n\\n%\\n\\nD S Y y b e t a r\\n\\ns s e c c u s\\n\\na t n e m\\n\\ni r e p x E\\n\\nm n\\n\\ns t i n u\\n\\ne s n o p s e R\\n\\n0.12\\n\\n0.10\\n\\n0.08\\n\\n0.06\\n\\n0.04\\n\\n0.02\\n\\n0\\n\\nKD = 30 nM\\n\\nm n\\n\\ns t i n u\\n\\ne s n o p s e R\\n\\n0.08\\n\\n0.06\\n\\n0.04\\n\\n0.02\\n\\n0\\n\\nKD1 = 80 nM KD2 = 27 nM\\n\\nm n\\n\\ns t i n u\\n\\ne s n o p s e R\\n\\n0.30\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.10\\n\\n0.05\\n\\n0\\n\\nKD = 1.4 μM\\n\\nm n\\n\\ns t i n u\\n\\ne s n o p s e R\\n\\n0.4\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\n0\\n\\nKD = 328 nM\\n\\nHA IL-7Ra INSR PD-L1 TrkA\\n\\n0\\n\\n100 Time (s)\\n\\n200\\n\\n0\\n\\n200\\n\\n400\\n\\nTime (s)\\n\\n600\\n\\n0\\n\\n100 Time (s)\\n\\n200\\n\\n0\\n\\n100 Time (s)\\n\\n200\\n\\n5,000 1,000 200 40 [Binder] (nM)\\n\\n1,000 333 111 37 12\\n\\n[Binder] (nM)\\n\\n10,000 2,000 400 80 [Binder] (nM)\\n\\n10,000 2,000 400 80 [Binder] (nM)\\n\\nd\\n\\nInfluenza HA\\n\\ne\\n\\ng\\n\\nr.m.s.d. = 0.63 Å\\n\\nKD = 28 nM\\n\\n0.5\\n\\nf\\n\\n)\\n\\nm n\\n\\n0.4\\n\\nKD = 28 nM\\n\\ns t i n u\\n\\n0.3\\n\\nh\\n\\nr.m.s.d. = 0.60 Å\\n\\ne s n o p s e R\\n\\n0.2\\n\\n0.1\\n\\n90º\\n\\n0\\n\\n90º\\n\\n0\\n\\n100\\n\\n200\\n\\n300\\n\\nTime (s)\\n\\n[Binder (nM) 5,000 1,000 200 40 8'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Fig. 6 | De novo design of protein-binding proteins. a, RFdiffusion generates protein binders given a target and specification of interface hotspot residues. b, De novo binders were designed to five protein targets; Influenza A H1 HA, IL-7Rα, InsR, PD-L1 and TrkA and hits with BLI response greater than or equal to 50% of the positive control were identified for all targets. For IL-7Rα, InsR, PD-L1 and TrkA, RFdiffusion has success rates roughly two orders of magnitude higher than the original design campaigns. We attribute one order of magnitude to RFdiffusion, and the second to filtering with AF2 (estimated success rates for previous campaigns if AF2 filtering had been used: HA, 0%; IL-7Rα, 2.2%; InsR, 5.5%; PD-L1, 3.7%; TrkA, 1.5%). c, For IL-7Rα, InsR, PD-L1 and TrkA, the highest affinity binder is shown above a BLI titration series. Reported KD values are based on global kinetic fitting with fixed global Rmax. d, The highest affinity HA binder, HA_20, binds with a KD of 28\\u2009nM.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Reported KD values are based on global kinetic fitting with fixed global Rmax. d, The highest affinity HA binder, HA_20, binds with a KD of 28\\u2009nM. c,d, Yellow or orange, target or hotspot'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='residues; grey, design model; purple, AF2 prediction (r.m.s.d. AF2 versus design). Binders: IL7Ra_55 (2.1\\u2009Å), InsulinR_30 (2.6\\u2009Å), PDL1_77 (1.5\\u2009Å), TrkA_88 (1.4\\u2009Å) (left to right in c) and HA_20 (1.7\\u2009Å) (d). e, Cryo-EM 2D class averages of HA_20 bound to influenza HA, strain A/USA:Iowa/1943 H1N1 (scale bar, 10\\u2009nm). f, 2.9\\u2009Å cryo-EM 3D reconstruction of the complex viewed along two orthogonal axes. HA_20 (purple) is bound to H1 along the stem of all three subunits. g, The cryo-EM structure of the HA_20 binder in complex closely matches the design model (r.m.s.d. to RFdiffusion design, 0.63\\u2009Å; yellow, influenza HA). h, Structure of the HA_20 binder alone superimposed on the design model viewed along two orthogonal axes. For cryo-EM panels, yellow, Influenza H1 map and/or structure; grey, HA_20 binder design model; purple, HA_20 binder map or structure.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='interfaces were often highly distinct from interfaces to these tar- gets in the PDB (Supplementary Figs. 11 and 12). To assess binder specificity, six of the highest affinity IL-7Rα binders were assessed by means of competition BLI, and all six competed for binding with a structurally validated positive control binding to the same site\\n\\n(Supplementary Fig. 10a; further work is required to fully characterize proteome-wide specificity).\\n\\nWe solved the structure of the highest affinity Influenza binder, HA_20, in complex with Iowa43 HA using cryo-EM (Extended Data Table 1). Raw electron micrographs revealed a well-folded HA\\n\\n1098 | Nature | Vol 620 | 31 August 2023'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='glycoprotein with clearly discernible side, top and tilted view orienta- tions suspended in a thin layer of vitreous ice (Extended Data Fig. 9a). The 2D class averages further show clear secondary structure elements corresponding to both Iowa43 HA (Extended Data Fig. 9b), as well as the HA_20 binder bound to the stem (Fig 6e). The 3D heterogenous refinement without symmetry revealed full occupancy of all three HA stem epitopes by the HA_20 binder. A final non-uniform 3D refinement reconstruction with C3 symmetry yielded a 2.9\\u2009Å map of the HA/HA_20 protein–protein complex (Fig 6f) and corresponding 3D structure that almost perfectly matches the computational design model (0.63 Å, Fig 6f,g; the sidechain interactions at the interface are very different from the closest structure in the PDB; Extended Data Fig. 9h). Over the binder alone, the experimental structure deviates from the RFdiffusion design by only 0.6\\u2009Å (Fig. 6h). These results demonstrate the ability of RFdiffusion to'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='the experimental structure deviates from the RFdiffusion design by only 0.6\\u2009Å (Fig. 6h). These results demonstrate the ability of RFdiffusion to generate new proteins with atomic level accuracy, and to precisely target functionally relevant sites on therapeutically important proteins.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Discussion RFdiffusion is a comprehensive improvement over current protein design methods. RFdiffusion readily generates diverse uncondi- tional designs up to 600 residues in length that are accurately pre- dicted by AF2, far exceeding the complexity and accuracy achieved by most previous methods (a recent Hallucination-based approach also achieved high unconditional performance53). Half of our tested unconditional designs express in a soluble way, and have circular dichroism spectra consistent with the design models and high ther- mostability. Despite their substantially increased complexity, the ideality and stability of RFdiffusion designs is akin to that of de novo protein designs generated using previous methods such as Rosetta. RFdiffusion enables generation of higher-order architectures with any desired symmetry, unlike Hallucination methods, which have so far been limited to cyclic symmetries. Electron microscopy confirmed that the structures of these oligomers are very'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='methods, which have so far been limited to cyclic symmetries. Electron microscopy confirmed that the structures of these oligomers are very similar to the design mod- els, which in many cases show little global similarity to known protein oligomers.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='There has been recent progress in scaffolding protein functional motifs using deep-learning methods (RF Hallucination, RFjoint Inpainting and diffusion), but Hallucination is slow for large systems, Inpainting fails when insufficient starting information is provided and previous diffusion methods had low accuracy. RFdiffusion outperforms these previous methods in the complexity of the motifs that can be scaf- folded, the precision with which sidechains are positioned (for cataly- sis and other functions), and the accuracy of motif recapitulation by AF2. The design of MDM2 binding proteins with three orders of magni- tude higher affinities than the scaffolded P53 motif demonstrates the robustness of RFdiffusion motif scaffolding. Combining accurate motif scaffolding with the design of symmetric assemblies enabled consist- ent and atomically precise positioning of sidechains to coordinate Ni2+ ions across diverse tetrameric assemblies'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='For binder design from target structural information alone, previous work required testing tens of thousands of sequences12. RFdiffusion, when combined with improved filtering26 raises experimental success rates by two orders of magnitude; high-affinity binders can be identi- fied from dozens of designs, in many cases eliminating the require- ment for slow and expensive high-throughput screening (at least for the non-polar sites targeted here; further studies will be required to assess success rates on more polar target sites and sites without native binding partners). A high-resolution cryo-EM structure of one of these designs in complex with influenza HA shows that RFdiffusion can design functional proteins with atomic accuracy. Vázquez Torres et al. demonstrate the ability of RFdiffusion to design picomolar affin- ity binders to flexible helical peptides54, further highlighting its use for de novo binder design. Vázquez Torres et al. also show how RFdiffusion'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='can be extended for protein model refinement by partial noising and denoising, which enables tuneable sampling around a given input structure. For peptide binder design, this enabled increases in affin- ity of nearly three orders of magnitude without high-throughput screening.\\n\\nThe breadth and complexity of problems solvable with RFdiffusion and the robustness and accuracy of the solutions far exceeds what has been achieved previously. In a manner reminiscent of the generation of images from text prompts, RFdiffusion makes possible, with mini- mal specialist knowledge, the generation of functional proteins from minimal molecular specifications (for example, high-affinity binders to a user-specified target protein, and diverse protein assemblies from user-specified symmetries).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='The power and scope of RFdiffusion can be extended in several directions. RF has recently been extended to nucleic acids and protein–nucleic acid complexes55, which should enable RFdiffusion to design nucleic acid binding proteins and perhaps folded RNA struc- tures. Extension of RF to incorporate ligands should similarly enable extension of RFdiffusion to explicitly model ligand atoms, and allow the design of protein–ligand interactions. The ability to customize RFdif- fusion to specific design challenges by addition of external potentials and by fine-tuning (as illustrated here for catalytic site scaffolding, binder-targeting and fold specification), along with continued improve- ments to the underlying methodology, should enable de novo protein design to achieve still higher levels of complexity, to approach and, in some cases, surpass what natural evolution has achieved.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Online content Any methods, additional references, Nature Portfolio reporting summa- ries, source data, extended data, supplementary information, acknowl- edgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41586-023-06415-8.\\n\\n1.\\n\\n2.\\n\\nDauparas, J. et al. Robust deep learning-based protein sequence design using ProteinMPNN. Science 378, 49–56 (2022). Ferruz, N., Schmidt, S. & Höcker, B. ProtGPT2 is a deep unsupervised language model for protein design. Nat. Commun. 13, 4348 (2022).\\n\\n3. Singer, J. M. et al. Large-scale design and refinement of stable proteins using\\n\\nsequence-only models. PLoS ONE 17, e0265020 (2022).\\n\\n4. Wang, J. et al. Scaffolding protein functional sites using deep learning. Science 377,\\n\\n5.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='sequence-only models. PLoS ONE 17, e0265020 (2022).\\n\\n4. Wang, J. et al. Scaffolding protein functional sites using deep learning. Science 377,\\n\\n5.\\n\\n387–394 (2022). Trippe, B. L. et al. Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. in The Eleventh International Conference on Learning Representations (2023).\\n\\n6. Anishchenko, I. et al. De novo protein design by deep network hallucination. Nature 600,\\n\\n547–552 (2021).\\n\\n7. Wicky, B. I. M. et al. Hallucinating symmetric protein assemblies. Science 378, 56–61\\n\\n(2022).\\n\\n8. Anand, N. & Achim, T. Protein structure and sequence generation with equivariant\\n\\n9.\\n\\ndenoising diffusion probabilistic models. Preprint at https://doi.org/10.48550/arXiv.2205. 15019 (2022). Luo, S. et al. Antigen-specific antibody design and optimization with diffusion-based generative models. in Adv. Neural Information Processing Systems Vol. 35 (eds Koyejo, S. et al.) 9754–9767 (Curran Associates, 2022).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='10. Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N. & Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. in Proc. 32nd International Conference on Machine Learning Vol. 37 (eds Bach, Francis and Blei, David) 2256–2265 (PMLR, 2015).\\n\\n11. Ho, J., Jain, A. & Abbeel, P. Denoising diffusion probabilistic models. in Adv. Neural\\n\\nInformation Processing Systems Vol. 33 (eds Larochelle, H. et al.) 6840–6851 (Curran Associates, 2020).\\n\\n12. Cao, L. et al. Design of protein-binding proteins from the target structure alone. Nature\\n\\n605, 551–560 (2022).\\n\\n13. Kuhlman, B. et al. Design of a novel globular protein fold with atomic-level accuracy.\\n\\nScience 302, 1364–1368 (2003).\\n\\n14. Ramesh, A. et al. Zero-shot text-to-image generation. in Proc. 38th International\\n\\nConference on Machine Learning Vol. 139 (eds Meila, M. & Zhang, T.) 8821–8831 (PMLR, 2021).\\n\\n15. Saharia, C. et al. Photorealistic text-to-image diffusion models with deep language'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='15. Saharia, C. et al. Photorealistic text-to-image diffusion models with deep language\\n\\nunderstanding. in Adv. Neural Information Processing Systems Vol. 35 (eds Koyejo, S. et al.) 36479–36494 (Curran Associates, 2022).\\n\\n16. Wu, K. E. et al. Protein structure generation via folding diffusion. Preprint at https://doi.\\n\\n17.\\n\\norg/10.48550/arXiv.2209.15611 (2022). Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583–589 (2021).\\n\\nNature | Vol 620 | 31 August 2023 | 1099\\n\\n18. Baek, M. et al. Accurate prediction of protein structures and interactions using a three-track\\n\\n41. Chène, P. Inhibiting the p53-MDM2 interaction: an important target for cancer therapy.\\n\\nneural network. Science 373, 871–876 (2021).\\n\\nNat. Rev. Cancer 3, 102–109 (2003).\\n\\n19. Watson, J. L., Bera, A., Juergens, D., Wang, J. & Baker, D. X-ray crystallographic validation\\n\\n42. Kussie, P. H. et al. Structure of the MDM2 oncoprotein bound to the p53 tumor suppressor'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='42. Kussie, P. H. et al. Structure of the MDM2 oncoprotein bound to the p53 tumor suppressor\\n\\nof design from this paper. Science 377, 387–394 (2022).\\n\\ntransactivation domain. Science 274, 948–953 (1996).\\n\\n20. Wu, R. et al. High-resolution de novo structure prediction from primary sequence.\\n\\n43. Hunt, A. C. et al. Multivalent designed proteins neutralize SARS-CoV-2 variants of\\n\\nPreprint at https://doi.org/10.1101/2022.07.21.500999 (2022).\\n\\n21. Lin, Z. et al. Language models of protein sequences at the scale of evolution enable\\n\\nconcern and confer protection against infection in mice. Sci. Transl. Med. 14, eabn1252 (2022).\\n\\naccurate structure prediction. Science 379, 1123–1130 (2023).\\n\\n44. Silverman, J. et al. Multivalent avimer proteins evolved by exon shuffling of a family of'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='44. Silverman, J. et al. Multivalent avimer proteins evolved by exon shuffling of a family of\\n\\n22. Berman, H. M. et al. The Protein Data Bank. Nucleic Acids Res. 28, 235–242 (2000). 23. De Bortoli, V. et al. Riemannian score-based generative modelling. in Adv. Neural Information Processing Systems Vol. 35 (eds Koyejo, S. et al.) 2406–2422 (Curran Associates, 2022).\\n\\nhuman receptor domains. Nat. Biotechnol. 23, 1556–1561 (2005).\\n\\n45. Detalle, L. et al. Generation and characterization of ALX-0171, a potent novel therapeutic\\n\\nnanobody for the treatment of respiratory syncytial virus infection. Antimicrob. Agents Chemother. 60, 6–13 (2016).\\n\\n24. Leach, A., Schmon, S. M., Degiacomi, M. T. & Willcocks, C. G. Denoising diffusion\\n\\nprobabilistic models on SO(3) for rotational alignment. In Proc. ICLR 2022 Workshop on Geometrical and Topological Representation Learning (2022).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='probabilistic models on SO(3) for rotational alignment. In Proc. ICLR 2022 Workshop on Geometrical and Topological Representation Learning (2022).\\n\\n46. Strauch, E.-M. et al. Computational design of trimeric influenza-neutralizing proteins targeting the hemagglutinin receptor binding site. Nat. Biotechnol. 35, 667–671 (2017).\\n\\n25. Chen, T., Zhang, R. & Hinton, G. Analog bits: generating discrete data using diffusion models with self-conditioning. in The Eleventh International Conference on Learning Representations (2023).\\n\\n26. Bennett, N.R. et al. Improving de novo protein binder design with deep learning. Nat.\\n\\n47. Boyoglu-Barnum, S. et al. Quadrivalent influenza nanoparticle vaccines induce broad\\n\\nprotection. Nature 592, 623–628 (2021).\\n\\n48. Walls, A. C. et al. Elicitation of potent neutralizing antibody responses by designed protein nanoparticle vaccines for SARS-CoV-2. Cell 183, 1367–1382.e17 (2020).\\n\\nCommun. 14, 2625 (2023).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Commun. 14, 2625 (2023).\\n\\n27. Anand, N. & Huang, P. Generative modeling for protein structures. in Adv. Neural Information Processing Systems Vol. 31 (eds Bengio, S. et al.) (Curran Associates, 2018). Ingraham, J. et al. Illuminating protein space with a programmable generative model. Preprint at bioRxiv https://doi.org/10.1101/2022.12.01.518682 (2022).\\n\\n28.\\n\\n49. Salgado, E. N., Lewis, R. A., Mossin, S., Rheingold, A. L. & Tezcan, F. A. Control of protein oligomerization symmetry by metal coordination: C2 and C3 symmetrical assemblies through CuII and NiII coordination. Inorg. Chem. 48, 2726–2728 (2009).\\n\\n50. Salgado, E. N. et al. Metal templated design of protein interfaces. Proc. Natl Acad. Sci.\\n\\nUSA 107, 1827–1832 (2010).\\n\\n51. Quijano-Rubio, A., Ulge, U. Y., Walkey, C. D. & Silva, D.-A. The advent of de novo proteins\\n\\n29. Lee, J. S. & Kim, P. M. ProteinSGM: Score-based generative modeling for de novo protein\\n\\nfor cancer immunotherapy. Curr. Opin. Chem. Biol. 56, 119–128 (2020).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='for cancer immunotherapy. Curr. Opin. Chem. Biol. 56, 119–128 (2020).\\n\\ndesign. Preprint at bioRxiv https://doi.org/10.1101/2022.07.13.499967 (2022).\\n\\n52. Chevalier, A. et al. Massively parallel de novo protein design for targeted therapeutics.\\n\\n30. Onuchic, J. N., Luthey-Schulten, Z. & Wolynes, P. G. Theory of protein folding: the energy\\n\\nNature 550, 74–79 (2017).\\n\\n31.\\n\\nlandscape perspective. Annu. Rev. Phys. Chem. 48, 545–600 (1997). Jendrusch, M., Korbel, J. O. & Sadiq, S. K. AlphaDesign: a de novo protein design framework based on AlphaFold. Preprint at bioRxiv https://doi.org/10.1101/2021.10.11. 463937 (2021).\\n\\n32. Basanta, B. et al. An enumerative algorithm for de novo design of proteins with diverse\\n\\npocket structures. Proc. Natl Acad. Sci. USA 117, 22135–22145 (2020).\\n\\n33. Pan, X. et al. Expanding the space of protein geometries by computational design of\\n\\n53. Frank, C. et al. Efficient and scalable de novo protein design using a relaxed sequence'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='53. Frank, C. et al. Efficient and scalable de novo protein design using a relaxed sequence\\n\\nspace. Preprint at bioRxiv https://doi.org/10.1101/2023.02.24.529906 (2023).\\n\\n54. Torres, S. V. et al. De novo design of high-affinity protein binders to bioactive helical peptides. Preprint at bioRxiv https://doi.org/10.1101/2022.12.10.519862 (2022). 55. Baek, M., McHugh, R., Anishchenko, I., Baker, D. & DiMaio, F. Accurate prediction of nucleic acid and protein-nucleic acid complexes using RoseTTAFoldNA. Preprint at bioRxiv https://doi.org/10.1101/2022.09.09.507333 (2022).\\n\\nde novo fold families. Science 369, 1132–1136 (2020).\\n\\n34. Marcandalli, J. et al. Induction of potent neutralizing antibody responses by a designed protein nanoparticle vaccine for respiratory syncytial virus. Cell 176, 1420–1431.e17 (2019).\\n\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\n\\n35. Butterfield, G. L. et al. Evolution of a designed protein assembly encapsulating its own\\n\\nRNA genome. Nature 552, 415–420 (2017).\\n\\n36. Goodsell, D. S. & Olson, A. J. Structural symmetry and protein function. Annu. Rev.\\n\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\n\\nBiophys. Biomol. Struct. 29, 105–153 (2000).\\n\\n37. Sterner, R. & Höcker, B. Catalytic versatility, stability, and evolution of the (βα)8-barrel enzyme fold. Chem. Rev. 105, 4038–4055 (2005).\\n\\n38. Sesterhenn, F. et al. De novo protein design enables the precise induction of\\n\\nRSV-neutralizing antibodies. Science 368, eaay5051 (2020).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='38. Sesterhenn, F. et al. De novo protein design enables the precise induction of\\n\\nRSV-neutralizing antibodies. Science 368, eaay5051 (2020).\\n\\n39. Yang, C. et al. Bottom-up de novo design of functional proteins with complex structural\\n\\nfeatures. Nat. Chem. Biol. 17, 492–500 (2021).\\n\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='40. Glasgow, A. et al. Engineered ACE2 receptor traps potently neutralize SARS-CoV-2. Proc.\\n\\nNatl Acad. Sci. USA 117, 28046–28055 (2020).\\n\\n© The Author(s) 2023\\n\\n1100 | Nature | Vol 620 | 31 August 2023\\n\\nReporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.\\n\\nData availability Design structures, AF2 models and experimental measurements are available at https://figshare.com/s/439fdd59488215753bc3. Cryo-EM maps and corresponding atomic models for the Influenza HA binder in Fig. 6d–h have been deposited in the PDB and the Electron Microscopy Data Bank under accession codes 8SK7 and EMDB-40557, respectively. Electron microscopy data collected for the HE0537 oligomer are avail- able at EMDB-40602.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Code availability Code for running RFdiffusion has been released on GitHub, free for academic, personal and commercial use at https://github.com/Rosetta- Commons/RFdiffusion. It is also available as a Google Colab notebook, accessible through GitHub.\\n\\n56. Yeh, A. H.-W. et al. De novo design of luciferases using deep learning. Nature 614,\\n\\n774–780 (2023).\\n\\n57. Ribeiro, A. J. M. et al. Mechanism and Catalytic Site Atlas (M-CSA): a database of enzyme\\n\\nreaction mechanisms and active sites. Nucleic Acids Res. 46, D618–D623 (2018). 58. Leaver-Fay, A. et al. ROSETTA3: an object-oriented software suite for the simulation and\\n\\ndesign of macromolecules. Methods Enzymol. 487, 545–574 (2011).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='design of macromolecules. Methods Enzymol. 487, 545–574 (2011).\\n\\nAcknowledgements We thank N. Anand and D. Tischer for helpful discussions, and I. Kalvet and Y. Kipnis for providing helpful Rosetta scripts. We thank A. Dosey for the provision of purified influenza HA protein. We thank R. Wu, J. Mou, K. Choi, L. Wu and D. Blei for valuable feedback during writing. We thank I. Haydon for help with graphics. We also thank L. Goldschmidt and K. VanWormer, respectively, for maintaining the computational and wet laboratory resources at the Institute for Protein Design. This work was supported by gifts from Microsoft (D.J., M.B. and D.B.), Amgen (J.L.W.), the Audacious Project at the Institute for Protein'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Design (B.L.T., I.S., J.Y., H.E. and D.B.), the Washington State General Operating Fund supporting the Institute for Protein Design (P.V. and I.S.), grant no. INV-010680 from the Bill and Melinda Gates Foundation (W.B.A., D.J., J.W. and D.B.), grant no. DE-SC0018940 MOD03 from the US Department of Energy Office of Science (A.J.B. and D.B.), grant no. 5U19AG065156-02 from the National Institute for Aging (S.V.T. and D.B.), an EMBO long-term fellowship no. ALTF 139-2018 (B.I.M.W.), the Open Philanthropy Project Improving Protein Design Fund (R.J.R. and D.B.), The Donald and Jo Anne Petersen Endowment for Accelerating Advancements in Alzheimer’s Disease Research (N.R.B.), a Washington Research Foundation Fellowship (S.J.P.), a Human Frontier Science Program Cross Disciplinary Fellowship (grant no. LT000395/2020-C, L.F.M.), an EMBO Non-Stipendiary Fellowship (grant no. ALTF 1047-2019, L.F.M.), the Defense Threat Reduction Agency grant nos. HDTRA1-19-1-0003 (N.H. and D.B.) and'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Non-Stipendiary Fellowship (grant no. ALTF 1047-2019, L.F.M.), the Defense Threat Reduction Agency grant nos. HDTRA1-19-1-0003 (N.H. and D.B.) and HDTRA12210012 (F.D.), the Institute for Protein Design Breakthrough Fund (A.C. and D.B.), an EMBO Postdoctoral Fellowship (grant no. ALTF 292-2022, J.L.W.) and the Howard Hughes Medical Institute (A.C., W.S., R.J.R. and D.B.), an NSF-GRFP (J.Y.), an NSF Expeditions grant (no. 1918839, J.Y., R.B. and T.S.J.), the Machine Learning for Pharmaceutical Discovery and Synthesis consortium (J.Y., R.B. and T.S.J.), the Abdul Latif Jameel Clinic for Machine Learning in Health (J.Y., R.B. and T.S.J.), the DTRA Discovery of Medical Countermeasures Against New and Emerging threats program (J.Y., R.B. and T.S.J.), EPSRC Prosperity Partnership grant no. EP/T005386/1 (E.M.) and the DARPA Accelerated Molecular Discovery program and the Sanofi Computational Antibody Design grant (J.Y., R.B. and T.S.J.). We thank Microsoft and AWS for generous gifts of cloud'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Discovery program and the Sanofi Computational Antibody Design grant (J.Y., R.B. and T.S.J.). We thank Microsoft and AWS for generous gifts of cloud computing resources.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Author contributions J.L.W., D.J., N.R.B., B.L.T., J.Y. and D.B. conceived the study. J.L.W., D.J., N.R.B., W.A., B.L.T. and J.Y. trained RFdiffusion. B.L.T. and J.Y., with assistance from V.D.B. and E.M., extended diffusion to residue orientations. H.E.E., D.J., J.L.W., N.R.B., N.H., W.S., P.V. and I.S. generated experimentally characterized designs. W.A., B.L.T., J.Y., D.J., J.L.W. and N.R.B. generated computational designs. H.E.E., A.J.B., R.J.R., L.F.M., B.I.M.W., S.J.P., N.H., A.C., S.V.T., J.L.W. and B.L.T. experimentally characterized designs. J.W., A.L. and W.S. contributed additional code. S.O. implemented RFdiffusion on Google Colab. M.B. and F.D. trained RF. D.B., T.S.J. and R.B. offered supervision throughout the project. J.L.W., D.J., B.L.T., N.R.B., J.Y., H.E. and D.B. wrote the manuscript. All authors read and contributed to the manuscript. J.L.W. and D.J. agree that the order of their respective names may be changed for personal pursuits to best suit their own'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='to the manuscript. J.L.W. and D.J. agree that the order of their respective names may be changed for personal pursuits to best suit their own interests.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Competing interests The authors declare no competing interests.\\n\\nAdditional information Supplementary information The online version contains supplementary material available at https://doi.org/10.1038/s41586-023-06415-8. Correspondence and requests for materials should be addressed to David Baker. Peer review information Nature thanks Arne Elofsson, Giulia Palermo, Alex Pritzel and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Reprints and permissions information is available at http://www.nature.com/reprints.\\n\\nArticle\\n\\nExtended Data Fig. 1 | See next page for caption.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Extended Data Fig. 1 | Training ablations reveal determinants of RFdiffusion success. A–C) RFdiffusion can generate high quality large unconditional monomers. Designs are routinely accurately recapitulated by AF2 (see also Fig. 2c), with high confidence (A) for proteins up to approximately 400 amino acids in length. B) Further orthogonal validation of designs by ESMFold. C) Recapitulation of the design structure is often better with ESMFold compared with AF2. For each backbone, the best of 8 ProteinMPNN sequences is plotted, with points therefore paired by backbone rather than sequence. D) Comparing RFdiffusion trained with MSE loss on Cα atoms and N-Cα-C backbone frames (Methods 2.5), rather than with FAPE loss8,17. The MSE loss is not invariant to the global coordinate frame, unlike FAPE loss, and is required for good performance at unconditional generation (left, two-proportion z-test of in silico success rate, n\\u2009=\\u2009400 designs per condition, z\\u2009=\\u20094.1, p\\u2009=\\u20094.1e-5). For motif'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='at unconditional generation (left, two-proportion z-test of in silico success rate, n\\u2009=\\u2009400 designs per condition, z\\u2009=\\u20094.1, p\\u2009=\\u20094.1e-5). For motif scaffolding problems, where the ‘motif’ provides a means to align the global coordinate frame between timesteps, FAPE loss performs approximately as well as MSE loss, suggesting the L2 nature of MSE loss (as opposed to the L1 loss in FAPE) is not empirically critical for performance. E) Allowing the model to condition on its X0 prediction at the previous timestep (see Supplementary Methods 2.4) improves designs. Designs with self-conditioning (pink) have improved recapitulation by AF2 (left) and better AF2 confidence in the prediction (right). Two-proportion z-test of in silico success rate, n\\u2009=\\u2009800 designs per condition z\\u2009=\\u200911.4, p\\u2009=\\u20096.1e-30. F) RFdiffusion leverages the representations learned during RF pre-training. RFdiffusion fine-tuned from pre-trained RF (pink) comprehensively outperforms a model trained for an equivalent amount of'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='during RF pre-training. RFdiffusion fine-tuned from pre-trained RF (pink) comprehensively outperforms a model trained for an equivalent amount of time, from untrained weights (gray). For context, sequences generated by ProteinMPNN on these output backbones are little'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='better than sampling ProteinMPNN sequences from random Gaussian-sampled coordinates (white). Two-proportion z-test of in silico success rate, pre-training vs without pre-training (or vs random noise; both have zero success rate), n\\u2009=\\u2009800 designs per condition, z =\\u200923.0, p\\u2009=\\u20093.1e-117. Note that the data in pink in D–F is the same data, reproduced in each plot for clarity. G) The median (by AF2 r.m.s.d. vs design) 300 amino acid unconditional sample highlighting the importance of self-conditioning and pre-training. Without pre-training (at least when trained with equivalent compute), RFdiffusion outputs bear little resemblance to proteins (gray, left). Without self-conditioning, outputs show characteristic protein secondary structures, but lack core-packing and ideality (gray, middle). With pre-training and self-conditioning, proteins are diverse and well-packed (pink, right). H) Greater coherence during unconditional denoising may partly explain the effect of self-conditioning.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='are diverse and well-packed (pink, right). H) Greater coherence during unconditional denoising may partly explain the effect of self-conditioning. Successive X0 predictions are more similar when the model can self-condition (lower r.m.s.d. between X0 predictions, pink curve). Data are aggregated from unconditional design trajectories of 100, 200 and 300 residues. I) During the reverse (generation) process, the noise added at each step can be scaled (reduced). Reducing the noise scale improves the in silico design success rates (left, middle; two- proportion z-test of in silico success rate, n\\u2009=\\u2009800 designs per condition, 0 vs 0.5: z\\u2009=\\u20091.7, p\\u2009=\\u20090.09, 0 vs 1: z\\u2009=\\u20096.5, p\\u2009=\\u20096.8e-11; 0.5 vs 1: z\\u2009=\\u20094.8, p\\u2009=\\u20091.4e-6). This comes at the expense of diversity, with the number of unique clusters at a TM-score cutoff of 0.6 reduced when noise is reduced (right). Note throughout this figure the 6EXZ_long benchmarking problem is abbreviated to 6EXZ for brevity. Boxplots represent median±IQR; tails:'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='(right). Note throughout this figure the 6EXZ_long benchmarking problem is abbreviated to 6EXZ for brevity. Boxplots represent median±IQR; tails: min/max excluding outliers (±1.5xIQR).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Article'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Extended Data Fig. 2 | RFdiffusion learns the distribution of the denoising process, and inference efficiency can be improved. A) Analysis of simulated forward (noising) and reverse (denoising) trajectories shows that the distribution of Cα coordinates and residue orientations closely match, demonstrating that RFdiffusion has learned the distribution of the denoising process as desired. Left to right: i) average distance between a Cα coordinate at Xt and its position in X0; ii) average distance between a Cα coordinate at Xt and Xt-1; iii) average distance between adjacent Cα coordinates at Xt; iv) average rotation distance between a residue orientation at Xt and X0; v) average rotation distance between a residue orientation at Xt and Xt-1. B-C) While RFdiffusion is trained to generate samples over 200 timesteps, in many cases, trajectories can be shortened to improve computational efficiency. B) Larger steps can be taken between timesteps at inference. Decreasing the number of'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='can be shortened to improve computational efficiency. B) Larger steps can be taken between timesteps at inference. Decreasing the number of timesteps speeds up inference, and often does not decrease in silico success rates (left)'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='(for example, on an NVIDIA A4000 GPU, 100 amino acid designs can be generated with 15 steps, in ~11s, with an in silico success rate of over 60%). When normalized for compute budget (center) it is often much more efficient to run more trajectories with fewer timesteps. This can be done without loss of diversity in samples (right). For harder problems (e.g. unconditional 300 amino acids), one must strike an intermediate number of total timesteps (e.g., T\\u2009=\\u200950) for optimal compute efficiency. Note that for all other analyses in the paper, 200 inference steps were used, in line with how RFdiffusion is trained. C) An alternative to taking larger steps is to stop trajectories early (possible because RFdiffusion predicts X0 at every timestep). In many cases, trajectories can be stopped at timestep 50–75 with little effect on the final in silico success rate of designs (left), and when normalized by compute budget (center), success rates per unit time are typically higher generating more'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='success rate of designs (left), and when normalized by compute budget (center), success rates per unit time are typically higher generating more designs with early-stopping. Again, this can be done without a significant loss in diversity (right).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Extended Data Fig. 3 | Unconditionally-generated designs are folded and thermostable. A) Four 200 amino acid and fourteen 300 amino acid proteins were tested for expression and stability. 9/18 designs expressed, with a major peak at the expected elution volume. Blue: 300 amino acid proteins; Purple: 200 amino acid proteins. B) Colored AF2 predictions overlaid on gray design\\n\\nmodels (left), circular dichroism spectra at 25\\u2009°C (blue) and 95\\u2009°C (pink) (middle) and circular dichroism melt curves (right) for all 9 designs passing expression thresholds. In all cases, proteins remain well folded even at 95\\u2009°C. Note that data on 300aa_3 and 300aa_8 are duplicated from Fig. 2f, reproduced here for clarity.\\n\\nArticle\\n\\nExtended Data Fig. 4 | See next page for caption.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Extended Data Fig. 4 | RFdiffusion can condition on fold information to generate specific, thermostable folds. A) 6WVS is a previously-described de novo designed TIM barrel (left). A fine-tuned RFdiffusion model can condition on 1D and 2D inputs representing this protein fold, specifically secondary structure (middle, bottom) and block-adjacency information (middle, top) (see Supplementary Methods 4.3.2). RFdiffusion then generates proteins that closely recapitulate this course-grained fold information (right). B) Outputs are diverse with respect to each other. With this coarse-grained fold specification, in silico successful designs are much more diverse (as quantified by pairwise TM-scores) compared to diversity generated through simply sampling many sequences for the original PDB backbone (6WVS). C) NTF2 folds are useful scaffolds for de novo enzyme design56, and can also be readily generated with fold-conditioning in RFdiffusion. Designs are diverse and closely recapitulated by'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='de novo enzyme design56, and can also be readily generated with fold-conditioning in RFdiffusion. Designs are diverse and closely recapitulated by AF2. D) In silico success rates are high with fold- conditioned diffusion. TIM barrels are generated with an AF2 in silico success rate of 42.5% (left bar, pink) with in silico success incorporating both AF2'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='metrics and a TM-score vs 6WVS > 0.5. NTF2 folds are generated with an AF2 in silico success rate of 54.1% (right bar, pink), with in silico success incorporating both AF2 metrics and a TM-score vs PDB: 1GY6 > 0.5. In silico success was further validated with ESMFold (blue bars), where a pLDDT > 80 was used as the confidence metric for success. Gray: RFdiffusion design, colors: AF2 prediction. E) 11 TIM barrel designs were purified alongside the 6WVS positive control. Ten of these express and elute predominantly as monomers (note that the designs are approximately 4kDa larger than 6WVS). F) Eight designs expressed sufficiently for analysis by circular dichroism. All designs are folded, with circular dichroism spectra consistent with the designed structure (middle), and similar to 6WVS. Designs were also all highly thermostable, with CD melt analyses demonstrating designs were folded even at 95\\u2009°C (right). Designs are shown in gray, with the AF2 predictions overlaid in colors (left).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='melt analyses demonstrating designs were folded even at 95\\u2009°C (right). Designs are shown in gray, with the AF2 predictions overlaid in colors (left). Note that data on 6WVS and TIM_barrel_6 are duplicated from Fig. 2g, reproduced here for clarity.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Article\\n\\nExtended Data Fig. 5 | See next page for caption.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Extended Data Fig. 5 | Symmetric oligomer design with RFdiffusion. A) Due to the (near-perfect - see Supplementary Methods 3.1) equivariance properties of RFdiffusion, X0 predictions from symmetric inputs are also symmetric, even at very early timepoints (and becoming increasingly symmetric through time; r.m.s.d. vs symmetrized: t\\u2009=\\u2009200 1.20 Å; t\\u2009=\\u2009150 0.40 Å; t\\u2009=\\u200950 0.06 Å; t\\u2009=\\u20090 0.02Å). Gray: symmetrized (top left) subunit; colors: RFdiffusion X0 prediction. B) In silico success rates for symmetric oligomer designs of various cyclic and dihedral symmetries. In silico success is defined here as the proportion of designs for which AF2 yields a prediction from a single sequence that has mean pLDDT > 80 and backbone r.m.s.d. over the oligomer between the design model and AF2 < 2Å. Note that 16 sequences per RFdiffusion design were sampled. C) Box plots of the distribution of backbone r.m.s.d.s between AF2 and the RFdiffusion design model with and without the use of external potentials'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='C) Box plots of the distribution of backbone r.m.s.d.s between AF2 and the RFdiffusion design model with and without the use of external potentials during the trajectory. The external potentials used are the ‘inter-chain’ contact potential (pushing chains together), as well as the ‘intra-chain’ contact potential (making chains more globular). Using these potentials dramatically improves in silico'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='success (Two-proportion z-test of in silico success rate: n\\u2009=\\u2009100 designs per condition, z\\u2009=\\u20094.3, p\\u2009=\\u20091.9e-5). D) Designs are diverse with respect to the training dataset (the PDB). While the monomers (typically 60–100 AA) show reasonable alignment to the PDB (median 0.72), the whole oligomeric assemblies showed little resemblance to the PDB (median 0.50). E) Additional examples of design models (left) against AF2 predictions (right) for C3, C5, C12, and D4 symmetric designs (the symmetries not displayed in Fig. 3) with backbone r.m.s.d.s (Å) against their AF2 predictions of 0.82, 0.63, 0.79, and 0.78 with total amino acids 750, 900, 960, 640. F) Additional nsEM data for symmetric designs. The model is shown on the left and the 2D class averages on the right for each design. G) Two orthogonal side views of HE0537 by cryo-EM. Representative 2D class averages from the cryo-EM data are shown to the right of 2D projection images of the computational design model (lowpass filtered to 8\\u2009Å),'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='2D class averages from the cryo-EM data are shown to the right of 2D projection images of the computational design model (lowpass filtered to 8\\u2009Å), which appear nearly identical to the experimental data. Scale bars shown (white) are 60 Å. Boxplot represents median ± IQR; tails: min/max excluding outliers (±1.5xIQR).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Article\\n\\nExtended Data Fig. 6 | See next page for caption.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Extended Data Fig. 6 | External potentials for generating pockets around substrate molecules. A–D) Example in silico successful designs for enzyme classes 2–5 (ref. 57, see also Fig. 4). Native enzyme (PDB: 1CWY, 1DE3, 1P1X, 1SNZ); catalytic site (teal); RFdiffusion output (gray: model, colors: AF2 prediction). Metrics (AF2 vs design backbone r.m.s.d., AF2 vs design motif backbone r.m.s.d., AF2 vs design motif full-atom r.m.s.d., AF2 pAE): EC2: 0.93 Å, 0.50 Å, 1.29 Å, 3.51; EC3: 0.92 Å, 0.60 Å, 1.07 Å, 4.59; EC4: 0.93 Å, 0.80 Å, 1.03 Å, 4.41; EC5: 0.78 Å, 0.44 Å, 1.14 Å, 3.32. E–H) Implicit modeling of a substrate while scaffolding a retroaldolase active site triad [TYR1051-LYS1083-TYR1180] from PDB: 5AN7. E) The potential used to implicitly model the substrate, which has both a repulsive and attractive field (see Supplementary Methods 4.4). F) Left: Kernel densities demonstrate that without using the external potential (pink), designs often fall into two failure modes: (1) no pocket,'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='F) Left: Kernel densities demonstrate that without using the external potential (pink), designs often fall into two failure modes: (1) no pocket, and (2) clashes with the substrate. Right: clashes (substrate < 3 Å of the backbone) & pockets (no clash and > 16 Cα within 3–8 Å of substrate) with and without the potential. Two- proportion z-test: n\\u2009=\\u200971/51 +/− potential; clashes z\\u2009=\\u2009−2.05, p\\u2009=\\u20090.02, pocket'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='z\\u2009=\\u2009−2.27, p\\u2009=\\u20090.01. Each datapoint represents a design already passing the stringent in silico success metrics (AF2 motif r.m.s.d. < 1 Å, AF2 backbone r.m.s.d. < 2 Å, AF2 pAE < 5). Note that the potential and clash definition pertain only to backbone Cα atoms, and do not currently include sidechain atoms. G) Designs close to the labeled local maxima of the kernel density estimate. Without the potential, the catalytic triad is predominantly (1) exposed on the surface with no residues available to provide substrate stabilization or (2) buried in the protein core, preventing substrate access. With the potential, the catalytic triad is predominantly (3), partially buried in a concave pocket with shape complementary to the substrate. Backbone atoms within 3 Å of the substrate are shown in red. H) A variety of diverse designs with pockets made using the potential, with no clashes between the substrate and the AF2- predicted backbone. The functional form and parameters used for the pocket'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='using the potential, with no clashes between the substrate and the AF2- predicted backbone. The functional form and parameters used for the pocket potential are detailed in Supplementary Methods 4.4. In each case the substrate is superimposed on the AF2 prediction of the catalytic triad.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Article\\n\\nExtended Data Fig. 7 | See next page for caption.\\n\\nExtended Data Fig. 7 | Additional Ni2+ binding C4 oligomers. A) AF2 predictions of a subset of the experimentally verified Ni2+ binding oligomers, with corresponding isothermal titration calorimetry (ITC) binding isotherms for the wild-type (blue) and H52A mutant (pink) below. Note that these, with Fig. 5, encompass all of the experimentally validated outputs deriving from unique RFdiffusion backbones. Wild-type dissociation constants are displayed in each plot. We observe a mixture of endothermic (NiB2.10, NiB2.23, NiB2.15) and exothermic isotherms. For all cases displayed we observe no binding to the ion for H52A mutants, indicating the scaffolded histidine at position 52 is critical for ion binding. KD values in the isotherms indicate binding of the ion'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='with the designed stoichiometry (1:4 Ni2+:protein). Note that each backbone depicted is from a unique RFdiffusion sampling trajectory, and that models and data for designs NiB2.15, NiB1.12, NiB1.20 and NiB1.17 from Fig. 5 are duplicated here for ease of viewing. B) Size exclusion chromatograms for elutions from the 44 purifications suggest the vast majority of designs are soluble and have the correct oligomeric state. C) Size exclusion chromatograms for 20 H52A mutants show that the mutants remain soluble and retain the intended oligomeric state. Note that only 18 of these 20 had wild-type sequences that definitively bound nickel. Note also that for ITC plots, points represent single measurements.\\n\\nArticle\\n\\nExtended Data Fig. 8 | See next page for caption.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Extended Data Fig. 8 | Targeted unconditional and fold-conditioned protein binder design. A-B) The ability to specify where on a target a designed binder should bind is crucial. Specific “hotspot” residues can be input to a fine-tuned RFdiffusion model, and with these inputs, binders almost universally target the correct site. A) IL-7Rα (PDB: 3DI3) has two patches that are optimal for binding, denoted Site 1 and Site 2 here. For each site, 100 designs were generated (without fold-specification). B) Without guidance, designs typically target Site 1 (left bar, gray), with contact defined as Cα-Cα distance between binder and hotspot reside < 10 Å. Specifying Site 1 hotspot residues increases further the efficiency with which Site 1 is targeted (left bar, pink). In contrast, specifying the Site 2 hotspot residues can completely redirect RFdiffusion, allowing it to efficiently target this site (right bar, pink). C-D) As well as conditioning on hotspot residue information, a fine-tuned'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='RFdiffusion, allowing it to efficiently target this site (right bar, pink). C-D) As well as conditioning on hotspot residue information, a fine-tuned RFdiffusion model can also condition on input fold information (secondary structure and block-adjacency information - see Supplementary Methods 4.5). This effectively allows the specification of a (for instance, particularly compatible) fold that the binder should adopt. C) Two examples showing binders can be specified to adopt either a ferredoxin fold (left) or a particular helical bundle fold (right). D) Quantification of the efficiency of fold-conditioning. Secondary structure inputs were accurately respected (top, pink). Note that in this design target and target site, RFdiffusion without fold-specification made generally helical'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='designs (right, gray bar). Block-adjacency inputs were also respected for both input folds (bottom, pink). E) Reducing the noise added at each step of inference improves the quality of binders designed with RFdiffusion, both with and without fold-conditioning. As an example, the distribution of AF2 interaction pAEs (known to indicate binding when pAE < 1026) is shown for binders designed to PD-L1. In both cases, the proportion of designs with interaction pAE < 10 is high (blue curve), and improved when the noise is scaled by a factor 0.5 (pink curve) or 0 (yellow curve). F) Full in silico success rates for the protein binders designed to five targets. In each case, the best fold- conditioned results are shown (i.e. from the most target-compatible input fold), and the success rates at each noise scale are separated. In line with current best practice26, we tested using Rosetta FastRelax58 before designing the sequence with ProteinMPNN, but found that this did not systematically improve'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='practice26, we tested using Rosetta FastRelax58 before designing the sequence with ProteinMPNN, but found that this did not systematically improve designs. In silico success is defined in line with current best practice26: AF2 pLDDT of the monomer > 80, AF2 interaction pAE < 10, AF2 r.m.s.d. monomer vs design < 1 Å. G) Experimentally-validated de novo protein binders were identified for all five of the targets. Designs that bound at 10 μM during single point BLI screening with a response equal to or greater than 50% of the positive control were considered binders. Concentration is denoted by hue for designs that were screened at concentrations less than 10 μM and thus may be false negatives.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Article\\n\\nExtended Data Fig. 9 | See next page for caption.\\n\\nExtended Data Fig. 9 | Cryo-electron microscopy structure determination of designed Influenza HA binder. A) Representative raw micrograph showing ideal particle distribution and contrast. B) 2D Class averages of Influenza H1+HA_20 binder with clearly defined secondary structure elements and a full- sampling of particle view angles (scale bar = 10\\u2009nm). C) Cryo-EM local resolution map calculated using an FSC value of 0.143 viewed along two different angles. Local resolution estimates range from ~2.3 Å at the core of H1 to ~3.4 Å along the periphery of the N-terminal helix of the HA_20 binder. D) Cryo-EM structure of'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='the full H1+HA_20 binder complex (purple: HA_20; yellow: H1; teal: glycans). E) Global resolution estimation plot. F) Orientational distribution plot demonstrating complete angular sampling. G) 3D ab initio (left) and 3D heterogenous refinement (right - unsharpened) outputs, performed in the absence of applied symmetry, and showing clear density of the HA_20 binder bound to all three stem epitopes of the Iowa43 HA glycoprotein trimer, in all maps. H) The designed binder has topological similarity to 5VLI, a protein in the PDB, but binds with very different interface contacts.\\n\\nArticle\\n\\nExtended Data Table 1 | Cryo-EM data collection, refinement and validation statistics\\n\\nCorresponding author(s): David Baker\\n\\nLast updated by author(s): June 22nd, 2023\\n\\nReporting Summary'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Corresponding author(s): David Baker\\n\\nLast updated by author(s): June 22nd, 2023\\n\\nReporting Summary\\n\\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency in reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.\\n\\nStatistics For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\\n\\nn/a Confirmed\\n\\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\\n\\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\\n\\nThe statistical test(s) used AND whether they are one- or two-sided Only common tests should be described solely by name; describe more complex techniques in the Methods section.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content=\"A description of all covariates tested\\n\\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\\n\\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\\n\\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted Give P values as exact values whenever suitable.\\n\\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\\n\\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\\n\\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content=\"Estimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\\n\\nOur web collection on statistics for biologists contains articles on many of the points above.\\n\\nSoftware and code\\n\\nPolicy information about availability of computer code\\n\\nData collection\\n\\nRFdiffusion 1.0.0 (this study), ProteinMPNN, AlphaFold2, TMalign, Protein-Protein BLAST 2.11.0+, SerialEM\\n\\nData analysis\\n\\nMatplotlib 3.6.2, ScIPy 1.9.3, Seaborn 0.11.2, PyMOL 2.5.0, ForteBio Data Analysis Software Version 9.0.0.14, pycorn 0.19, CryoSparc v4.0.3, Microcal PEAQ-ITC Analysis Software\\n\\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.\\n\\nData\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Data\\n\\nPolicy information about availability of data All manuscripts must include a data availability statement. This statement should provide the following information, where applicable:\\n\\nAccession codes, unique identifiers, or web links for publicly available datasets - A description of any restrictions on data availability - For clinical datasets or third party data, please ensure that the statement adheres to our policy\\n\\nDesign structures, AlphaFold2 models and experimental measurements are available at https://figshare.com/s/439fdd59488215753bc3. Cryo-EM maps and corresponding atomic models for the Influenza HA binder in Figure 6D-H have been deposited in the PDB and the Electron Microscopy Data Bank under accession\\n\\nn a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n1'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='n a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n1\\n\\ncodes 8SK7 and EMDB-40557, respectively. Electron microscopy data collected for the HE0537 oligomer is available at EMDB-40602. Cryo-EM data collection, refinement and validation statistics are supplied in Extended Data Table 1.\\n\\nResearch involving human participants, their data, or biological material\\n\\nPolicy information about studies with human participants or human data. See also policy information about sex, gender (identity/presentation), and sexual orientation and race, ethnicity and racism.\\n\\nReporting on sex and gender\\n\\nN/A\\n\\nReporting on race, ethnicity, or other socially relevant groupings\\n\\nN/A\\n\\nPopulation characteristics\\n\\nN/A\\n\\nRecruitment\\n\\nN/A\\n\\nEthics oversight\\n\\nN/A\\n\\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\\n\\nField-specific reporting'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='N/A\\n\\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\\n\\nField-specific reporting\\n\\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\\n\\nLife sciences\\n\\nBehavioural & social sciences\\n\\nEcological, evolutionary & environmental sciences\\n\\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\\n\\nLife sciences study design\\n\\nAll studies must disclose on these points even when the disclosure is negative.\\n\\nSample size\\n\\nVariable depending on analysis performed. Detailed in figure legends. Sample sizes were chosen prior to the experiment, and were decided arbitrarily by the experimenter (rather than by statistical test), but were large enough to draw meaningful conclusions from the experiment.\\n\\nData exclusions\\n\\nNone\\n\\nReplication'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Data exclusions\\n\\nNone\\n\\nReplication\\n\\nEach dataset contains many (n reported in figure legends) independent measurements.\\n\\nRandomization\\n\\nN/A (all analysis was automated, so each datapoint was generated computationally under controlled and uniform settings)\\n\\nBlinding\\n\\nN/A (all analysis was automated, so there was no user intervention that could have introduced bias)\\n\\nBehavioural & social sciences study design\\n\\nAll studies must disclose on these points even when the disclosure is negative.\\n\\nStudy description\\n\\nBriefly describe the study type including whether data are quantitative, qualitative, or mixed-methods (e.g. qualitative cross-sectional, quantitative experimental, mixed-methods case study).\\n\\nResearch sample'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Research sample\\n\\nState the research sample (e.g. Harvard university undergraduates, villagers in rural India) and provide relevant demographic information (e.g. age, sex) and indicate whether the sample is representative. Provide a rationale for the study sample chosen. For studies involving existing datasets, please describe the dataset and source.\\n\\nSampling strategy\\n\\nDescribe the sampling procedure (e.g. random, snowball, stratified, convenience). Describe the statistical methods that were used to predetermine sample size OR if no sample-size calculation was performed, describe how sample sizes were chosen and provide a rationale for why these sample sizes are sufficient. For qualitative data, please indicate whether data saturation was considered, and what criteria were used to decide that no further sampling was needed.\\n\\nData collection'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Data collection\\n\\nProvide details about the data collection procedure, including the instruments or devices used to record the data (e.g. pen and paper, computer, eye tracker, video or audio equipment) whether anyone was present besides the participant(s) and the researcher, and whether the researcher was blind to experimental condition and/or the study hypothesis during data collection.\\n\\nTiming\\n\\nIndicate the start and stop dates of data collection. If there is a gap between collection periods, state the dates for each sample cohort.\\n\\nn a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n2\\n\\nData exclusions\\n\\nIf no data were excluded from the analyses, state so OR if data were excluded, provide the exact number of exclusions and the rationale behind them, indicating whether exclusion criteria were pre-established.\\n\\nNon-participation'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Non-participation\\n\\nState how many participants dropped out/declined participation and the reason(s) given OR provide response rate OR state that no participants dropped out/declined participation.\\n\\nRandomization\\n\\nIf participants were not allocated into experimental groups, state so OR describe how participants were allocated to groups, and if allocation was not random, describe how covariates were controlled.\\n\\nEcological, evolutionary & environmental sciences study design\\n\\nAll studies must disclose on these points even when the disclosure is negative.\\n\\nStudy description\\n\\nBriefly describe the study. For quantitative data include treatment factors and interactions, design structure (e.g. factorial, nested, hierarchical), nature and number of experimental units and replicates.\\n\\nResearch sample'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Research sample\\n\\nDescribe the research sample (e.g. a group of tagged Passer domesticus, all Stenocereus thurberi within Organ Pipe Cactus National Monument), and provide a rationale for the sample choice. When relevant, describe the organism taxa, source, sex, age range and any manipulations. State what population the sample is meant to represent when applicable. For studies involving existing datasets, describe the data and its source.\\n\\nSampling strategy\\n\\nNote the sampling procedure. Describe the statistical methods that were used to predetermine sample size OR if no sample-size calculation was performed, describe how sample sizes were chosen and provide a rationale for why these sample sizes are sufficient.\\n\\nData collection\\n\\nDescribe the data collection procedure, including who recorded the data and how.\\n\\nTiming and spatial scale'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Data collection\\n\\nDescribe the data collection procedure, including who recorded the data and how.\\n\\nTiming and spatial scale\\n\\nIndicate the start and stop dates of data collection, noting the frequency and periodicity of sampling and providing a rationale for these choices. If there is a gap between collection periods, state the dates for each sample cohort. Specify the spatial scale from which the data are taken\\n\\nData exclusions\\n\\nIf no data were excluded from the analyses, state so OR if data were excluded, describe the exclusions and the rationale behind them, indicating whether exclusion criteria were pre-established.\\n\\nReproducibility\\n\\nDescribe the measures taken to verify the reproducibility of experimental findings. For each experiment, note whether any attempts to repeat the experiment failed OR state that all attempts to repeat the experiment were successful.\\n\\nRandomization'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Randomization\\n\\nDescribe how samples/organisms/participants were allocated into groups. If allocation was not random, describe how covariates were controlled. If this is not relevant to your study, explain why.\\n\\nBlinding\\n\\nDescribe the extent of blinding used during data acquisition and analysis. If blinding was not possible, describe why OR explain why blinding was not relevant to your study.\\n\\nDid the study involve field work?\\n\\nYes\\n\\nNo\\n\\nField work, collection and transport\\n\\nField conditions\\n\\nDescribe the study conditions for field work, providing relevant parameters (e.g. temperature, rainfall).\\n\\nLocation\\n\\nState the location of the sampling or experiment, providing relevant parameters (e.g. latitude and longitude, elevation, water depth).\\n\\nAccess & import/export Describe the efforts you have made to access habitats and to collect and import/export your samples in a responsible manner and in'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='compliance with local, national and international laws, noting any permits that were obtained (give the name of the issuing authority, the date of issue, and any identifying information).\\n\\nDisturbance\\n\\nDescribe any disturbance caused by the study and how it was minimized.\\n\\nReporting for specific materials, systems and methods\\n\\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response.\\n\\nn a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n3\\n\\nMaterials & experimental systems n/a Involved in the study\\n\\nMethods n/a Involved in the study\\n\\nAntibodies\\n\\nChIP-seq\\n\\nEukaryotic cell lines\\n\\nFlow cytometry\\n\\nPalaeontology and archaeology\\n\\nMRI-based neuroimaging\\n\\nAnimals and other organisms'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Antibodies\\n\\nChIP-seq\\n\\nEukaryotic cell lines\\n\\nFlow cytometry\\n\\nPalaeontology and archaeology\\n\\nMRI-based neuroimaging\\n\\nAnimals and other organisms\\n\\nClinical data\\n\\nDual use research of concern\\n\\nPlants\\n\\nAntibodies Antibodies used\\n\\nDescribe all antibodies used in the study; as applicable, provide supplier name, catalog number, clone name, and lot number.\\n\\nValidation\\n\\nDescribe the validation of each primary antibody for the species and application, noting any validation statements on the manufacturer’s website, relevant citations, antibody profiles in online databases, or data provided in the manuscript.\\n\\nEukaryotic cell lines\\n\\nPolicy information about cell lines and Sex and Gender in Research\\n\\nCell line source(s)\\n\\nState the source of each cell line used and the sex of all primary cell lines and cells derived from human participants or vertebrate models.\\n\\nAuthentication'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Authentication\\n\\nDescribe the authentication procedures for each cell line used OR declare that none of the cell lines used were authenticated.\\n\\nMycoplasma contamination\\n\\nConfirm that all cell lines tested negative for mycoplasma contamination OR describe the results of the testing for mycoplasma contamination OR declare that the cell lines were not tested for mycoplasma contamination.\\n\\nCommonly misidentified lines (See ICLAC register)\\n\\nName any commonly misidentified cell lines used in the study and provide a rationale for their use.\\n\\nPalaeontology and Archaeology\\n\\nSpecimen provenance\\n\\nProvide provenance information for specimens and describe permits that were obtained for the work (including the name of the issuing authority, the date of issue, and any identifying information). Permits should encompass collection and, where applicable, export.\\n\\nSpecimen deposition\\n\\nIndicate where the specimens have been deposited to permit free access by other researchers.\\n\\nDating methods'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Specimen deposition\\n\\nIndicate where the specimens have been deposited to permit free access by other researchers.\\n\\nDating methods\\n\\nIf new dates are provided, describe how they were obtained (e.g. collection, storage, sample pretreatment and measurement), where they were obtained (i.e. lab name), the calibration program and the protocol for quality assurance OR state that no new dates are provided.\\n\\nTick this box to confirm that the raw and calibrated dates are available in the paper or in Supplementary Information.\\n\\nEthics oversight\\n\\nIdentify the organization(s) that approved or provided guidance on the study protocol, OR state that no ethical approval or guidance was required and explain why not.\\n\\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\\n\\nAnimals and other research organisms\\n\\nPolicy information about studies involving animals; ARRIVE guidelines recommended for reporting animal research, and Sex and Gender in Research'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Policy information about studies involving animals; ARRIVE guidelines recommended for reporting animal research, and Sex and Gender in Research\\n\\nLaboratory animals\\n\\nFor laboratory animals, report species, strain and age OR state that the study did not involve laboratory animals.\\n\\nWild animals\\n\\nProvide details on animals observed in or captured in the field; report species and age where possible. Describe how animals were caught and transported and what happened to captive animals after the study (if killed, explain why and describe method; if released, say where and when) OR state that the study did not involve wild animals.\\n\\nReporting on sex\\n\\nIndicate if findings apply to only one sex; describe whether sex was considered in study design, methods used for assigning sex. Provide data disaggregated for sex where this information has been collected in the source data as appropriate; provide overall\\n\\nn a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='n a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n4\\n\\nnumbers in this Reporting Summary. Please state if this information has not been collected. Report sex-based analyses where performed, justify reasons for lack of sex-based analysis.\\n\\nField-collected samples\\n\\nFor laboratory work with field-collected samples, describe all relevant parameters such as housing, maintenance, temperature, photoperiod and end-of-experiment protocol OR state that the study did not involve samples collected from the field.\\n\\nEthics oversight\\n\\nIdentify the organization(s) that approved or provided guidance on the study protocol, OR state that no ethical approval or guidance was required and explain why not.\\n\\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\\n\\nClinical data'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Note that full information on the approval of the study protocol must also be provided in the manuscript.\\n\\nClinical data\\n\\nPolicy information about clinical studies All manuscripts should comply with the ICMJE guidelines for publication of clinical research and a completed CONSORT checklist must be included with all submissions.\\n\\nClinical trial registration Provide the trial registration number from ClinicalTrials.gov or an equivalent agency.\\n\\nStudy protocol\\n\\nNote where the full trial protocol can be accessed OR if not available, explain why.\\n\\nData collection\\n\\nDescribe the settings and locales of data collection, noting the time periods of recruitment and data collection.\\n\\nOutcomes\\n\\nDescribe how you pre-defined primary and secondary outcome measures and how you assessed these measures.\\n\\nDual use research of concern\\n\\nPolicy information about dual use research of concern\\n\\nHazards'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Dual use research of concern\\n\\nPolicy information about dual use research of concern\\n\\nHazards\\n\\nCould the accidental, deliberate or reckless misuse of agents or technologies generated in the work, or the application of information presented in the manuscript, pose a threat to:\\n\\nNo Yes\\n\\nPublic health\\n\\nNational security\\n\\nCrops and/or livestock\\n\\nEcosystems\\n\\nAny other significant area\\n\\nExperiments of concern\\n\\nDoes the work involve any of these experiments of concern:\\n\\nNo Yes\\n\\nDemonstrate how to render a vaccine ineffective\\n\\nConfer resistance to therapeutically useful antibiotics or antiviral agents\\n\\nEnhance the virulence of a pathogen or render a nonpathogen virulent\\n\\nIncrease transmissibility of a pathogen\\n\\nAlter the host range of a pathogen\\n\\nEnable evasion of diagnostic/detection modalities\\n\\nEnable the weaponization of a biological agent or toxin\\n\\nAny other potentially harmful combination of experiments and agents\\n\\nPlants\\n\\nSeed stocks'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Enable the weaponization of a biological agent or toxin\\n\\nAny other potentially harmful combination of experiments and agents\\n\\nPlants\\n\\nSeed stocks\\n\\nReport on the source of all seed stocks or other plant material used. If applicable, state the seed stock centre and catalogue number. If plant specimens were collected from the field, describe the collection location, date and sampling procedures.\\n\\nNovel plant genotypes\\n\\nDescribe the methods by which all novel plant genotypes were produced. This includes those generated by transgenic approaches, gene editing, chemical/radiation-based mutagenesis and hybridization. For transgenic lines, describe the transformation method, the number of independent lines analyzed and the generation upon which experiments were performed. For gene-edited lines, describe the editor used, the endogenous sequence targeted for editing, the targeting guide RNA sequence (if applicable) and how the editor\\n\\nn a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='n a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n5\\n\\nwas applied.\\n\\nAuthentication\\n\\nDescribe any authentication procedures for each seed stock used or novel genotype generated. Describe any experiments used to assess the effect of a mutation and, where applicable, how potential secondary effects (e.g. second site T-DNA insertions, mosiacism, off-target gene editing) were examined.\\n\\nChIP-seq\\n\\nData deposition\\n\\nConfirm that both raw and final processed data have been deposited in a public database such as GEO.\\n\\nConfirm that you have deposited or provided access to graph files (e.g. BED files) for the called peaks.\\n\\nData access links May remain private before publication.\\n\\nFor \"Initial submission\" or \"Revised version\" documents, provide reviewer access links. For your \"Final submission\" document, provide a link to the deposited data.\\n\\nFiles in database submission\\n\\nProvide a list of all files available in the database submission.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Files in database submission\\n\\nProvide a list of all files available in the database submission.\\n\\nGenome browser session (e.g. UCSC)\\n\\nProvide a link to an anonymized genome browser session for \"Initial submission\" and \"Revised version\" documents only, to enable peer review. Write \"no longer applicable\" for \"Final submission\" documents.\\n\\nMethodology\\n\\nReplicates\\n\\nDescribe the experimental replicates, specifying number, type and replicate agreement.\\n\\nSequencing depth\\n\\nDescribe the sequencing depth for each experiment, providing the total number of reads, uniquely mapped reads, length of reads and whether they were paired- or single-end.\\n\\nAntibodies\\n\\nDescribe the antibodies used for the ChIP-seq experiments; as applicable, provide supplier name, catalog number, clone name, and lot number.\\n\\nPeak calling parameters\\n\\nSpecify the command line program and parameters used for read mapping and peak calling, including the ChIP, control and index files used.\\n\\nData quality'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content=\"Data quality\\n\\nDescribe the methods used to ensure data quality in full detail, including how many peaks are at FDR 5% and above 5-fold enrichment.\\n\\nSoftware\\n\\nDescribe the software used to collect and analyze the ChIP-seq data. For custom code that has been deposited into a community repository, provide accession details.\\n\\nFlow Cytometry\\n\\nPlots\\n\\nConfirm that:\\n\\nThe axis labels state the marker and fluorochrome used (e.g. CD4-FITC).\\n\\nThe axis scales are clearly visible. Include numbers along axes only for bottom left plot of group (a 'group' is an analysis of identical markers).\\n\\nAll plots are contour plots with outliers or pseudocolor plots.\\n\\nA numerical value for number of cells or percentage (with statistics) is provided.\\n\\nMethodology\\n\\nSample preparation\\n\\nDescribe the sample preparation, detailing the biological source of the cells and any tissue processing steps used.\\n\\nInstrument\\n\\nIdentify the instrument used for data collection, specifying make and model number.\\n\\nSoftware\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Instrument\\n\\nIdentify the instrument used for data collection, specifying make and model number.\\n\\nSoftware\\n\\nDescribe the software used to collect and analyze the flow cytometry data. For custom code that has been deposited into a community repository, provide accession details.\\n\\nCell population abundance\\n\\nDescribe the abundance of the relevant cell populations within post-sort fractions, providing details on the purity of the samples and how it was determined.\\n\\nGating strategy\\n\\nDescribe the gating strategy used for all relevant experiments, specifying the preliminary FSC/SSC gates of the starting cell population, indicating where boundaries between \"positive\" and \"negative\" staining cell populations are defined.\\n\\nTick this box to confirm that a figure exemplifying the gating strategy is provided in the Supplementary Information.\\n\\nn a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n6\\n\\nMagnetic resonance imaging\\n\\nExperimental design\\n\\nDesign type'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='l i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n6\\n\\nMagnetic resonance imaging\\n\\nExperimental design\\n\\nDesign type\\n\\nIndicate task or resting state; event-related or block design.\\n\\nDesign specifications\\n\\nSpecify the number of blocks, trials or experimental units per session and/or subject, and specify the length of each trial or block (if trials are blocked) and interval between trials.\\n\\nBehavioral performance measures\\n\\nState number and/or type of variables recorded (e.g. correct button press, response time) and what statistics were used to establish that the subjects were performing the task as expected (e.g. mean, range, and/or standard deviation across subjects).\\n\\nAcquisition\\n\\nImaging type(s)\\n\\nSpecify: functional, structural, diffusion, perfusion.\\n\\nField strength\\n\\nSpecify in Tesla\\n\\nSequence & imaging parameters'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Imaging type(s)\\n\\nSpecify: functional, structural, diffusion, perfusion.\\n\\nField strength\\n\\nSpecify in Tesla\\n\\nSequence & imaging parameters\\n\\nSpecify the pulse sequence type (gradient echo, spin echo, etc.), imaging type (EPI, spiral, etc.), field of view, matrix size, slice thickness, orientation and TE/TR/flip angle.\\n\\nArea of acquisition\\n\\nState whether a whole brain scan was used OR define the area of acquisition, describing how the region was determined.\\n\\nDiffusion MRI\\n\\nUsed\\n\\nNot used\\n\\nPreprocessing\\n\\nPreprocessing software\\n\\nProvide detail on software version and revision number and on specific parameters (model/functions, brain extraction, segmentation, smoothing kernel size, etc.).\\n\\nNormalization\\n\\nIf data were normalized/standardized, describe the approach(es): specify linear or non-linear and define image types used for transformation OR indicate that data were not normalized and explain rationale for lack of normalization.\\n\\nNormalization template'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Normalization template\\n\\nDescribe the template used for normalization/transformation, specifying subject space or group standardized space (e.g. original Talairach, MNI305, ICBM152) OR indicate that the data were not normalized.\\n\\nNoise and artifact removal\\n\\nDescribe your procedure(s) for artifact and structured noise removal, specifying motion parameters, tissue signals and physiological signals (heart rate, respiration).\\n\\nVolume censoring\\n\\nDefine your software and/or method and criteria for volume censoring, and state the extent of such censoring.\\n\\nStatistical modeling & inference\\n\\nModel type and settings\\n\\nSpecify type (mass univariate, multivariate, RSA, predictive, etc.) and describe essential details of the model at the first and second levels (e.g. fixed, random or mixed effects; drift or auto-correlation).\\n\\nEffect(s) tested'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Effect(s) tested\\n\\nDefine precise effect in terms of the task or stimulus conditions instead of psychological concepts and indicate whether ANOVA or factorial designs were used.\\n\\nSpecify type of analysis:\\n\\nWhole brain\\n\\nROI-based\\n\\nBoth\\n\\nStatistic type for inference\\n\\nSpecify voxel-wise or cluster-wise and report all relevant parameters for cluster-wise methods.\\n\\n(See Eklund et al. 2016)\\n\\nCorrection\\n\\nDescribe the type of correction and how it is obtained for multiple comparisons (e.g. FWE, FDR, permutation or Monte Carlo).\\n\\nModels & analysis\\n\\nn/a Involved in the study\\n\\nFunctional and/or effective connectivity\\n\\nGraph analysis\\n\\nMultivariate modeling or predictive analysis\\n\\nFunctional and/or effective connectivity\\n\\nReport the measures of dependence used and the model details (e.g. Pearson correlation, partial correlation, mutual information).\\n\\nGraph analysis\\n\\nReport the dependent variable and connectivity measure, specifying weighted graph or binarized graph,\\n\\nn a t u r e p o r t f o\\n\\nl i'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/s41586-023-06415-8.pdf'}, page_content='Graph analysis\\n\\nReport the dependent variable and connectivity measure, specifying weighted graph or binarized graph,\\n\\nn a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n7\\n\\nGraph analysis\\n\\nMultivariate modeling and predictive analysis\\n\\nsubject- or group-level, and the global and/or node summaries used (e.g. clustering coefficient, efficiency, etc.).\\n\\nSpecify independent variables, features extraction and dimension reduction, model, training and evaluation metrics.\\n\\nn a t u r e p o r t f o\\n\\nl i\\n\\no\\n\\n|\\n\\nr e p o r t i n g s u m m a r y\\n\\nA p r i l\\n\\n2 0 2 3\\n\\n8'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='7 1 0 2\\n\\ny a M 7 1\\n\\n]\\n\\nG L . s c [\\n\\n2 v 6 7 0 7 0 . 3 0 7 1 : v i X r a\\n\\nSMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules\\n\\nEsben Jannik Bjerrum*\\n\\nWildcard Pharmaceutical Consulting, Frødings Allé 41, 2860 Søborg, Denmark *) esben@wildcardconsulting.dk\\n\\nAbstract'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='Simpliﬁed Molecular Input Line Entry System (SMILES) is a single line text representation of a unique molecule. One molecule can however have multiple SMILES strings, which is a reason that canonical SMILES have been deﬁned, which ensures a one to one correspondence between SMILES string and molecule. Here the fact that multiple SMILES represent the same molecule is explored as a technique for data augmentation of a molecular QSAR dataset modeled by a long short term memory (LSTM) cell based neural network. The augmented dataset was 130 times bigger than the original. The network trained with the augmented dataset shows better performance on a test set when compared to a model built with only one canonical SMILES string per molecule. The correlation coeﬃcient R2 on the test set was improved from 0.56 to 0.66 when using SMILES enumeration, and the root mean square error (RMS) likewise fell from 0.62 to 0.55. The technique also works in the prediction phase. By taking the average per'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='and the root mean square error (RMS) likewise fell from 0.62 to 0.55. The technique also works in the prediction phase. By taking the average per molecule of the predictions for the enumerated SMILES a further improvement to a correlation coeﬃcient of 0.68 and a RMS of 0.52 was found.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='Introduction\\n\\nshown Neural networks and deep learning has image such as interesting application successes, classiﬁcation[1], and speech recognition[2]. One of the issues that limits their general applicability in the QSAR domain may be the limited sizes of the labeled datasets available, although successes do appear.[3] Limited datasets necessitates harsh regularization or shallow and narrow architectures. Within image anal- ysis and classiﬁcation, data augmentation techniques has been used with excellent results.[4, 5, 6, 7] As an example, a dataset of labeled images can be enlarged by operations such as mirroring, rotation, morphing and zooming. The afterwards trained network gets more robust towards such variations and the neural network can recognize the same object in diﬀerent versions.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='Neural networks has also been used on molec- ular data, where the input may be calculated descriptors,[3] neural network interpretation of the molecular graph[8] or also SMILES representations.[9] Simpliﬁed Molecular Input Line Entry System (SMILES) is a single line text based molecular nota- tion format.[10] A single molecule has multiple pos- sible SMILES strings, which has led to the deﬁni- tion of a canonical SMILES,[11] which ensures that a molecule corresponds to a single SMILES string. The possibilities for variation in the SMILES strings of simple molecules are limited. Propane has two possibilities CCC and C(C)C. But as the molecule gets larger in size and more complex in branching,\\n\\nTolueneSMILESEnumerationCc1ccccc1c1ccccc1Cc1(C)ccccc1c1c(C)cccc1c1cc(C)ccc1c1ccc(C)cc1c1cccc(C)c1'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='TolueneSMILESEnumerationCc1ccccc1c1ccccc1Cc1(C)ccccc1c1c(C)cccc1c1cc(C)ccc1c1ccc(C)cc1c1cccc(C)c1\\n\\nFigure 1: SMILES enumeration enables data augmen- tation. The molecule toluene corresponds to seven dif- ferent SMILES, the top one is the canonical smile. One data point with toluene in the dataset would thus leads to seven samples in the augmented dataset.\\n\\nthe number of possible SMILES strings grows rapidly. Toluene with seven atoms, has seven possible SMILES strings (Figure 1).\\n\\nHere data augmentation of molecular structures with SMILES enumeration for QSAR studies will be investigated using long short term memory (LSTM) cell neural networks inspired by networks used for Twitter tweets sentiment analysis.[12]\\n\\n1\\n\\nMethods\\n\\nSMILES enumeration'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='1\\n\\nMethods\\n\\nSMILES enumeration\\n\\nSMILES enumeration was done with a Python script utilizing the cheminformatics library RDKit.[13] The atom ordering of the molecule is scrambled ran- domly by converting to molﬁle format[14] and chang- ing the atom order, before converting back to the RDKit mol A SMILES is then gener- ated using RDKit with the default option of pro- ducing canonical SMILES set to false, where dif- ferent atom orderings lead to diﬀerent SMILES. The SMILES strings is then compared and possible added to a growing set of unique SMILES strings. The process is repeated a predeﬁned number of times. The python functions are available on github: https://github.com/Ebjerrum/SMILES-enumeration\\n\\nformat.\\n\\nMolecular dataset'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='format.\\n\\nMolecular dataset\\n\\nThe dataset was obtained from Sutherland et al 2003.[15] It consists of 756 dihydrofolate inhibitors with P. carinii DHFR inhibition data. The dataset was split in test and a training set in a 1:9 ratio. It was expanded with SMILES enumeration and the SMILES strings were padded with spaces to ﬁxed length of 74, which is one characters longer than the longest SMILES in the dataset. It was subsequently vectorized by one-hot encoding the characters into a bit matrix with one bit set for the corresponding char- acter in each row using a generated char to int dic- tionary. Molecules where the associated aﬃnity was not a number were removed. The associated IC50 data was converted to log IC50 and normalized to unit variance and mean zero with utilities from Scikit- learn.[16]\\n\\nLSTM neural network'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='LSTM neural network\\n\\nTwo diﬀerent neural networks were built and trained using Keras version 1.1.2[17] with Theano v. 0.8.0[18] as back end. One or more LSTM layers were used in batch mode, and the ﬁnal state fed to a feed-forward neural network with a single linear output neuron. The network layout was optimized using Bayesian op- timization with Gaussian processes as implemented in the Python package GpyOpt[19] version 1.0.3, vary- ing the hyper parameters listed in Table 1. 10 ini- tial trainings was done before using the GP_MCMC and the EI_MCMC acquisition function to sample new hyper parameter sets.[20] One network was op- timized and trained only using a dataset with canon- ical SMILES, whereas the other were optimized and trained with the dataset that expanded with SMILES enumeration. In the rest of the publication they will be referred to as the canonical model and enumerated model, respectively.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='All computations and training were done on a Linux workstation (Ubuntu Mate 16.04) with 4 GB of\\n\\n2\\n\\nram, i5-2405S CPU @ 2.50GHz and an Nvidia Geforce GTX1060 graphics card with 6 GB of ram.\\n\\nResults\\n\\nFiltering, splitting and SMILES enumeration re- sulted in a canonical SMILES dataset with 602 train molecules and 71 test molecules, whereas the enumer- ated dataset had 79143 and 9412 rows for train and test, respectively. This corresponds to an augmenta- tion factor of approximately 130. Each molecule had on average 130 alternative SMILES representations. Optimization of the architecture yielded two diﬀer- ent best conﬁgurations of hyper parameters, depend- ing on the dataset used. The best hyper parameters found for each dataset are shown in Table 2.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='The train history is shown in Figure 2. The best neural network trained on the canonical dataset had a loss of 0.44 including regularization penalty and a mean square error of 0.22 and 0.41 for train and test set, respectively. The curves for the training us- ing the canonical dataset are very noisy (Figure 2A). The best neural network trained on the enumerated dataset loss of 0.18 including regularization penalty and a mean square error of 0.09 and 0.30 for train and test set, respectively. The training curve is sig- niﬁcantly less noisy than for the canonical dataset (Figure 2B).\\n\\nBoth neural networks were used to predict the IC50 values from the canonical and enumerated datasets, and the scatter plots are shown in Figure 3.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='Both neural networks were used to predict the IC50 values from the canonical and enumerated datasets, and the scatter plots are shown in Figure 3.\\n\\nThe correlation coeﬃcients and root mean square deviation (RMS) are tabulated in Table 3. The combination with the worst performance was pre- dicting the test set molecules is using enumerated SMILES neural network model trained on the canon- ical dataset. Which has a correlation coeﬃcient of 0.26 and an RMS of 0.84. The bad correlation is clearly visible from Figure 3 plot C. The best per- formance predicting the test set, was seen with the combination of the enumerated model and the enu- merated SMILES. Here the correlation coeﬃcient is 0.66 and the RMS 0.55. The two other combinations, canonical model-canonical SMILES and enumerated model, canonical SMILES are close in performance (Table 3).2'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='Figure 4 show a scatter plot of the average predic- tion for each molecule obtained with the enumerated model. The calculated correlation coeﬃcient is 0.68 for the test set and the RMS is 0.52.\\n\\nDiscussion\\n\\nThe results clearly suggest that SMILES enumera- tion as a data augmentation technique for molecular data has beneﬁts. The model trained on canonical data is not able to predict many of the alternative SMILES of the train and test set as is evident for\\n\\nTable 1: Hyper parameter Search Space\\n\\nParameter Number of LSTM layers Number of units in LSTM layers Dropout for input gates (dropout_W) Dropout for recurrent connection (dropout_U) Number of dense hidden layers Hidden layer size Weight regularization on dense layer, L1 Weight regularization on dense layer, L2 Learning rate\\n\\nSearch Space [1,2] [32, 64, 128, 256] 0 – 0.2 0 – 0.5 [0,1] [4, 8, 16, 32, 64, 128] 0 – 0.2 0 – 0.2 0.05-0.0001\\n\\nType Discrete Discrete Continuous Continuous Discrete Discrete Continuous Continuous Continuous'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='Type Discrete Discrete Continuous Continuous Discrete Discrete Continuous Continuous Continuous\\n\\nTable 2: Best Hyperparameters found\\n\\nParameter\\n\\nCanonical Model Enumerated Model\\n\\nNumber of LSTM layers Number of units in LSTM layers Dropout for input gates (dropout_W) Dropout for recurrent connections (dropout_U) Number of dense hidden layers Hidden layer size Weight regularization on dense layer, L1 Weight regularization on dense layer, L2 Learning rate\\n\\n1 128 0.0 0.0 0 N/A 0.2 0.2 0.0001\\n\\n1 64 0.19 0.0 0 N/A 0.005 0.01 0.005\\n\\nFigure 2: Training history for the two datasets and neural networks. A: Neural network trained on canonical SMILES shows a noisy curve where the best model has a test loss of 0.41. B: Neural network trained on enumerated SMILES obtains the best model with a test loss of 0.30. Blue lines are the mean square error without regularization penalty, green is loss including regularization penalty and the red line is mean square error on the test set.\\n\\n3'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='3\\n\\nFigure 3: Scatter plots of predicted vs. true values. Left column shows scatter plots obtained with the model trained on canonical SMILES only. Right column shows predictions with the model trained on enumerated data. Top row is scatter plots with only canonical SMILES and bottom row is predictions of the enumerated dataset. The blue line denotes the perfect correlation (y = x).\\n\\n4\\n\\nTable 3: Statistics of predicted values, values are for Train/Test set respectively\\n\\nDataset\\n\\nTrain Test Enumerated Train Test\\n\\nCanonical\\n\\nCanonical Model R2 0.78 0.56 0.25 0.26\\n\\nRMS 0.46 0.62 0.88 0.84\\n\\nEnumerated\\n\\nModel R2 0.85 0.63 0.87 0.66\\n\\nRMS 0.39 0.56 0.37 0.55\\n\\nFigure 4: Average of predictions from the enumerated model for each molecule. Train set R2 is 0.88 and RMS is 0.38. Test set R2 is 0.68 and RMS is 0.52.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='Figure 3 plot C, where the bad generalization to non canonical SMILES strings are evident. Instead the best performance was observed by taking the average for each molecule of the predictions of the enumer- ated SMILES using the enumerated model (Figure 4), which shows that the SMILES enumeration can also be of value in the sampling phase. The canonical model needed a lot more epochs to train, but here it must be considered that the dataset contained 130 times fewer examples. Thus each epoch in the train- ing was only 3 mini batches leading to 3 updates of the weights, whereas the enumerated dataset had ap- proximately 360 updates of the weights of the neural network per epoch. The curves in Figure 2 thus rep- resents 3000 and 18000 updates of the weights. The higher overhead of running more epochs however led to approximately the same wall clock time in train- ing. The hyper parameters found during the opti- mization of the network architecture and amount of regularization was not'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='wall clock time in train- ing. The hyper parameters found during the opti- mization of the network architecture and amount of regularization was not entirely as expected. The ex- pectation was that the canonical dataset would prefer a smaller and simpler network with a larger regular- ization. Instead the canonical dataset has a larger amount of LSTM cells (128) with no dropout, but a much larger regularization of the ﬁnal weights to the input neuron (L1 and L2 maxed out at 0.2). The enu- merated model had fewer LSTM cells (64) and thus fewer connections, but nevertheless found dropout on the input to the LSTM cells to be beneﬁcial. To test if'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='5'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='the diﬀerences were due to the Bayesian optimization getting trapped in a local minimum, the network ar- chitecture found for the enumerated dataset was test trained with the dataset with the canonical SMILES only. The ﬁrst try with a learning rate of 0.005 failed (results not shown), but lowering the learning rate to the one found for the canonical SMILES (0.0001), gave a model with a correlation coeﬃcient of 0.5 and RMS of 0.68 on the train set. The predictive per- formance was even lower with 0.45 and 0.69, for R2 and RMS respectively. The diﬀerences in hyper pa- rameters after optimization of using the two diﬀerent datasets thus seems justiﬁed. The study lacks the division into train, test and validation set, where the hyper parameters are tuned on the test set, but the ﬁ- nal performance evaluated on the validation set. The observed prediction performance of the LSTM-QSAR models are thus likely overestimated to some degree. However, this study is focused on the gains of us- ing'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='prediction performance of the LSTM-QSAR models are thus likely overestimated to some degree. However, this study is focused on the gains of us- ing SMILES enumeration and not on producing the optimal DHFR QSAR model. The performance on both the train and test set are lower for the canonical model. If the diﬀerences in performance had been due to to over-ﬁtting, the smaller dataset would probably have had an advantage.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='The use of SMILES as descriptors for QSAR is not new[21, 22, 21] and is as an example implemented in the CORAL software.[23] The approach in the CORAL software is however very diﬀerent from the one in this study. CORAL software breaks down the SMILES into single atoms, double atoms and triple atoms (Sk, SSk and SSSk) as well as some extra man- ually coded extracted features such as BOND, NOSP, HALO and PAIR.[23, 21] The approach seems close to using a mixture of topological torsions[24] with one, two and three atoms and atom-pair[25] ﬁngerprints. The LSTM-QSAR used in this approach directly uses the SMILES string and supposedly let the model best extract the features from the SMILES strings that best ﬁt with the task at hand, and similar approaches have been shown to outperform other common ma- chine learning algorithms[22], although the details of optimization of the competing algorithms were not completely clear.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='SMILES were also used recently in an application of a neural network based auto-encoder.[9] Here the SMILES are used as input to a neural network with the task of recreating the input sequence. The infor- mation is passed through a “bottle-neck” layer in be- tween the encoder and the decoder, which limits the direct transfer of information. The bottle neck layer thus ends up as a more continuous ﬂoating point vec- tor representation of the molecule, which can be used to explore the chemical space near an input molecule, interpolate between molecules and link the vector representation to physico-chemical properties. The amount of unlabeled molecules for the study already surpassed the needed amount, but could in principle be expanded even more with the SMILES enumera-'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='tion technique described here. SMILES enumeration could possible allow the autoencoder to be trained with smaller and more focused datasets of biological interest. Additionally, tt would be interesting to see if diﬀerent SMILES of the same molecule ends up with the same vector representation or in entirely diﬀerent areas in the continuous molecular representations.\\n\\nLSTM networks have also been used in QSAR ap- plications demonstrating learning transfer from large datasets to smaller.[26] Here the input was however not SMILES strings but rather molecular graph con- volution layers[27] working directly on the molecular graph representation. The approach thus more di- rectly reads in the topology of the molecular model, rather than indirectly letting the network infer the topology from the SMILES branching and ring clo- sures deﬁned by the brackets and numbering in the SMILES strings.\\n\\nConclusion'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='Conclusion\\n\\nThis short investigation has shown promise in us- ing SMILES enumeration as a data augmentation technique for neural network QSAR models based on SMILES data. SMILES enumeration enables the use of more limited sizes of labeled data sets for use in modeling by more complex neural net- work models. SMILES enumeration gives more robust QSAR models both when predicting sin- gle SMILES, but even more when taking the av- erage prediction using enumerated SMILES for the same molecule. The SMILES enumeration code as well as some of the scripts used for generating the LSTM-QSAR models are available on GitHub: https://github.com/Ebjerrum/SMILES-enumeration\\n\\nConﬂicts of Interest\\n\\nE. J. Bjerrum is the owner of Wildcard Pharmaceu- tical Consulting. The company is usually contracted by biotechnology/pharmaceutical companies to pro- vide third party services\\n\\nReferences'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='References\\n\\n[1] P. Y. Simard, D. Steinkraus, J. C. Platt, et al., Best practices for convolutional neural networks applied to visual document analysis., in: ICDAR, Vol. 3, Citeseer, 2003, pp. 958–962.\\n\\n[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al., Deep neural networks for acoustic modeling in speech recog- nition: The shared views of four research groups, IEEE Signal Processing Magazine 29 (6) (2012) 82–97.\\n\\n6\\n\\n[3] A. Mayr, G. Klambauer, T. Unterthiner, S. Hochreiter, Deeptox: toxicity prediction using deep learning, Frontiers in Environmental Sci- ence 3 (2016) 80.\\n\\n[4] A.-D. Almási,\\n\\nS. Woźniak, V. Cristea, Y. Leblebici, T. Engbersen, Review of ad- vances in neural networks: Neural design technology stack, Neurocomputing 174 (2016) 31–41.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='[5] K. Chatﬁeld, K. Simonyan, A. Vedaldi, A. Zis- serman, Return of the devil in the details: Delv- ing deep into convolutional nets, arXiv preprint arXiv:1405.3531.\\n\\n[6] X. Cui, V. Goel, B. Kingsbury, Data aug- mentation for deep neural network acoustic IEEE/ACM Trans. Audio, Speech modeling, (2015) 1469–1477. and Lang. Proc. 23 (9) doi:10.1109/TASLP.2015.2438544. URL 2015.2438544\\n\\nhttp://dx.doi.org/10.1109/TASLP.\\n\\n[7] J. Schmidhuber, Deep learning in neural net- works: An overview, Neural networks 61 (2015) 85–117.\\n\\n[8] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, P. Riley, Molecular graph convolutions: moving beyond ﬁngerprints, Journal of computer-aided molecular design 30 (8) (2016) 595–608.\\n\\n[9] R. Gómez-Bombarelli, D. Duvenaud, J. M. Hernández-Lobato, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, A. Aspuru-Guzik, Automatic chemical design using a data-driven continuous representation of molecules, arXiv preprint arXiv:1610.02415.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='[10] D. Weininger, Smiles, a chemical language and information system. 1. introduction to method- ology and encoding rules, in: Proc. Edinburgh Math. SOC, Vol. 17, 1970, pp. 1–14.\\n\\n[11] N. M. O’Boyle, Towards a universal smiles rep- resentation - a standard method to generate canonical smiles based on the inchi., Journal of cheminformatics 4 (2012) 22. doi:10.1186/ 1758-2946-4-22.\\n\\n[12] D. Tang, B. Qin, X. Feng, T. Liu, Target- dependent sentiment classiﬁcation with long short term memory, CoRR, abs/1512.01100.\\n\\n[13] G. A. Landrum, Rdkit: Open-source cheminfor-\\n\\nmatics software (2016). URL //github.com/rdkit/rdkit\\n\\nhttp://www.rdkit.org/,https:\\n\\n[14] Ctﬁle formats, http://accelrys.com/products/- informatics/cheminformatics/ctﬁle-formats/no- fee.php (Dec 2011).\\n\\nURL informatics/cheminformatics/ ctfile-formats/no-fee.php\\n\\nhttp://accelrys.com/products/'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='URL informatics/cheminformatics/ ctfile-formats/no-fee.php\\n\\nhttp://accelrys.com/products/\\n\\n[15] J. J. Sutherland, L. A. O’Brien, D. F. Weaver, Spline-ﬁtting with a genetic algorithm: a method for developing classiﬁcation structure-activity re- lationships., Journal of chemical information and computer sciences 43 (2003) 1906–1915. doi: 10.1021/ci034143r.\\n\\n[16] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Van- derplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine Learning Research 12 (2011) 2825–2830.\\n\\n[17] F.\\n\\nChollet, https://github.com/fchollet/keras (2015).\\n\\nkeras,'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='[18] R. Al-Rfou, G. Alain, A. Almahairi, C. Anger- mueller, D. Bahdanau, N. Ballas, F. Bastien, J. Bayer, A. Belikov, A. Belopolsky, Y. Ben- gio, A. Bergeron, J. Bergstra, V. Bisson, J. Bleecher Snyder, N. Bouchard, N. Boulanger- Lewandowski, X. Bouthillier, A. de Brébis- son, O. Breuleux, P.-L. Carrier, K. Cho, J. Chorowski, P. Christiano, T. Cooijmans, M.-A. Côté, M. Côté, A. Courville, Y. N. Dauphin, O. Delalleau, J. Demouth, G. Des- jardins, S. Dieleman, L. Dinh, M. Ducoﬀe, V. Dumoulin, S. Ebrahimi Kahou, D. Er- han, Z. Fan, O. Firat, M. Germain, X. Glo- rot, I. Goodfellow, M. Graham, C. Gulcehre, P. Hamel, I. Harlouchet, J.-P. Heng, B. Hi- dasi, S. Honari, A. Jain, S. Jean, K. Jia, M. Korobov, V. Kulkarni, A. Lamb, P. Lam- blin, E. Larsen, C. Laurent, S. Lee, S. Lefran- cois, S. Lemieux, N. Léonard, Z. Lin, J. A. Livezey, C. Lorenz, J. Lowin, Q. Ma, P.-A. Manzagol, O. Mastropietro, R. T. McGibbon, R. Memisevic, B. van Merriënboer, V. Michal- ski, M. Mirza, A. Orlandi, C. Pal, R.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='J. Lowin, Q. Ma, P.-A. Manzagol, O. Mastropietro, R. T. McGibbon, R. Memisevic, B. van Merriënboer, V. Michal- ski, M. Mirza, A. Orlandi, C. Pal, R. Pascanu, M. Pezeshki, C. Raﬀel, D. Renshaw, M. Rock- lin, A. Romero, M. Roth, P. Sadowski, J. Sal- vatier, F. Savard, J. Schlüter, J. Schulman, G. Schwartz, I. V. Serban, D. Serdyuk, S. Sha- banian, E. Simon, S. Spieckermann, S. R. Subra- manyam, J. Sygnowski, J. Tanguay, G. van Tul- der, J. Turian, S. Urban, P. Vincent, F. Visin, H. de Vries, D. Warde-Farley, D. J. Webb, M. Willson, K. Xu, L. Xue, L. Yao, S. Zhang, Y. Zhang, Theano: A Python framework for fast computation of mathematical expressions, arXiv e-prints abs/1605.02688. URL http://arxiv.org/abs/1605.02688'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='[19] T. G.\\n\\nauthors, Gpyopt:\\n\\noptimization\\n\\nframework\\n\\nA bayesian python,\\n\\nin\\n\\n7\\n\\nhttp://github.com/SheﬃeldML/GPyOpt (2016).\\n\\n[20] C. Wang, R. M. Neal, Mcmc methods for gaus- sian process models using fast approximations for the likelihood, arXiv preprint arXiv:1305.2235.\\n\\n[21] A. Worachartcheewan, P. Mandi, V. Prachay- asittikul, A. P. Toropova, A. A. Toropov, C. Nantasenamat, Large-scale qsar study of aro- matase inhibitors using smiles-based descriptors, Chemometrics and Intelligent Laboratory Sys- tems 138 (2014) 120–126.\\n\\n[22] S. Jastrzebski, D. Lesniak, W. M. Czarnecki,\\n\\nLearning to SMILE(S), CoRR abs/1602.06289. URL http://arxiv.org/abs/1602.06289\\n\\n[23] A. P. Toropova, A. A. Toropov, Coral software: prediction of carcinogenicity of drugs by means of the monte carlo method, European Journal of Pharmaceutical Sciences 52 (2014) 21–25.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1703.07076.pdf'}, page_content='[24] R. Nilakantan, N. Bauman, J. S. Dixon, R. Venkataraghavan, Topological torsion: a new molecular descriptor for sar applications. com- parison with other descriptors, Journal of Chem- ical Information and Computer Sciences 27 (2) (1987) 82–85.\\n\\n[25] R. E. Carhart, D. H. Smith, R. Venkataraghavan, Atom pairs as molecular features in structure- activity studies: deﬁnition and applications, Journal of Chemical Information and Computer Sciences 25 (2) (1985) 64–73.\\n\\n[26] H. Altae-Tran, B. Ramsundar, A. S. Pappu, V. Pande, Low data drug discovery with one-shot learning, arXiv preprint arXiv:1611.03199.\\n\\n[27] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, R. P. Adams, Convolutional networks on graphs for learning molecular ﬁngerprints, in: Advances in neural information processing systems, 2015, pp. 2224–2232.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='9 1 0 2\\n\\nv o N 2 1\\n\\n]\\n\\nG L . s c [\\n\\n1 v 8 3 7 4 0 . 1 1 9 1 : v i X r a\\n\\nSMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery\\n\\nShion Honda1,2,3, Shoi Shi1,2,3, Hiroki R. Ueda1,2,3 1University of Tokyo 2International Research Center for Neurointelligence 3RIKEN Center for Biosystems Dynamics Research shion honda@ipc.i.u-tokyo.ac.jp, {sshoi0322-tky,uedah-tky}@umin.ac.jp\\n\\nAbstract\\n\\ncompound-protein afﬁnity prediction ( ¨Ozt¨urk, ¨Ozg¨ur, and Ozkirimli 2018).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='In drug-discovery-related tasks such as virtual screening, machine learning is emerging as a promising way to pre- dict molecular properties. Conventionally, molecular ﬁnger- prints (numerical representations of molecules) are calculated through rule-based algorithms that map molecules to a sparse discrete space. However, these algorithms perform poorly for shallow prediction models or small datasets. To address this issue, we present SMILES Transformer. Inspired by Trans- former and pre-trained language models from natural lan- guage processing, SMILES Transformer learns molecular ﬁn- gerprints through unsupervised pre-training of the sequence- to-sequence language model using a huge corpus of SMILES, a text representation system for molecules. We performed benchmarks on 10 datasets against existing ﬁngerprints and graph-based methods and demonstrated the superiority of the proposed algorithms in small-data settings where pre-training facilitated good generalization. Moreover, we'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='and demonstrated the superiority of the proposed algorithms in small-data settings where pre-training facilitated good generalization. Moreover, we deﬁne a novel metric to concurrently measure model accuracy and data efﬁ- ciency.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='In order to apply machine learning to drug discovery, molecular data must be transformed into a readable format for machine learning. One major approach is to transform molecular data into a simpliﬁed molecular input line en- try system (SMILES), a text representation of molecules that is commonly used in many databases (Xu et al. 2017; G´omez-Bombarelli et al. 2018). Recently, graph-based ap- proaches (Duvenaud et al. 2015; Kearnes et al. 2016) have been proposed, which usually show better performance than text-based approaches, such as SMILES, in QSPR tasks. In these studies, the models are designed for large fully-labeled training data settings, which requires huge labeled datasets and a QSPR model for one-shot learning (Altae-Tran et al. 2017). However, in most cases, it is difﬁcult to prepare large labeled datasets of experimentally validated molecular properties or afﬁnities to proteins, so that graph-based ap- proaches might have limited application. Therefore, the de-'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='validated molecular properties or afﬁnities to proteins, so that graph-based ap- proaches might have limited application. Therefore, the de- velopment of a high-performing algorithm for small datasets will be required.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='1\\n\\nIntroduction\\n\\nRecently, deep learning has emerged as a powerful machine learning technology. When applied to big data, deep learn- ing can show equal or even better performance than humans in many domains such as computer vision (He et al. 2016), natural language processing (NLP) (Devlin et al. 2019; Yang et al. 2019), making decisions (Silver et al. 2017), and medicine (Jin, Barzilay, and Jaakkola 2018). Based on pro- jected performance benchmarks, deep learning is expected to be useful a tool to handle time-consuming tasks.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Drug discovery is a process to ﬁnd a new drug for a dis- ease of interest from a chemical library and validate its efﬁ- cacy and safety in clinical trials. This process usually takes more than a decade and is costly, and therefore may be im- provable by deep learning methods. Indeed, deep learning has been applied to the process of drug discovery including quantitative structure-property relationships (QSPR) predic- tion (Duvenaud et al. 2015; Xu et al. 2017), molecule gener- ation and lead optimization (G´omez-Bombarelli et al. 2018; Jin, Barzilay, and Jaakkola 2018), retrosynthesis planning (Segler, Preuss, and Waller 2018; Schwaller et al. 2018), and'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Given recent progress in the NLP ﬁeld (Peters et al. 2018; Devlin et al. 2019; Yang et al. 2019), a pre-training approach may be a promising way to address this challenge. Lan- guage model pre-training can exploit huge unlabeled cor- pora to learn the representations of words and sentences and then the pre-trained model is ﬁne-tuned to downstream tasks using a relatively smaller set of labeled data. In- deed, pre-training approaches have been implemented in the cheminformatics ﬁeld: a pre-trained sequence-to-sequence learning models (seq2seq) composed of RNNs (Sutskever, Vinyals, and Le 2014) or variational autoencoders (VAE) (Kingma and Welling 2014) by decoding SMILES from the learned representations (G´omez-Bombarelli et al. 2018; Xu et al. 2017; Kusner, Paige, and Hern´andez-Lobato 2017; Goh et al. 2018; Bjerrum and Sattarov 2018; Winter et al. 2019). However, these studies did not demonstrate perfor- mance in small data settings. In other words, the perfor- mance on small data'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Winter et al. 2019). However, these studies did not demonstrate perfor- mance in small data settings. In other words, the perfor- mance on small data settings of pre-training approaches in the cheminformatics ﬁeld has not been evaluated yet. In this study, by applying the latest pre-training method in the NLP ﬁeld to cheminformatics, we propose a new approach called'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Figure 1: The illustration of SMILES Transformer pre- training and ﬁngerprint extraction.\\n\\nSMILES Transformer (ST) that shows higher performance on small data settings than other approaches. ST is based on a Transformer (Vaswani et al. 2017) pre-trained in an un- supervised way that produce continuous, data-driven ﬁnger- prints of molecules given SMILES. These ﬁngerprints grasp the semantics of molecules and can be fed to arbitrary pre- dictive models for many downstream tasks.\\n\\nIn order to evaluate the QSPR performance on small data settings, we focused on data efﬁciency. However, because there are few works focusing on data efﬁciency, which met- ric should be used is elusive. The most related work may be done by (Wu et al. 2018), where model performance is evaluated against the size of the training set and data efﬁ- ciency is emphasized as well as the best score. In this study, we propose a novel scalar metric to evaluate data efﬁciency. Our proposed model is described in Figure 1.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='To sum up, our contributions include the following:\\n\\nWe propose a data-driven ﬁngerprinting model, SMILES Transformer, which works well with simple predictors and enables state-of-the-art data efﬁciency in 5 out of 10 datasets in MoleculeNet.\\n\\nWe pre-train Transformers with unlabeled SMILES to learn their representations and show the potential of text- based models compared to baseline models including graph convolutions.\\n\\nWe propose a scalar metric for data efﬁciency that mea- sures model performance under different sizes of training data. In the ﬁrst section, we will explain how ST is trained and the ﬁngerprints are extracted. In the second section, we deﬁne the metric for data efﬁciency. In the third section, we will compare the performance of ST ﬁngerprints against other methods using 10 different datasets from MoleculeNet and more deeply inspect the pre-trained ST including latent space visualization. Finally, we discuss possible future di- rections.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='2 Methods In this section, we introduce the SMILES Transformer archi- tecture, pre-training settings, and how to design ST ﬁnger- prints. We then propose a novel metric for data efﬁciency.\\n\\n2.1 SMILES Transformer Model Architecture Unlike RNNs, Transformers (Vaswani et al. 2017) do not have recurrent connections and are therefore more stable and faster to converge. Moreover, featurization they empirically show better performance on long sequences and complicated prob- they are chosen as the de lems than RNNs. Hence, facto standard models in NLP (Devlin et al. 2019; Yang et al. 2019).\\n\\nWe built an encoder-decoder network with 4 Transformer blocks for each with PyTorch (Paszke et al. 2017). Each Transformer block has 4-head attentions with 256 embed- ding dimensions and 2 linear layers.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Pre-training settings We pre-trained ST with 861,000 un- labeled SMILES randomly sampled from ChEMBL24, a dataset of bioactive and real molecules (Gaulton et al. 2016). The SMILES was split into symbols (e.g., ’c’, ’Br’, ’=’, ’(’, ’2’) and then the symbols were one-hot encoded to input to the network. To alleviate bias for the canonical representa- tion of SMILES, we randomly transformed them every time they were used by the SMILES enumerator (Bjerrum 2017). Following the original paper (Vaswani et al. 2017), we used the sum of token encoding and positional encoding to in- put to the network. The network was trained for 5 epochs to minimize the cross entropy between the input SMILES and the output probability by the Adam optimizer (Kingma and Ba 2015). After convergence, the network achieved a per- plexity of 1.0, meaning perfect decoding from the encoded representations.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Fingerprint extraction As the outputs of the Transform- ers are contextualized word-level representations, ST out- puts a sequence of symbol-level (atom-level) represen- tations. Therefore, we need to pool them to obtain the molecule-level representations (ﬁngerprints). We concate- nated the four vectors to get the ﬁngerprints: mean and max pooled output of the last layer, the ﬁrst output of the last and the penultimate layer. Now we have a 1024-dimensional ﬁn- gerprint for each molecule from ST. This ﬁngerprint is de- signed to have the same dimensionality with the baseline we use for, the extended-connectivity ﬁngerprint (ECFP) (Rogers and Hahn 2010).\\n\\n2.2 Data Efﬁciency Metric (DEM) Here we discuss how to measure the data efﬁciency of a pre- dictive model f in terms of the metric m. Intuitively, data efﬁciency can be measured by averaging the metric m of the model f trained with different sizes of the training data.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='More formally, let (X, Y ) denote the whole available dataset and (Xi, Yi) denote the test data sampled from (X, Y ) at the rate of 1 − i. Then, the training data and the model trained with them can be represented as (X \\\\ Xi, Y \\\\ Yi) and fi, respectively. The metric m should be chosen to\\n\\nTable 1: Summarized information of MoleculeNet (Wu et al. 2018). ”R” and ”C” in the type column indicates regression and classiﬁcation respectively.\\n\\nCategory\\n\\nPhysical chemistry\\n\\nBiophysics\\n\\nPhysiology\\n\\nDataset ESOL FreeSolv Lipophilicity MUV HIV BACE BBBP Tox21 SIDER ClinTox\\n\\nMetric Tasks Type Mols RMSE 1128 R RMSE 643 R RMSE 4200 R 93127 C PRC-AUC 41913 ROC-AUC C 1522 C 2053 C 8014 C 1427 C 1491 C\\n\\nDescription Aqueous solubility Hydration free energy Octanol/water distribution coefﬁcient (logD) 17 tasks from PubChem BioAssay Ability to inhibit HIV replication\\n\\n1 1 1 17 1 1 1 12 27 2\\n\\nROC-AUC Binding results for inhibitors of human BACE-1 ROC-AUC ROC-AUC ROC-AUC ROC-AUC'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='1 1 1 17 1 1 1 12 27 2\\n\\nROC-AUC Binding results for inhibitors of human BACE-1 ROC-AUC ROC-AUC ROC-AUC ROC-AUC\\n\\nBlood-brain barrier penetration Toxicity measurements Adverse drug reactions on 27 system organs Clinical trial toxicity and FDA approval status\\n\\nTable 2: Comparison of data efﬁciency metric (DEM) with the baseline models on 10 datasets from MoleculeNet (Wu et al. 2018). The up/down arrows show that the higher/lower score is better, respectively.\\n\\nDataset ST+MLP (Ours) ECFP+MLP RNNS2S+MLP GraphConv\\n\\nESOL ↓ 1.144 1.741 1.317 1.673\\n\\nFrSlv ↓ Lipo ↓ MUV ↑ HIV ↑ BACE ↑ BBBP ↑ Tox21 ↑ 2.246 3.043 2.987 3.476\\n\\n1.169 1.090 1.219 1.062\\n\\n0.009 0.036 0.010 0.004\\n\\n0.683 0.697 0.682 0.723\\n\\n0.719 0.769 0.717 0.744\\n\\n0.900 0.760 0.884 0.795\\n\\n0.706 0.616 0.702 0.687\\n\\nSider ↑ ClinTox ↑ 0.559 0.588 0.558 0.557\\n\\n0.963 0.515 0.904 0.936'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='0.719 0.769 0.717 0.744\\n\\n0.900 0.760 0.884 0.795\\n\\n0.706 0.616 0.702 0.687\\n\\nSider ↑ ClinTox ↑ 0.559 0.588 0.558 0.557\\n\\n0.963 0.515 0.904 0.936\\n\\nbe suitable for the tasks. That is, in classiﬁcation tasks m should be the area under the receiver operation character- istics (ROC-AUC) or the F1 score and in regression tasks m should be the R2 score or the root mean squared error (RMSE).\\n\\nNow the proposed Data Efﬁciency Metric (DEM) is for-\\n\\nmulated as:\\n\\n3.1 Performance on Downstream Tasks Datasets We evaluated the performance of our pre-trained SMILES Transformer on 10 datasets from MoleculeNet (Wu et al. 2018), a benchmark for molecular property prediction. These datasets were chosen because they do not use 3D in- formation and the sizes are not too large. The datasets are different from each other in their domains, task types, and sizes.\\n\\nMDE(f, m) =\\n\\n1 |I|\\n\\n(cid:88)\\n\\nm(fi, Xi, Yi)\\n\\ni∈I'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='MDE(f, m) =\\n\\n1 |I|\\n\\n(cid:88)\\n\\nm(fi, Xi, Yi)\\n\\ni∈I\\n\\nSince we used various datasets with a wide range of sizes in the experiment described below, the percentage of the training data i should be increased exponentially. There- fore, i is doubly increased from 1.25% to 80%, i.e., I = {0.0125, 0.025, ..., 0.4, 0.8}.\\n\\n3 Experiments\\n\\n(1)\\n\\nPhysical chemistry: ESOL, FreeSolv, and Lipophilicity • Biophysics: MUV, HIV, and BACE • Physiology: BBBP, Tox21, SIDER, and ClinTox The information about each dataset is summarized in Ta- ble 1. For the evaluation metrics, we used the root mean squared error (RMSE) for the regression tasks and the area under the receiver operating characteristic curve (ROC- AUC) or the area under the precision-recall curve (PRC- AUC) for the classiﬁcation tasks as suggested in (Wu et al. 2018).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='We conducted ﬁve experiments to see how SMILES Trans- former works from different perspectives. First, we evalu- ated the performance of ST against other baseline models on 10 chemical datasets. Second, we visualized the latent space to answer the question: why do ST ﬁngerprints work well for certain datasets? Third, we applied linear models to ST and other ﬁngerprints in order to validate that ST maps molecules to a good latent space by minimizing the contribu- tion of the models themselves. Fourth, we evaluated our ST and baseline models on a stratiﬁed dataset by the lengths of SMILES to see when ST provides an advantage. Finally, we compared the maximum performance of ST against state-of- the-art models under large data settings.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Baseline models We compared our pre-trained SMILES Transformer to the following three baseline models for molecular property prediction tasks: • ECFP4 (Rogers and Hahn 2010) is a hand-crafted ﬁnger- print. It hashes multi-scaled substructures to integers and makes a ﬁxed-length binary vector where 1 indicates the existence of the assigned substructure and 0 for the ab- sence. ECFP4 counts substructures with the diameters up to 4.\\n\\nRNNS2S (Xu et al. 2017) is another text-based pre- that adopts RNN Seq2seq for the trained ﬁngerprint model architecture.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='RNNS2S (Xu et al. 2017) is another text-based pre- that adopts RNN Seq2seq for the trained ﬁngerprint model architecture.\\n\\nGraphConv (Duvenaud et al. 2015) learns and predicts the target value directly through graph convolution oper- ations, rather than extracting ﬁngerprints and building an- other model for supervised downstream tasks. Although GraphConv is not a task-agnostic ﬁngerprint, we include it here as the state-of-the-art model. We used RDKit (Landrum 2016) to compute ECFP4 and DeepChem (Ramsundar et al. 2019) implementa- tion of GraphConv (with the default hyperparmeters). For RNNS2S, we implemented it with PyTorch (Paszke et al. 2017) and pre-trained it with the same dataset as ST. The encoder and the decoder are both 3-layer bidirectional gated recurrent units (GRUs) (Cho et al. 2014) with 256 hidden vector dimensions. We obtained the same dimension of ﬁn- gerprint as ST by concatenating two outputs from the last and the penultimate layer.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Experiment settings In the downstream tasks, we used simple models, such as multilayer perceptron (MLP) clas- siﬁers and regressors with the same default hyperparameters in scikit-learn (Pedregosa et al. 2011) in order to evaluate the performance of the three ﬁngerprints, themselves as much as possible. All of these ﬁngerprints have 1,024 dimensions. The datasets were randomly split (stratiﬁed for classiﬁca- tion) to train sets and test sets by the percentage i. Note that we did not use a scaffold split suggested in (Wu et al. 2018). We ran 20 trials for each split and report the mean score and standard deviation in Figure 2 and DEM in Table 2. The metrics were chosen as recommended in MoleculeNet.\\n\\nResults Table 2 shows DEM of the 4 models. ST achieves the best score in 5 out of 10 datasets, followed by ECFP and GraphConv.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Results Table 2 shows DEM of the 4 models. ST achieves the best score in 5 out of 10 datasets, followed by ECFP and GraphConv.\\n\\nSee Figure 2 for the performance change against the train size. In ESOL, FreeSolv, BBBP, and ClinTox, ST performs the best at almost all points by a signiﬁcant margin and es- pecially high scores when the train size is small compared to the other models. In Tox21, ST supports good prediction along RNNS2S, but is beaten by GraphConv as the train size increase. In Lipophilicity, MUV, BACE, and SIDER, ECFP or GraphConv can predict better than ST.\\n\\n3.2 Visualization of the Latent Space To inspect why our ST ﬁngerprints lead to good predic- tive performance, we visualized the latent space and decode some samples from it. For each dataset in MoleculeNet, we conducted the following procedure:\\n\\n1. Calculate the ST ﬁngerprint (1024-dimension) of each molecule.\\n\\n2. Reduce their dimensions to 2 with t-SNE (Maaten and Hinton 2008).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='1. Calculate the ST ﬁngerprint (1024-dimension) of each molecule.\\n\\n2. Reduce their dimensions to 2 with t-SNE (Maaten and Hinton 2008).\\n\\n3. Plot the reduced features into a 2-dimensional space col- oring by the target value.\\n\\n4. Choose a trajectory in the 2-dimensional space and divide it into 12 points.\\n\\n5. Find the nearest neighbors of the 12 points and draw the corresponding molecules.\\n\\nWe show the result of the three datasets where ST ﬁn- gerprints work especially well in Figure 3, that is, FreeSolv, BBBP, and ClinTox. In FreeSolv, it can be seen that there is a clear gradation from upper left to lower right, and the molecule becomes simpler (i.e., less loops and branches) along the trajectory. In BBBP and ClinTox, the categorical target values are successfully separated, but there is no clear trends in the decoded molecules.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='3.3 Application of Simple Predictive Models In Section 3.1, we used MLP for the predictive model to ST, RNNS2S, and ECFP, expecting that combining it with these ﬁngerprints would work comparably or better than Graph- Conv. Here we used the simplest models to measure the pure effect of the ﬁngerprints. To be speciﬁc, adding L2 regular- ization to avoid overﬁtting, we used ridge regression for re- gression tasks and logistic regression with L2 penalty for classiﬁcation tasks. We excluded MUV and SIDER from this experiment because their highly imbalanced columns caused errors in the solver and ROC-AUC functions imple- mented in scikit-learn (Pedregosa et al. 2011). We followed the same procedure as in Section 3.1 except for the model selection and datasets and the results are shown in Table 3. Our ST ﬁngerprints with linear models achieved the best scores in 5 out of 8 datasets, indicating that the ST ﬁnger- print is a strong ﬁngerprint that leads to the best performance regardless of'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='the best scores in 5 out of 8 datasets, indicating that the ST ﬁnger- print is a strong ﬁngerprint that leads to the best performance regardless of model selection.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='3.4 Stratiﬁed Scores by the Size of Molecules We conducted another study to inspect when ST has an advantage against other models. We stratiﬁed the BBBP dataset by the lengths of SMILES (similar to the sizes of the molecules) into 5 groups and evaluated within each group. The scores and the distributions of the lengths of SMILES are shown in Figure 4.\\n\\nFigure 4 indicates that the ROC-AUC score of ST in- creases along the length of SMILES, which is a similar trend to the other text-based ﬁngerprint, RNNS2S. On the other hand, GraphConv shows more or less the same performance regardless of the SMILES lengths. These results suggest that longer SMILES give ST richer information for better dis- crimination.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='3.5 Comparison with Record Scores Finally, we compared the maximum performance of ST un- der the large data setting with the reported scores in Molecu- leNet. Since the ST ﬁngerprint is proven to be better than the RNNS2S ﬁngerprint, we omitted it and instead added an- other graph-based model named Weave (Kearnes et al. 2016) to the baselines. In this experiment, the datasets were split into train, validation, test sets with the proportion of 80%, 10%, 10%. The validation sets were used for hyperparam- eter tuning and the test sets were only used for calculating the scores. To fairly compare with the reported scores, the datasets HIV, BACE, BBBP used a scaffold split and the oth- ers were split randomly. We choose the model and hyperpa- rameter set achieving the best validation score with optuna (Akiba et al. 2019), from a linear model with L2 penalty,'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Figure 2: Comparison of model performance against different train size on the 10 datasets. The top row indicates the results for the physical chemistry datasets, the second row indicates biophysics, and the two bottom rows indicate physiology, respectively. The scores were averaged over 20 trials and the error bars are the standard deviations\\n\\nFigure 3: Visualization of the latent space of SMILES Transformer. For three datasets, FreeSolv, BBBP, and ClinTox, the dimensions of ST ﬁngerprints of the molecules are reduced to 2 with t-SNE (Maaten and Hinton 2008). Then, the nearest neighbors of the 12 data points on a trajectories are plotted on the latent space (left panel). The 12 points are decoded to molecules and shown in the right panel. The color bar of the top left panel indicates the standardized free energy.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Table 3: Comparison of data efﬁciency metric (DEM) with the baseline models on the 8 datasets from MoleculeNet (Wu et al. 2018). The predictive models are ridge regression and logistic regression with L2 penalty. The up/down arrows show that the higher/lower score is better, respectively.\\n\\nDataset ST (Ours) ECFP RNNS2S\\n\\nESOL ↓ 1.140 1.678 1.288\\n\\nFrSlv ↓ Lipo ↓ HIV ↑ BACE ↑ BBBP ↑ Tox21 ↑ ClinTox ↑ 2.452 2.843 2.881\\n\\n1.213 1.174 1.194\\n\\n0.696 0.727 0.688\\n\\n0.720 0.790 0.727\\n\\n0.895 0.825 0.884\\n\\n0.711 0.710 0.709\\n\\n0.958 0.704 0.915\\n\\nTable 4: Comparison of the best achieved scores with the record scores on the 8 datasets from MoleculeNet (Wu et al. 2018). The scores of ECFP, GraphConv, and Weave are the reported scores in MoleculeNet. The up/down arrows show that the higher/lower score is the better, respectively.\\n\\nDataset Splitting ST (Ours) ECFP GraphConv Weave\\n\\nESOL ↓ HIV ↑ random random random scaffold 0.729 0.792 0.763 0.703\\n\\nFrSlv ↓\\n\\nLipo ↓\\n\\n0.72 0.99 0.97 0.61\\n\\n1.65 1.74 1.40 1.22'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='ESOL ↓ HIV ↑ random random random scaffold 0.729 0.792 0.763 0.703\\n\\nFrSlv ↓\\n\\nLipo ↓\\n\\n0.72 0.99 0.97 0.61\\n\\n1.65 1.74 1.40 1.22\\n\\n0.921 0.799 0.655 0.715\\n\\nBACE ↑ BBBP ↑ Tox21 ↑ ClinTox ↑ random scaffold 0.954 0.701 0.867 0.799 0.807 0.783 0.832 0.806\\n\\nscaffold 0.704 0.729 0.690 0.671\\n\\nrandom 0.802 0.822 0.829 0.820\\n\\nMLP, and LightGBM (Ke et al. 2017). We conducted three independent runs and reported the average scores in Table 4 ST achieves ﬁrst place only in ClinTox, but performs comparable to ECFP and graph-based models in the other datasets. We can conclude that our ST ﬁngerprints, if care- fully tuned, are still useful even when the large number of labels are available.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='4 Conclusions In this paper, we propose SMILES-Transformer, a data- driven molecular ﬁngerprint produced by a Transformer- based seq2seq pre-trained with a huge set of unlabeled SMILES. ST ﬁngerprints were shown to work well with any predictive model in MoleculeNet downstream tasks and is effective especially when there is not enough labeled data. When large labeled data are available, ST ﬁngerprints work comparable to other state-of-the-art baselines such as Graph- Conv. We also propose DEM, a novel metric for data efﬁ- ciency. In terms of DEM, the ST ﬁngerprint is better than existing methods in 5 out of 10 downstream tasks.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Future work can continue in three directions. First, re- placing the Transformer in ST with Transformer-XL, an extended model that can handle much longer sequences, will alleviate the length limit of ST. Second, ST will be even stronger when trained in a multi-task fashion as done in ChemNet (Goh et al. 2018): predicting automatically- calculated molecular descriptors (e.g., molecular weight, LogP) as well as decoding the input SMILES. This will help the model to learn more chemistry-relevant representa- tions. Finally, making use of the information of enumerated SMILES is one of the keys to improving text-based molecu- lar representations. As done in (Bjerrum and Sattarov 2018), a set of different SMILES of the same molecule can be used to restrict the latent space.\\n\\nFigure 4: ROC-AUC scores on each stratiﬁed group by the lengths of SMILES (left) and the distributions of the lengths of SMILES (right) of BBBP dataset.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Our implementation for SMILES-Transformer is avail- able at https://github.com/DSPsleeporg/smiles-transformer'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='References Akiba, T.; Sano, S.; Yanase, T.; Ohta, T.; and Koyama, M. 2019. Optuna: A next-generation hyperparameter optimiza- tion framework. In ACM SIGKDD, 2623–2631. ACM. Altae-Tran, H.; Ramsundar, B.; Pappu, A. S.; and Pande, V. 2017. Low data drug discovery with one-shot learning. ACS Cent. Sci. 3(4):283–293. Bjerrum, E., and Sattarov, B. 2018. Improving chemical autoencoder latent space and molecular de novo generation diversity with heteroencoders. Biomolecules 8(4):131. Bjerrum, E. J. 2017. Smiles enumeration as data augmen- tation for neural network modeling of molecules. arXiv preprint arXiv:1703.07076. Cho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning phrase representations using rnn encoder–decoder for statis- tical machine translation. In EMNLP, 1724–1734. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT,'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 4171–4186. Duvenaud, D. K.; Maclaurin, D.; Iparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-Guzik, A.; and Adams, R. P. 2015. Convolutional networks on graphs for learning molecular ﬁngerprints. In NeurIPS, 2224–2232. Gaulton, A.; Hersey, A.; Nowotka, M.; Bento, A. P.; Cham- bers, J.; Mendez, D.; Mutowo, P.; Atkinson, F.; Bellis, L. J.; Cibri´an-Uhalte, E.; et al. 2016. The chembl database in 2017. Nucleic acids research 45(D1):D945–D954. Goh, G. B.; Siegel, C.; Vishnu, A.; and Hodas, N. 2018. Us- ing rule-based labels for weak supervised learning: a chem- net for transferable chemical property prediction. In ACM SIGKDD, 302–310. ACM. G´omez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hern´andez-Lobato, J. M.; S´anchez-Lengeling, B.; She- berla, D.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.; and Aspuru-Guzik, A. 2018.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Hern´andez-Lobato, J. M.; S´anchez-Lengeling, B.; She- berla, D.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.; and Aspuru-Guzik, A. 2018. Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent. Sci. 4(2):268–276. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In CVPR, 770–778. Jin, W.; Barzilay, R.; and Jaakkola, T. 2018. Junction tree variational autoencoder for molecular graph generation. In ICML, 2328–2337. Ke, G.; Meng, Q.; Finley, T.; Wang, T.; Chen, W.; Ma, W.; Ye, Q.; and Liu, T.-Y. 2017. Lightgbm: A highly efﬁcient gradient boosting decision tree. In NeurIPS, 3146–3154. Kearnes, S.; McCloskey, K.; Berndl, M.; Pande, V.; and Ri- ley, P. 2016. Molecular graph convolutions: moving beyond ﬁngerprints. J. Comput. Aided Mol. Des. 30(8):595–608. Kingma, D. P., and Ba, J. 2015. Adam: A method for stochastic optimization. In ICLR. Kingma, D. P., and Welling, M. 2014. Auto-encoding vari-'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Kingma, D. P., and Ba, J. 2015. Adam: A method for stochastic optimization. In ICLR. Kingma, D. P., and Welling, M. 2014. Auto-encoding vari- ational Bayes. In ICLR. Kusner, M. J.; Paige, B.; and Hern´andez-Lobato, J. M. 2017.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='In ICML, 1945–1954.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Grammar variational autoencoder. JMLR. org. Landrum, G. 2016. Rdkit: Open-source cheminformatics. Maaten, L. v. d., and Hinton, G. 2008. Visualizing data using t-sne. J. Mach. Learn. Res. 9(Nov):2579–2605. ¨Ozt¨urk, H.; ¨Ozg¨ur, A.; and Ozkirimli, E. 2018. Deepdta: deep drug–target binding afﬁnity prediction. Bioinformatics 34(17):i821–i829. Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.; DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017. Automatic differentiation in pytorch. In NIPS. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; Dubourg, V.; et al. 2011. Scikit-learn: Machine learning in python. J. Mach. Learn. Res. 12(Oct):2825–2830. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. In NAACL-HLT, 2227–2237. Ramsundar, B.; Eastman, P.; Walters, P.; Pande, V.; Leswing, K.; and Wu, Z. 2019.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='Deep contextualized word representations. In NAACL-HLT, 2227–2237. Ramsundar, B.; Eastman, P.; Walters, P.; Pande, V.; Leswing, K.; and Wu, Z. 2019. Deep Learning for the Life Sciences. O’Reilly Media. https://www.amazon.com/Deep-Learning- Life-Sciences-Microscopy/dp/1492039837. Rogers, D., and Hahn, M. 2010. Extended-connectivity ﬁn- gerprints. J. Chem. Inf. Model. 50(5):742–754. Schwaller, P.; Laino, T.; Gaudin, T.; Bolgar, P.; Bekas, C.; and Lee, A. A. 2018. Molecular transformer for chemical re- action prediction and uncertainty estimation. arXiv preprint arXiv:1811.02633. Segler, M. H.; Preuss, M.; and Waller, M. P. 2018. Planning chemical syntheses with deep neural networks and symbolic ai. Nature 555(7698):604. Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.; Huang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton, A.; et al. 2017. Mastering the game of go without human knowledge. Nature 550(7676):354. Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='A.; et al. 2017. Mastering the game of go without human knowledge. Nature 550(7676):354. Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. In NeurIPS, 3104– 3112. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At- tention is all you need. In NeurIPS, 5998–6008. Winter, R.; Montanari, F.; No´e, F.; and Clevert, D.-A. 2019. Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations. Chem. Sci. 10(6):1692–1701. Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Ge- niesse, C.; Pappu, A. S.; Leswing, K.; and Pande, V. 2018. Moleculenet: a benchmark for molecular machine learning. Chem. Sci. 9(2):513–530. Xu, Z.; Wang, S.; Zhu, F.; and Huang, J. 2017. Seq2seq ﬁngerprint: An unsupervised deep molecular embedding for drug discovery. In ACM BCB, 285–294. ACM. Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; and'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='deep molecular embedding for drug discovery. In ACM BCB, 285–294. ACM. Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; and Le, Q. V. 2019. Xlnet: Generalized autoregres-'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/1911.04738.pdf'}, page_content='sive pretraining for language understanding. arXiv preprint arXiv:1906.08237.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='0 2 0 2\\n\\nt c O 3 2\\n\\n]\\n\\nG L . s c [\\n\\n2 v 5 8 8 9 0 . 0 1 0 2 : v i X r a\\n\\nChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction\\n\\nSeyone Chithrananda University of Toronto seyone.chithrananda@utoronto.ca\\n\\nGabriel Grand Reverie Labs gabe@reverielabs.com\\n\\nBharath Ramsundar DeepChem bharath.ramsundar@gmail.com\\n\\nAbstract'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='GNNs and chemical ﬁngerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the ﬁrst attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. While not at state-of-the-art, ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transform- ers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='1 Motivation\\n\\nMolecular property prediction has seen a recent resurgence thanks to the success of graph neural networks (GNNs) on various benchmark tasks [1, 2, 3, 4, 5, 6]. However, data scarcity remains a fundamental challenge for supervised learning in a domain in which each new labelled data point requires costly and time-consuming laboratory testing. Determining effective methods to make use of large amounts of unlabeled structure data remains an important unsolved challenge.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='Over the past two years, the transformer [7, 8] has emerged as a robust architecture for learning self-supervised representations of text. Transformer pretraining plus task-speciﬁc ﬁnetuning provides substantial gains over previous approaches to many tasks in natural language processing (NLP) [9, 10, 11]. Meanwhile, software infrastructure for transformers is maturing rapidly: HuggingFace [12] provides streamlined pretraining and ﬁnetuning pipelines, while packages like BertViz [13] offer sophisticated interfaces for attention visualization. Given the availability of millions of SMILES strings, transformers offer an interesting alternative to both expert-crafted and GNN-learned ﬁnger- prints. In particular, the masked language-modeling (MLM) pretraining task [8] commonly used for BERT-style architectures is analogous to atom masking tasks used in graph settings [14]. Moreover, since modern transformers are engineered to scale to massive NLP corpora, they offer practical advantages'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='used in graph settings [14]. Moreover, since modern transformers are engineered to scale to massive NLP corpora, they offer practical advantages over GNNs in terms of efﬁciency and throughput.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='Though simple in concept, the application of transformers to molecular data presents several questions that are severely underexplored. For instance: How does pretraining dataset size affect downstream task performance? What tokenization strategies work best for SMILES? Does replacing SMILES\\n\\nPreprint. Submitted to 34th Conference on Neural Information Processing Systems (NeurIPS 2020).\\n\\nwith a more robust string representation like SELFIES [15] improve performance? We aim to address these questions via one of the ﬁrst systematic evaluations of transformers on molecular property prediction tasks.\\n\\n2 Related Work'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='2 Related Work\\n\\nIn cheminformatics, there is a long tradition of training language models directly on SMILES to learn continuous latent representations [16, 17, 18]. Typically, these are RNN sequence-to-sequence models and their goal is to facilitate auxiliary lead optimization tasks; e.g., focused library generation [19]. Thus far, discussion of the transformer architecture in chemistry has been largely focused on a particular application to reaction prediction [20].'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='Some recent work has pretrained transformers for molecular property prediction and reported promis- ing results [21, 22]. However, the datasets used for pretraining have been relatively small (861K compounds from ChEMBL and 2M compounds from ZINC, respectively). Other work has used larger pretraining datasets (18.7M compounds from ZINC) [23] but the effects of pretraining dataset size, tokenizer, and string representation were not explored. In still other work, transformers were used for supervised learning directly without pretraining [24].\\n\\nRecently, a systematic study of self-supervised pretraining strategies for GNNs helped to clarify the landscape of those methods [14]. Our goal is to undertake a similar investigation for transformers to assess the viability of this architecture for property prediction.\\n\\n3 Methods'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='3 Methods\\n\\nChemBERTa is based on the RoBERTa [25] transformer implementation in HuggingFace [12]. Our implementation of RoBERTa uses 12 attention heads and 6 layers, resulting in 72 distinct attention mechanisms. So far, we have released 15 pre-trained ChemBERTa models on the Huggingface’s model hub; these models have collectively received over 30,000 Inference API calls to date.1\\n\\nWe used the popular Chemprop library for all baselines [6]. We trained the directed Message Passing Neural Network (D-MPNN) with default hyperparameters as well as the sklearn-based [26] Random Forest (RF) and Support Vector Machine (SVM) models from Chemprop, which use 2048-bit Morgan ﬁngerprints from RDKit [27, 28].\\n\\n3.1 PreTraining on PubChem 77M'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='3.1 PreTraining on PubChem 77M\\n\\nWe adopted our pretraining procedure from RoBERTa, which masks 15% of the tokens in each input string. We used a max. vocab size of 52K tokens and max. sequence length of 512 tokens. We trained for 10 epochs on all PubChem subsets except for the 10M subset, on which we trained for 3 epochs to avoid observed overﬁtting. Our hypothesis is that, in learning to recover masked tokens, the model forms a representational topology of chemical space that should generalize to property prediction tasks.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='For pretraining, we curated a dataset of 77M unique SMILES from PubChem [29], the world’s largest open-source collection of chemical structures. The SMILES were canonicalized and globally shufﬂed to facilitate large-scale pretraining. We divided this dataset into subsets of 100K, 250K, 1M, and 10M. Pretraining on the largest subset took approx. 48 hours on a single NVIDIA V100 GPU. We make this dataset publicly available and leave pretraining on the full 77M set to future work.\\n\\n3.2 Finetuning on MoleculeNet'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='3.2 Finetuning on MoleculeNet\\n\\nWe evaluated our models on several classiﬁcation tasks from MoleculeNet [30] selected to cover a range of dataset sizes (1.5K - 41.1K examples) and medicinal chemistry applications (brain penetrability, toxicity, and on-target inhibition). These included the BBBP, ClinTox, HIV, and Tox21 datasets. For datasets with multiple tasks, we selected a single representative task: the clinical toxicity (CT_TOX) task from ClinTox and the p53 stress-response pathway activation (SR-p53) task from\\n\\n1The main model directory can be viewed here. Each model includes the speciﬁc tokenizer (BPE, SMILES-\\n\\ntokenized), representation (SMILES, SELFIES) and number of training steps (’150k’) appended in its name.\\n\\n2\\n\\nBBBP 2,039\\n\\nClinTox (CT_TOX) 1,478\\n\\nHIV 41,127\\n\\nTox21 (SR-p53) 7,831\\n\\nROC\\n\\nPRC\\n\\nROC\\n\\nPRC\\n\\nROC\\n\\nPRC\\n\\nROC\\n\\nPRC\\n\\nChemBERTa 10M 0.643 0.708 D-MPNN 0.681 RF 0.702 SVM\\n\\n0.620 0.697 0.692 0.724\\n\\n0.733 0.906 0.693 0.833\\n\\n0.975 0.993 0.968 0.986\\n\\n0.622 0.752 0.780 0.763'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='0.620 0.697 0.692 0.724\\n\\n0.733 0.906 0.693 0.833\\n\\n0.975 0.993 0.968 0.986\\n\\n0.622 0.752 0.780 0.763\\n\\n0.119 0.152 0.383 0.364\\n\\n0.728 0.688 0.724 0.708\\n\\n0.207 0.429 0.335 0.345\\n\\nTable 1: Comparison of ChemBERTa pretrained on 10M PubChem compounds and Chemprop baselines on selected MoleculeNet tasks. We report both ROC-AUC and PRC-AUC to give a full picture of performance on class-imbalanced tasks.\\n\\nTox21. For each dataset, we generated an 80/10/10 train/valid/test split using the scaffold splitter from DeepChem [31]. During ﬁnetuning, we appended a linear classiﬁcation layer and backpropagated through the base model. We ﬁnetuned models for up to 25 epochs with early stopping on ROC- AUC. We release a tutorial in DeepChem which allows users to go through loading a pre-trained ChemBERTa model, running masked prediction tasks, visualizing the attention of the model on several molecules, and ﬁne-tuning the model on the Tox21 SR-p53 dataset.\\n\\n4 Results'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='4 Results\\n\\nOn the MoleculeNet tasks that we evaluated, ChemBERTa approaches, but does not beat, the strong baselines from Chemprop (Table 1).2 Nevertheless, downstream performance of ChemBERTa scales well with more pretraining data (Fig. 1). On average, scaling from 100K to 10M resulted in ∆ROC-AUC = +0.110 and ∆PRC-AUC = +0.059. (HIV was omitted from this analysis due to resource constraints.) These results suggest that ChemBERTa learns more robust representations with additional data and is able to leverage this information when learning downstream tasks.\\n\\nFigure 1: Scaling the pretraining size (100K, 250K, 1M, 10M) produces consistent improvements in downstream task performance on BBBP, ClinTox, and Tox21. Mean ∆AUC across all three tasks with a 68% conﬁdence interval is shown in light blue.\\n\\n2While Tox21 ROC-AUC is better than the baselines, PR-AUC is considerably lower.\\n\\n3\\n\\n(a)\\n\\n(b)\\n\\n(c)'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='2While Tox21 ROC-AUC is better than the baselines, PR-AUC is considerably lower.\\n\\n3\\n\\n(a)\\n\\n(b)\\n\\n(c)\\n\\nFigure 2: (a) Attention in GNNs highlights a problematic ketone in a Tox21 compound. (b) Attention over SMILES tokens in ChemBERTa provides a close analogue to graph attention. (c) Neural stack trace enables ﬁne-grained introspection of neuron behavior. (b - c) produced via BertViz [13].\\n\\n4.1 Tokenizers'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='4.1 Tokenizers\\n\\nOur default tokenization strategy uses a Byte-Pair Encoder (BPE) from the HuggingFace tokenizers library [12]. BPE is a hybrid between character and word-level representations, which allows for the handling of large vocabularies in natural language corpora. Motivated by the intuition that rare and unknown words can often be decomposed into multiple known subwords, BPE ﬁnds the best word segmentation by iteratively and greedily merging frequent pairs of characters [32]. We compare this tokenization algorithm with a custom SmilesTokenizer based on a regex from [20], which we have released as part of DeepChem [31].3'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='To compare tokenizers, we pretrained two identical models on the PubChem-1M set. The pretrained models were evaluated on the Tox21 SR-p53 task. We found that the SmilesTokenizer narrowly outperformed BPE by ∆PRC-AUC = +0.015. Though this result suggests that a more semantically- relevant tokenization may provide performance beneﬁts, further benchmarking on additional datasets is needed to validate this ﬁnding.\\n\\n4.2 SMILES vs. SELFIES\\n\\nIn addition to SMILES, we pretrained ChemBERTA on SELFIES (SELF-referencing Embedded Strings) [15]. SELFIES is an alternate molecular string representation designed for machine learning. Because every valid SELFIES corresponds to a valid molecule, we hypothesized that SELFIES would lead to a more robust model. However, we found no signiﬁcant difference in downstream performance on the Tox21 SR-p53 task. Further benchmarking is needed to validate this ﬁnding.\\n\\n4.3 Attention Visualization'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='4.3 Attention Visualization\\n\\nWe used BertViz [13] to inspect the attention heads of ChemBERTa (SmilesTokenizer version) on Tox21, and contrast them to the molecular graph visualization of an attention-based GNN. We found certain neurons that were selective for chemically-relevant functional groups, and aromatic rings. We also observed other neurons that tracked bracket closures – a ﬁnding in keeping with results on attention-based RNNs showing the ability to track nested parentheses [33, 34].\\n\\n5 Discussion\\n\\nIn this work, we introduce ChemBERTa, a transformer architecture for molecular property prediction. Initial results show that MLM pretraining provides a boost in predictive power for models on selected downstream tasks from MoleculeNet. However, with the possible exception of Tox21, ChemBERTa still performs below state-of-the-art on these tasks.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='Our current analysis covers only a small portion of the hypothesis space we hope to explore. We plan to expand our evaluations to all of MoleculeNet, undertake more systematic hyperparameter\\n\\n3https://deepchem.readthedocs.io/en/latest/tokenizers.html#smilestokenizer\\n\\n4\\n\\ntuning, experiment with larger masking rates, and explore multitask ﬁnetuning. In parallel, we aim to scale up pretraining, ﬁrst to the full PubChem 77M dataset, then to even larger sets like ZINC-15 (with 270 million compounds). This work will require us to improve our engineering infrastructure considerably.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='As we scale up, we are also actively investigating methods to improve sample efﬁciency. Alternative text-based pretraining methods like ELECTRA may be useful [10]. Separately, there is little question that graph representations provide useful inductive biases for learning molecular structures. Recent hybrid graph transformer models [22, 35] may provide better sample efﬁciency while retaining the scalability of attention-based architectures.\\n\\nBroader Impact'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='A core goal of AI for drug discovery is to accelerate the development of new and potentially life-saving medicines. Research to improve the accuracy and generalizability of molecular property prediction methods contributes directly to these aims. Nevertheless, machine learning—and particularly large- scale pretraining of the form we undertake here—is a resource-intensive process that has a growing carbon footprint [36]. According to the Machine Learning Emissions Calculator (https://mlco2. github.io/impact), we estimate that our pretraining generated roughly 17.1 kg CO2eq (carbon- dioxide equivalent) of emissions. Fortunately, Google Cloud Platform, which we used for this work, is certiﬁed carbon-neutral and offsets 100% of emissions (https://cloud.google.com/ sustainability). Even as we advocate for further exploration of large-scale pretraining for property prediction, we also encourage other researchers to be mindful of the environmental impact of these efforts and opt for'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='pretraining for property prediction, we also encourage other researchers to be mindful of the environmental impact of these efforts and opt for sustainable cloud compute solutions where possible.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='Acknowledgments and Disclosure of Funding\\n\\nWe would like to thank the Tyler Cowen and the Emergent Ventures fellowship for providing the research grant to S.C. for cloud computing and various research expenses, alongside the Thiel Foundation for funding the grant. Thanks to Mario Krenn, Alston Lo, Akshat Nigam, Professor Alan Aspuru-Guzik and the entire Aspuru-Guzik group for early discussions and mentorship regarding the potiential for applying large-scale transformers on molecular strings, as well as in motivating the utilization of SELFIES in this work.\\n\\nWe would also like to thank the entire DeepChem team for their support and early discussions on fostering the ChemBERTa concept, and helping with designing and hosting the Tokenizers API and ChemBERTa tutorial. Thanks to the Reverie team for authorizing our usage of the PubChem 77M dataset, which was processed, ﬁltered and split by them.\\n\\nReferences'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='References\\n\\n[1] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In Advances in neural information processing systems, pages 2224– 2232, 2015.\\n\\n[2] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond ﬁngerprints. Journal of computer-aided molecular design, 30(8):595–608, 2016.\\n\\n[3] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional\\n\\nnetworks. arXiv preprint arXiv:1609.02907, 2016.\\n\\n[4] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\\n\\nmessage passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\\n\\n[5] Connor W Coley, Regina Barzilay, William H Green, Tommi S Jaakkola, and Klavs F Jensen. Convolutional embedding of attributed molecular graphs for physical property prediction. Journal of chemical information and modeling, 57(8):1757–1772, 2017.\\n\\n[6] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molec-\\n\\n5\\n\\nular representations for property prediction. Journal of chemical information and modeling, 59(8):3370–3388, 2019.\\n\\n[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\n\\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017.\\n\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\\n\\n[9] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\\n\\n[10] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='[11] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\\n\\n[12] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transform- ers: State-of-the-art natural language processing. ArXiv, pages arXiv–1910, 2019.\\n\\n[13] Jesse Vig. A multiscale visualization of attention in the transformer model. CoRR,\\n\\nabs/1906.05714, 2019.\\n\\n[14] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='[15] Mario Krenn, Florian Hase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (selﬁes): A 100% robust molecular string representation. Machine Learning: Science and Technology, 2020.\\n\\n[16] Zheng Xu, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Seq2seq ﬁngerprint: An unsupervised deep molecular embedding for drug discovery. In Proceedings of the 8th ACM international conference on bioinformatics, computational biology, and health informatics, pages 285–294, 2017.\\n\\n[17] Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational\\n\\nautoencoder. arXiv preprint arXiv:1703.01925, 2017.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='[17] Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational\\n\\nautoencoder. arXiv preprint arXiv:1703.01925, 2017.\\n\\n[18] Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268–276, 2018.\\n\\n[19] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120–131, 2018.\\n\\n[20] Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572–1583, 2019.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='[21] Shion Honda, Shoi Shi, and Hiroki R Ueda. Smiles transformer: Pre-trained molecular\\n\\nﬁngerprint for low data drug discovery. arXiv preprint arXiv:1911.04738, 2019.\\n\\n[22] Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław\\n\\nJastrz˛ebski. Molecule attention transformer. arXiv preprint arXiv:2002.08264, 2020.\\n\\n[23] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, pages 429–436, 2019.\\n\\n6\\n\\n[24] Benson Chen, Regina Barzilay, and Tommi Jaakkola. Path-augmented graph transformer\\n\\nnetwork. arXiv preprint arXiv:1905.12712, 2019.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='6\\n\\n[24] Benson Chen, Regina Barzilay, and Tommi Jaakkola. Path-augmented graph transformer\\n\\nnetwork. arXiv preprint arXiv:1905.12712, 2019.\\n\\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.\\n\\n[26] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit- learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011.\\n\\n[27] David Rogers and Mathew Hahn. Extended-connectivity ﬁngerprints. Journal of chemical\\n\\ninformation and modeling, 50(5):742–754, 2010.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='[27] David Rogers and Mathew Hahn. Extended-connectivity ﬁngerprints. Journal of chemical\\n\\ninformation and modeling, 50(5):742–754, 2010.\\n\\n[28] Greg Landrum et al. Rdkit: Open-source cheminformatics. 2006. [29] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access to chemical data. Nucleic acids research, 47(D1):D1102–D1109, 2019.\\n\\n[30] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513–530, 2018.\\n\\n[31] B Ramsundar, P Eastman, E Feinberg, J Gomes, K Leswing, A Pappu, M Wu, and V Pande. Deepchem: Democratizing deep-learning for drug discovery, quantum chemistry, materials science and biology, 2016.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='[32] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: A text compression scheme that accelerates pattern matching. Technical report, Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999.\\n\\n[33] Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart M Shieber. Lstm networks\\n\\ncan perform dynamic counting. arXiv preprint arXiv:1906.03648, 2019.\\n\\n[34] Xiang Yu, Ngoc Thang Vu, and Jonas Kuhn. Learning the dyck language with attention-based seq2seq models. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 138–146, 2019.\\n\\n[35] Anonymous. Modelling drug-target binding afﬁnity using a bert based graph neural network.\\n\\nSubmitted to International Conference on Learning Representations, 2021.\\n\\n[36] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2010.09885.pdf'}, page_content='[36] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying\\n\\nthe carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.\\n\\n7'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Open Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nChemical Science\\n\\nEDGE ARTICLE\\n\\nView Article Online\\n\\nView Journal\\n\\nFine-tuning GPT-3 for machine learning electronic and functional properties of organic molecules\\n\\nCite this: DOI: 10.1039/d3sc04610a\\n\\nAll publication charges for this article have been paid for by the Royal Society of Chemistry\\n\\nZikai Xie,a Xenophon Evangelopoulos,a ¨Omer H. Omar, Andrew I. Cooper\\n\\na and Linjiang Chen\\n\\nc\\n\\nb Alessandro Troisi,\\n\\nb\\n\\nReceived 31st August 2023 Accepted 4th December 2023\\n\\nDOI: 10.1039/d3sc04610a'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"a and Linjiang Chen\\n\\nc\\n\\nb Alessandro Troisi,\\n\\nb\\n\\nReceived 31st August 2023 Accepted 4th December 2023\\n\\nDOI: 10.1039/d3sc04610a\\n\\nWe evaluate the eﬀectiveness of ﬁne-tuning GPT-3 for the prediction of electronic and functional properties of organic molecules. Our ﬁndings show that ﬁne-tuned GPT-3 can successfully identify and distinguish between chemically meaningful patterns, and discern subtle diﬀerences among them, exhibiting robust predictive performance for the prediction of molecular properties. We focus on assessing the ﬁne-tuned models' resilience to information loss, resulting from the absence of atoms or chemical groups, and to noise that we introduce via random alterations in atomic identities. We discuss the challenges and limitations inherent to the use of GPT-3 in molecular machine-learning tasks and\\n\\nrsc.li/chemical-science\\n\\nsuggest potential directions for future research and improvements to address these issues.\\n\\nIntroduction\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='rsc.li/chemical-science\\n\\nsuggest potential directions for future research and improvements to address these issues.\\n\\nIntroduction\\n\\nThere has been recent and growing interest in leveraging for diverse applications involving machine learning (ML) organic molecules, such as predicting molecular properties1–6 and using inverse design techniques to create new functional molecules.7–10 Such ML-oriented tasks have facilitated a deeper comprehension of structure–property relationships, led to the discovery of new chemical reactivity, and catalyzed the devel- opment of novel functional molecules and materials, including drugs and catalysts.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='The advent of the latest large language models (LLMs), notably GPT-3 and GPT-4,11,12 has quickly attracted the attention and interest of chemists. Indeed, despite inherent limitations and valid concerns about the way that LLMs operate, GPT models have emerged as tools that oﬀer the potential to trans- form the way chemists approach their research. LLMs, which are trained on vast amounts of text data, can generate human- like text, answer questions, and even perform tasks that require understanding and reasoning. Used with caution, almost any aspect of chemistry research might bene\\ue103t from such capabilities, while others may require additional enhancements to the LLMs, such as \\ue103ne-tuning and the use of plugins.\\n\\nOne of the most signi\\ue103cant impacts that LLMs may have on chemistry is their potential ability to accelerate research and discovery by interacting with human chemists. For example,'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='aLeverhulme Research Centre for Functional Materials Design, Materials Innovation Factory and Department of Chemistry, University of Liverpool, Liverpool L7 3NY, UK. E-mail: aicooper@liverpool.ac.uk\\n\\nbDepartment of Chemistry, University of Liverpool, Liverpool L69 3BX, UK\\n\\nGPT-4 has been integrated into an iterative process of discov- ering new metal–organic frameworks (MOFs), operating through a cooperative work\\ue104ow between GPT-4 and a human chemist.13 Through structured prompting of GPT-4 and in-text learning informed by human feedback, the human-arti\\ue103cial intelligence (AI) collaboration yielded the discovery of an iso- reticular series of MOFs, each synthesized using distinct strat- egies and optimal conditions.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Sophisticated, LLM-powered AI chemistry agents have been reported to accomplish tasks across organic synthesis, drug discovery, and materials design. One such example is Chem- Crow,14 a GPT-4-powered chemistry engine designed to streamline the reasoning process for various common chemical tasks, including drug and materials design and synthesis. ChemCrow combines chain-of-thought reasoning with expert- designed tools for chemistry. It operates by sequentially prompting GPT-4 with instructions, guiding it to reason about the current state of the task, consider its relevance to the \\ue103nal goal, and plan the next steps accordingly. ChemCrow has proven to be an eﬀective assistant to expert chemists, while also lowering the entry barrier for non-experts by oﬀering a simple interface to access accurate chemical knowledge.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='language processing and conversational capabilities, extensively pre-trained LLMs have demonstrated signi\\ue103cant potential in predicting molecular and material properties, as well as in the inverse design of func- tional molecules and materials. Task-speci\\ue103c \\ue103ne-tuning of GPT-3 has resulted in surprisingly eﬀective prediction perfor- mances across a range of chemistry ML tasks, o\\ue09den surpassing the performance of dedicated ML models speci\\ue103cally developed for these tasks.15 Notably, the \\ue103ne-tuning of GPT-3 showed exceptional strength in low-data ML tasks. Furthermore, the performance of the \\ue103ne-tuned GPT-3 models remained robust\\n\\nIn addition to their natural\\n\\ncSchool of Chemistry, School of Computer Science, University of Birmingham, Birmingham B15 2TT, UK. E-mail: l.j.chen@bham.ac.uk\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nChem. Sci.\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Chem. Sci.\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nChemical Science\\n\\nregardless of the representation used, such as chemical names or line representations like SMILES or SELFIES. This suggests that GPT-3 is adept at extracting correlations from any form of text. However, it is crucial to exercise caution when interpreting the success of such \\ue103ne-tuned GPT-3 models. Impressive performance likely indicates that the GPT-3 model has identi- \\ue103ed and exploited correlations in the data for predictions. It does not necessarily imply that these correlations are chemi- cally meaningful or causal.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"There is a rapidly growing community of researchers who are exploring ways to leverage LLMs for chemical discovery chal- lenges. A recent thematic hackathon provided 14 compelling examples of how LLMs can revolutionize materials science and chemistry.16 These examples spanned a wide array of applica- tions, from predictive modeling to the creation of educational tools, illustrating the models' capacity to go beyond their initial training parameters. The event highlighted the ability of LLMs to extract information from unstructured data and to seam- lessly integrate diﬀerent tools via natural language interfaces. The versatility of LLMs displayed in projects for predictive modeling, interface design, knowledge extraction, and educa- tional tool development indicates their potential to enhance work\\ue104ow eﬃciency, minimize errors, and increase productivity within scienti\\ue103c \\ue103elds.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"In this study, we hypothesize that the combination of GPT- 3's language understanding capabilities and the inherently human-readable nature of the SMILES notation17 may enable eﬀective recognition of signi\\ue103cant patterns within chemical structures and capture the dependencies of molecular proper- ties on these structures. To test this hypothesis, we approach several molecular property prediction tasks by applying GPT-3 to the classi\\ue103cation of SMILES strings. Our aim is to explore GPT-3's ability to discern subtle diﬀerences in molecular structures and to accurately classify compounds into speci\\ue103c categories, as de\\ue103ned by their molecular properties.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='We focus on assessing the eﬃcacy of \\ue103ne-tuning GPT-3 for predicting the electronic properties of organic molecules. We use a dataset of organic molecules extracted from the Cambridge Structural Database, previously reported by some of the authors here.18 The dataset consists of 48 182 organic molecules, all with documented synthetic pathways and stability in the solid state. Their electronic properties, relevant to semiconductor applica- tions, were determined by quantum chemical calculations. We present results for \\ue103ne-tuned GPT-3 models in predicting ener- getics of the frontier molecular orbitals; that is, energies of the Highest Occupied Molecular Orbital (HOMO) and Lowest Unoccupied Molecular Orbital (LUMO). We compare the performance of these GPT-3 models with that of message- passing graph neural networks.19 Additionally, we test the robustness of our \\ue103ne-tuned GPT-3 models against ‘adversarial attacks’, explore the potential explicability of the models in correlating molecular'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='robustness of our \\ue103ne-tuned GPT-3 models against ‘adversarial attacks’, explore the potential explicability of the models in correlating molecular structure with properties, and the evaluate the ability of \\ue103ne-tuned GPT-3 models to make predictions for ‘unknown’ molecules that were not represented in the training set. Finally, we discuss the limitations and challenges associated with using LLMs in chemical classi\\ue103cation tasks and propose potential avenues for future research and improvements.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Chem. Sci.\\n\\nView Article Online\\n\\nEdge Article\\n\\nMethods Background knowledge of GPT-3'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"Generative Pre-trained Transformer 3 (GPT-3),11 developed by is a sophisticated large-scale language generation OpenAI, it employs self- model. Using a transformer architecture, attention mechanisms to manage long-range dependencies within text. The model can generate sentences that harmonize with any given context based on the highest probability. GPT-3 was pre-trained on a wide-ranging corpus of text data, including text, books, and articles, under an unsupervised internet learning framework. This pre-training phase empowered the model to predict the next word in a sentence, thereby facili- tating the learning of patterns, structures, and representations in language. With its 175 billion parameters, GPT-3 stands as one of the largest language models currently available. Post pre- training, GPT-3 underwent \\ue103ne-tuning using task-speci\\ue103c data, preparing it for speci\\ue103c applications such as text generation, machine translation, and question answering. This process optimized the model's\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"data, preparing it for speci\\ue103c applications such as text generation, machine translation, and question answering. This process optimized the model's capabilities and performance for these speci\\ue103c tasks.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"The GPT-3 model incorporates a multi-layered self-attention mechanism, borrowed from the transformer model, into the decoder segment of the encoder–decoder architecture. This allows GPT-3 to capture dependencies among all words in a sentence simultaneously, thus enabling it to comprehend long-range dependencies and contextual information. One of GPT-3's notable features is its ability for zero-shot and few-shot learning. In other words, it can generate coherent text with little or no task-speci\\ue103c training data, indicating its comprehensive understanding of the structure of language. Additionally, GPT-3 exhibits transfer learning, seamlessly applying knowledge from one domain to another. However, as noted frequently by others, GPT-3 may also generate incorrect or nonsensical responses and display biases that are inherent in its training data.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"Pre-trained GPT-3 models, such as ‘ada’, can be \\ue103ne-tuned to specialize in speci\\ue103c tasks or domains using OpenAI's API. Fine- tuning refers to the process of adapting a base model, which has already been pre-trained on a vast corpus of generalized data, to perform better on a more speci\\ue103c task. During this process, the model's parameters are adjusted to minimize errors for the new task. This allows the model to tailor its knowledge for the speci\\ue103c task, enhancing its performance.\\n\\nSimpli\\ue103ed molecular input line entry system (SMILES)\\n\\nSMILES is a notation system used in chemistry that provides a compact, human-readable way to represent a molecular structure using ASCII strings.18 A SMILES string is composed of atomic symbols and indications of connectivity and is read from le\\ue09d to right. Hydrogen atoms are usually not explicitly repre- sented as it is assumed that they are present as required by the molecule's standard valences.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='In a broad sense, SMILES can be considered a type of language, designed to provide a standardized method of writing chemical structures in text form. Like all languages, SMILES has its own syntax (rules about structure) and semantics (rules\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Creative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article\\n\\nabout meaning). The syntax includes the use of speci\\ue103c char- acters to represent atoms, bonds, ring structures, and branches, while the semantics de\\ue103ne how these characters are interpreted as chemical structures. In this respect, learning to write in SMILES is somewhat akin to learning a new language where understanding the rules and conventions is crucial. However, unlike human languages, the SMILES notation lacks grammar rules that govern word order and does not convey meaning through the arrangement of ‘words’ into ‘sentences’. Each SMILES string represents a single molecule, and in common usage they are not typically read sequentially to extract addi- tional meaning.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"Fine-tuning GPT-3 for molecular ML tasks When a SMILES string such as ‘c1ccccc1’ for benzene is input into the chat interface of a GPT model, the typical response provided is: “The SMILES string c1ccccc1 represents benzene, a simple aromatic compound with a ring structure.” This response re\\ue104ects the GPT model's stored knowledge, which does not extend to predicting the electronic or functional properties of molecules. To enable the GPT model to predict such properties, it must be \\ue103ne-tuned with data that is speci\\ue103c to the task at hand, such as molecular property prediction.\\n\\nAll our \\ue103ne-tuned GPT-3 models made use of the “ada” base model. The training data points were structured as prompt- completion pairs, following the format given below, and stored in JSONL \\ue103les:\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='{\"prompt\":\"SMILES\",\"completion\":\"property class label\"}. In this format, the SMILES string of a molecule serves as the prompt, which is then completed with a class label assigned to the molecule for a speci\\ue103c property (e.g., its HOMO value). The property class labels were categorized as 0/1, 0/1/2, and 0/1/2/3 for binary, ternary, and quaternary classi\\ue103cations, respectively. The GPT series of models, like other language processing models, use a step known as tokenization as part of their pre- processing. In this process, a text segment is divided into smaller units known as tokens, which can range in size from a single character to an entire word. Each token is subsequently converted into a unique ID using a vocabulary list on which the model has been trained. Every word or character within this vocabulary list corresponds to a unique ID. This series of token IDs is then input into the GPT model for processing. The model employs these token IDs to comprehend the structure and'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='ID. This series of token IDs is then input into the GPT model for processing. The model employs these token IDs to comprehend the structure and semantics of the input text and produce an equivalent output. This output, also a series of token IDs, is ultimately converted back into text for user readability. is, detokenized) Throughout this work, the default tokenizer of the GPT API was used. Fig. 1 provides an illustration of a tokenized SMILES string and its corresponding sequence of token IDs.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='(that\\n\\nResults and discussion Machine learning molecular properties\\n\\nWe focused primarily on a dataset of organic semiconductor (OSC) molecules extracted from the Cambridge Structural Database (CSD),18 which is referred to as the OSCs dataset\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nView Article Online\\n\\nChemical Science\\n\\nFig. 1 Tokenization of SMILES strings for GPT models.\\n\\nherea\\ue09der. This dataset comprises 48 182 organic molecules, each accompanied by its SMILES representation and several quantum-chemically computed electronic properties. We \\ue103ne- tuned the “ada” base model of GPT-3 for multiclass classi\\ue103ca- tion tasks on HOMO and LUMO values. Class thresholds were determined by values that equally segmented the property (HOMO or LUMO) value range into the required number of classes. The entire dataset was randomly split into two sets, with 80% of the data allocated for \\ue103ne-tuning and the remaining 20% reserved for hold-out validation.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='All overall accuracy values, reported in Table 1, are for predictions on the hold-out validation set. For the ternary classi\\ue103cation of HOMO and LUMO, the \\ue103ne-tuned GPT-3 models achieved high prediction accuracies of 0.92 and 0.94 respectively. However, as the number of classi\\ue103cation classes increased from 3 to 5, the performance of \\ue103ne-tuned GPT-3 models was noticeably impacted, as indicated by the signi\\ue103- cantly lower prediction accuracies for HOMO predictions (Table 1). This suggests inherent limitations in the applicability of \\ue103ne- tuning GPT-3 for molecular ML tasks. For example, such models might be applicable for inexpensively sorting large numbers of candidate molecules into batches for subsequent electronic structure calculations—for example, to identify molecules that are likely to have ‘high’ or a ‘low’ HOMO (or LUMO) energies, or a narrow optical gap (e.g., high HOMO, low LUMO pairs), but such models are unlikely to be useful for near-quantitative predictions, for which a'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='or a narrow optical gap (e.g., high HOMO, low LUMO pairs), but such models are unlikely to be useful for near-quantitative predictions, for which a much larger number of classi\\ue103cations classes would be required.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"A graph neural network (GNN) was chosen to be a baseline model for benchmarking the performance of \\ue103ne-tuned GPT-3 models on the same molecular ML tasks. All GNN-based results reported here were obtained using the Chemprop package, which implements a directed message passing neural network (D-MPNN).20 Chemprop's D-MPNN has demonstrated robust capabilities in predicting molecular properties across a range of topics, from computed electronic properties to protein binding aﬃnities and to molecular toxicities. We used\\n\\nChem. Sci.\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nView Article Online\\n\\nChemical Science\\n\\nEdge Article\\n\\nTable 1 Fine-tuning GPT-3 for molecular ML tasks\\n\\nDataset\\n\\nSize\\n\\nPrediction task\\n\\nData splita\\n\\nNumber of classes\\n\\nGPT-3 accuracyb\\n\\nGNN accuracy\\n\\nDescriptors accuracyc\\n\\nOSCs OSCs OSCs OSCs AMPs\\n\\n48 182 48 182 48 182 48 182 572\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Data splita\\n\\nNumber of classes\\n\\nGPT-3 accuracyb\\n\\nGNN accuracy\\n\\nDescriptors accuracyc\\n\\nOSCs OSCs OSCs OSCs AMPs\\n\\n48 182 48 182 48 182 48 182 572\\n\\nHOMO HOMO HOMO LUMO HER\\n\\nTrain : test = 0.8 : 0.2 Train : test = 0.8 : 0.2 Train : test = 0.8 : 0.2 Train : test = 0.8 : 0.2 Strati\\ue103ed 10-fold\\n\\n3 4 5 3 2\\n\\n0.92 0.68 0.60 0.94 0.88\\n\\n0.94 0.75 0.68 0.94 0.86\\n\\n0.87 0.47 0.40 0.91 0.87'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='3 4 5 3 2\\n\\n0.92 0.68 0.60 0.94 0.88\\n\\n0.94 0.75 0.68 0.94 0.86\\n\\n0.87 0.47 0.40 0.91 0.87\\n\\na For each of the ML tasks on the OSCs dataset, the same training and test (hold-out validation) data were used by GPT-3 and GNN models. b GPT-3 was independently \\ue103ne-tuned for each ML task. c The RDKit function CalcMolDescriptors() was used to calculate all available descriptors. Molecules for which signi\\ue103cantly fewer descriptors were calculated were excluded; descriptors with missing values for any molecule were also discarded. A\\ue09derwards, feature selection was done using the SelectKBest (K = 20) method in scikit-learn. The resultant features were scaled individually to the range of [0,1]. Finally, an SVM classi\\ue103er was trained for the speci\\ue103c task.\\n\\nthe default molecular graph representation generated by Chemprop, without augmenting it with any additional atom-, bond-, or molecule-level features.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"Table 1 shows that for ternary classi\\ue103cation of HOMO and LUMO on the OSCs dataset, the \\ue103ne-tuned GPT-3 models per- formed on par with the trained GNN models. However, GPT-3 slightly underperformed compared to the GNN models on the 4-class and 5-class classi\\ue103cation tasks for HOMO. This is perhaps unsurprising, as essentially both the SMILES repre- sentation input into GPT-3 and the molecular graph represen- tation input into the GNN encode the same information regarding a molecule's atoms and their connectivity.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"We also explored the dependence of GPT-3's prediction performance on the size of the data used for \\ue103ne-tuning. We \\ue103ne-tuned GPT-3 for ternary classi\\ue103cations of HOMO and LUMO using various fractions of the complete OSCs dataset, ranging from 1% to 80% of the 48 182 data points. For comparison, GNN models were trained on the same classi\\ue103ca- tion tasks using the same training data as for the \\ue103ne-tuning of GPT-3. The learning curves obtained for the various machine learning tasks and models are shown in Fig. 2. With fewer than 1000 training data points (1% and 2% of the OSCs dataset), the \\ue103ne-tuned GPT-3 models performed poorly, achieving accura- cies below 0.6 on the hold-out validation data. However, signi\\ue103cant improvements in prediction performance were observed when the size of the training data increased to 20% of the complete OSCs dataset, with prediction accuracies exceeding 0.9 for both HOMO and LUMO classi\\ue103cations. Further expanding the training data size up to 80% of the OSCs\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"with prediction accuracies exceeding 0.9 for both HOMO and LUMO classi\\ue103cations. Further expanding the training data size up to 80% of the OSCs dataset only marginally improved the prediction performance of the \\ue103ne-tuned GPT-3 models, achieving accuracies of 0.92 and 0.94 for HOMO and LUMO, respectively. The GNN's prediction performance was nearly equivalent to that of GPT-3 when the training data size was 20% or larger. However, GNN outperformed GPT-3 in the low-data region. This may in part be attributed to two factors: (1) the molecular graph representation being chemically more expressive than SMILES for the ML tasks, and/or (2) the \\ue103ne-tuning of GPT-3 necessitating a suﬃ- cient amount of data to capture the patterns in SMILES relevant to the ML tasks.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"models for a second molecular dataset. This dataset includes 572 aromatic organic molecules that were experimentally assessed by some of the authors here for sacri\\ue103cial photo- catalytic hydrogen evolution.21 Employing the same procedures and setups as for the OSCs dataset, we \\ue103ne-tuned GPT-3 models and trained GNN models to predict hydrogen evolution rates (HERs) for these aromatic molecular photocatalysts (AMPs). To compare the performances of these GPT-3 and GNN models with the ML studies from the original work, we implemented strati\\ue103ed 10-fold cross-validation. The \\ue103ne-tuned GPT-3 ach- ieved an accuracy score of 0.88, slightly outperforming the GNN's score of 0.86 and closely matching the highest prediction performance (0.89 accuracy) reported in the original work. It is interesting that GPT-3 is competitive here with ML models in the original work21 that made use of engineered, domain- speci\\ue103c features to encode molecular electronic properties.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Another baseline was established to benchmark the perfor- mance of the \\ue103ne-tuned GPT-3 on the molecular ML tasks, as shown in Table 1. This involved calculating all available molec- ular descriptors for the molecules in the dataset (either OSCs or\\n\\nIn addition to the results from the OSCs dataset, Table 1 also presents results from both \\ue103ne-tuned GPT-3 models and GNN\\n\\nFig. 2 Learning curves for ternary classiﬁcations of HOMO and LUMO by ﬁne-tuned GPT-3 and trained GNN models. The inset provides a close-up view of the curves for training data sizes that comprise 20% or more of the complete OSCs dataset.\\n\\nChem. Sci.\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Creative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article\\n\\nAMPs) from SMILES using the RDKit package, which yielded over 200 descriptors for most molecules. Molecules for which signif- icantly fewer descriptors were calculated were excluded. Simi- larly, descriptors with missing values for any molecule were also discarded. The resulting complete matrix was subjected to a feature selection process using the SelectKBest method within the scikit-learn package, retaining the top 20 descriptors as determined by univariate statistical tests. A support vector machine (SVM) classi\\ue103er was then trained for each speci\\ue103ed ML task. Table 1 shows that the descriptor-based models consistently underperformed in the molecular ML tasks compared to the \\ue103ne- tuned GPT-3 and GNN models.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='These results indicate that \\ue103ne-tuning GPT-3 could be an eﬀective approach for ML tasks related to molecular properties. It may be particularly advantageous given that \\ue103ne-tuning GPT- 3 requires minimal eﬀort in preparing ML inputs, compared to the eﬀort required in designing and calculating molecular features or, to a lesser extent, generating molecular graphs. However, GPT-3 functions as a “black box” with only a few parameters available for adjustment, and as such it does not provide physical insight or explainability like ML models trained on engineered, physicochemical features. Nor does it oﬀer the same level of explicability that is possible with GNN models.\\n\\nAblation study 1: single-atom removal'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Ablation study 1: single-atom removal\\n\\nWe next conducted a series of ablation tests, where certain sections of the SMILES prompts were systematically removed or ‘ablated’ to assess the robustness of the \\ue103ne-tuned GPT-3 models against information loss. By comparing the predic- tions using the complete prompt (i.e., complete SMILES strings) to those of the ablated versions (with certain parts of the SMILES strings removed), we aimed to (i) determine if the \\ue103ne- tuned GPT-3 models had learned chemically meaningful patterns, rather than merely “memorizing” the training data, and (ii) get a sense of the inner workings of the models.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='The \\ue103rst type of ablation test involved single-atom removal: each of the non-hydrogen (H), non-carbon (C) atoms in a SMILES string was removed, one at a time, and these ablated SMILES strings were used as prompts for the corresponding \\ue103ne-tuned GPT-3 model (Fig. 3a). For the example SMILES shown in Fig. 3a, \\ue103ve ablated SMILES strings would be created, each with either one of the two oxygen atoms, one of the two chlorine atoms, or the nitrogen atom removed. We used a designated empty token, denoted as <missing>, to replace the atom being ablated. The non-hydrogen, non-carbon atoms involved in the complete OSCs dataset included elements: boron (B), nitrogen (N), oxygen (O), \\ue104uorine (F), silicon (Si), phosphorus (P), sulfur (S), chlorine (Cl), arsenic (As), selenium (Se), bromine (Br), and iodine (I).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='All ablation tests were conducted using the \\ue103ne-tuned GPT-3 model for ternary classi\\ue103cation of HOMO, which was trained using 80% of the complete OSCs dataset. These ablation tests were performed on all data points in the 20% hold-out valida- tion set that were correctly predicted using complete SMILES strings. As a result, 7714 SMILES strings were examined, leading\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nView Article Online\\n\\nChemical Science\\n\\nto a total of 45 763 single-atom-removal ablation tests. Out of these 45 763 ablated SMILES strings, 43 588 tests (95.2%) yiel- ded the same classi\\ue103cation predictions as their corresponding complete SMILES strings. This \\ue103nding suggests that the \\ue103ne- tuned GPT-3 model was resilient to minor information loss in the text prompts, indicating a degree of robustness.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Fig. 3b provides a breakdown of the 45 763 ablation tests conducted on the 7714 SMILES strings. The vast majority (7106) of these SMILES strings underwent no more than 10 ablation tests each (as shown in the \\ue103rst column of the table in Fig. 3b), meaning that they contained no more than 10 non-hydrogen, non-carbon atoms. Out of these, 6015 SMILES strings remained unaﬀected by the removal of a single atom, as the agreement between predictions based on complete and ablated SMILES strings was 100% for all of them (these SMILES strings contained 1 to 10 non-hydrogen, non-carbon atoms). Conversely, 15 SMILES strings, each containing between 1 and 5 non-hydrogen, non-carbon atoms, were found to be highly sensitive to single-atom ablations, yielding a 0% agreement rate. Two of these molecules are shown in Fig. 3c, with their corresponding CSD reference codes labeled.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='As SMILES strings contain increasingly more non-hydrogen, non-carbon atoms, they generally become less problematic for the \\ue103ne-tuned GPT-3 model to predict correctly, with a few exceptions such as the molecules shown in Fig. 3d–f. Our visual inspections of the molecules with 11 to 20 atoms to ablate suggests that the high number of ablatable atoms relative to the size of the molecule of diﬀerent elemental types makes it challenging for GPT-3 to handle. However, this empirical ‘rule’ does not always hold, as demonstrated by the molecule in Fig. 3g, which has the largest number of atoms (69 atoms of 5 elemental types) to ablate in this set, yet it yielded 100% agreement between complete and ablated SMILES strings.\\n\\nAblation study 2: single-group removal'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='The second type of ablation test we conducted involved the removal of speci\\ue103c chemical groups from SMILES strings such as, for example, a nitrile group. This was done by replacing the atoms involved in the chemical group with <missing> annota- tions, as illustrated in Fig. 4a. We considered 15 diﬀerent chemical groups, which are listed in Table 2. To locate and identify substructures representing the intended chemical groups in the complete SMILES strings, we used the SMARTS representation and the RDKit package. To ensure a suﬃcient number of molecules containing each chemical group for ablation tests, we \\ue103ne-tuned the GPT-3 models using 40% of the complete OSCs dataset, reserving the remaining 60% of the data for ablation tests. As shown in Fig. 2 and discussed above, GPT- 3 models \\ue103ne-tuned with 40% of the complete OSCs dataset achieved comparable predictive abilities to those \\ue103ne-tuned with 80% of the dataset. We \\ue103ne-tuned GPT-3 for ternary clas- si\\ue103cations of both HOMO and LUMO'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='comparable predictive abilities to those \\ue103ne-tuned with 80% of the dataset. We \\ue103ne-tuned GPT-3 for ternary clas- si\\ue103cations of both HOMO and LUMO values.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Table 2 summarizes the results of the ablation tests per- formed on the 15 chemical groups. Like the single-atom abla- tion tests, these tests were only conducted on molecules in the hold-out validation set that were correctly predicted by the \\ue103ne-\\n\\nChem. Sci.\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nView Article Online\\n\\nChemical Science\\n\\nEdge Article'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Fig. 3 (a) Illustration of single-atom ablation tests, where one non-hydrogen, non-carbon atom is removed from the SMILES string and replaced with a designated empty token, <missing>. (b) A breakdown of results for the 45 763 ablation tests conducted on 7714 SMILES strings. The horizontal axis indicates the number of ablation tests conducted on a speciﬁc molecule, and the vertical axis represents the agreement rate between predictions based on complete and ablated SMILES strings. For instance, if a molecule contained three non-hydrogen, non-carbon atoms to be ablated and one out of the three ablated SMILES strings yielded the same prediction as the complete SMILES string (i.e., a 33% agreement rate), this molecule would be counted towards the table element that corresponds to 1–10 ablation tests and an agreement rate in the range of (30%, 40%]. The numbers displayed within the table represent the numbers of molecules categorized by the respective table elements. (c)–(g) Representative'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='40%]. The numbers displayed within the table represent the numbers of molecules categorized by the respective table elements. (c)–(g) Representative molecules corresponding to the labeled table elements in (b).'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='tuned GPT-3 models. In each test, only one type of chemical group was ablated at a time. If a molecule contained multiple instances of the same chemical group, each was ablated one at a time, leading to a corresponding number of ablation tests. For example, 1833 ablation tests were performed on nitrile-group- containing molecules from the hold-out 60% of the OSCs dataset (that were correctly predicted based on complete SMILES strings). This number exceeds the actual number of these molecules, as some contained multiple nitrile groups. In 91% of these 1833 tests, the HOMO predictions based on SMILES strings with one nitrile group ablated agreed with the predictions using the complete SMILES strings.\\n\\nOur results suggest that, across the 15 chemical groups probed, the \\ue103ne-tuned GPT-3 model attributed signi\\ue103cant importance to the acetylene, enamine, nitro, ketone, and sulfonamide groups in its HOMO predictions. This is evident as'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='the model altered its HOMO class assignments in more than 10% of the ablation tests for each of these groups. For LUMO predictions, the \\ue103ne-tuned GPT-3 model only altered its LUMO class assignments in more than 10% of the ablation tests for the thiol and sulfonic acid groups. However, the quantities of ablation tests for these two chemical groups were low (56 and 28 respectively), implying that the low agreement rates could be due to the small sample sizes of the tests. One possible inter- pretation here is that the more ‘important’ functionalities tend to be those that participate in electronic p-conjugation.\\n\\nWe further examined a few molecules by implementing a diﬀerent test. Instead of ablating the atoms belonging to the chemical group of interest, we replaced them with atoms of randomly selected elemental types (Fig. 4b). For the molecule shown in Fig. 4, the \\ue103ne-tuned GPT-3 model correctly assigned the HOMO class to the ablated SMILES string (Fig. 4a). We then\\n\\nChem. Sci.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Chem. Sci.\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nView Article Online\\n\\nEdge Article\\n\\nChemical Science\\n\\nFig. 4 (a) Illustration of single-group ablation tests, where a chemical group is identiﬁed (in this case a nitrile group) using its SMARTS notation and replaced with <missing> annotations for atoms belonging to the chemical group. (b) Instead of ablating atoms from the SMILES string, each atom belonging to the target chemical group was replaced with an atom of a randomly selected element type (B, N, O, F, Si, P, S, Cl, As, Se, Br, or I). For each of several investigated molecules, 100 such random variants of the SMILES string were tested.\\n\\nTable 2 Single-group ablation tests\\n\\nHOMO\\n\\nLUMO\\n\\nChemical group\\n\\nSMARTS\\n\\nNo. of tests\\n\\nAgreement (%)\\n\\nNo. of tests\\n\\nAgreement (%)'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Table 2 Single-group ablation tests\\n\\nHOMO\\n\\nLUMO\\n\\nChemical group\\n\\nSMARTS\\n\\nNo. of tests\\n\\nAgreement (%)\\n\\nNo. of tests\\n\\nAgreement (%)\\n\\nNitrile Nitro Imine Enamine Ketone Carbonyl with nitrogen Carbonyl with oxygen Thiol Thiocarbonyl Sulfone\\n\\n[NX1]#[CX2] [$([NX3](=O)=O),$([NX3+](=O)[O-])][!#8] [$([CX3]([#6])[#6]),$([CX3H][#6])]=[$([NX2][#6]),$([NX2H])] [NX3][$(C=C),$(cc)] [#6][CX3](=O)[#6] [OX1]=CN\\n\\n[CX3](=[OX1])O\\n\\n*-[S;D1] *=[S;D1] [$([#16X4](=[OX1])(=[OX1])([#6])[#6]),$([#16X4+2]([OX1-])([OX1-])([#6]) [#6])] *-[S;D4](=O)(=O)-[O;D1] [$([#16X4](=[OX1])(=[OX1])([#6])[OX2H0]),$([#16X4+2]([OX1-])([OX1-])([#6]) [OX2H0])] [$([#16X4]([NX3])(=[OX1])(=[OX1])[#6]),$([#16X4+2]([NX3])([OX1-])([OX1-]) [#6])] *-[C;D2]#[C;D1;H] Acetylene Halogens: F, Cl, Br, I *-[#9,#17,#35,#53]\\n\\nSulfonic acid Sulfonate\\n\\nSulfonamide\\n\\n1833 3485 1780 16 747 4647 4234\\n\\n4940\\n\\n57 1452 236\\n\\n39 69\\n\\n351\\n\\n83 7131\\n\\n91 86 97 85 87 91\\n\\n93\\n\\n91 92 90\\n\\n97 97\\n\\n89\\n\\n81 90\\n\\n1831 3968 1867 16 817 5015 4521\\n\\n5260\\n\\n56 1455 262\\n\\n28 95\\n\\n383'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='57 1452 236\\n\\n39 69\\n\\n351\\n\\n83 7131\\n\\n91 86 97 85 87 91\\n\\n93\\n\\n91 92 90\\n\\n97 97\\n\\n89\\n\\n81 90\\n\\n1831 3968 1867 16 817 5015 4521\\n\\n5260\\n\\n56 1455 262\\n\\n28 95\\n\\n383\\n\\n77 7456\\n\\n94 93 97 92 96 97\\n\\n96\\n\\n79 94 91\\n\\n82 95\\n\\n97\\n\\n96 95\\n\\ngenerated 100 randomly mutated SMILES strings as shown in Fig. 4b. In 80% of these mutated SMILES strings, the same \\ue103ne- tuned GPT-3 model failed to assign the correct HOMO class. This observation, which is not unique to the example provided, seems to suggest that the GPT-3 model \\ue103lled in the ‘missing’ tokens before making the property prediction. This might partially explain the high agreement rates between predictions based on complete and ablated SMILES strings (Table 2). However, there were numerous cases where the \\ue103ne-tuned GPT- 3 model gave identical predictions irrespective of the mutations to the SMILES string.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='predictions for unknown molecules. To do this, we identi\\ue103ed molecules belonging to families as identi\\ue103ed by the presence of common moieties using conjugated molecular fragments. For example, within the OSCs dataset, we found 72 molecules that contained at least one tetracene fragment, as illustrated in Fig. 5. Once we identi\\ue103ed such a family of molecules, we excluded all members of that family from both the \\ue103ne-tuning of a GPT-3 model and the training of a GNN model, hence making this class of molecules eﬀectively ‘unknown’. The remainder of the dataset was then used to train these models. We then used these ML models to predict the target molecular properties for the unknown family of molecules.\\n\\nPredicting molecular properties for ‘unknown’ molecules We further evaluated the eﬀectiveness of \\ue103ne-tuning GPT-3 for generating learning molecular properties by machine'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='the \\ue103rst \\ue103ve families of polycyclic aromatic hydrocarbon-containing molecules (labelled 1–5 in Table 3) was eﬀectively classi\\ue103ed by the GPT-3 models, which were \\ue103ne-\\n\\nEach of\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nChem. Sci.\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nView Article Online\\n\\nChemical Science\\n\\nEdge Article\\n\\n8) or imides (9–11). In all cases, GNN models outperformed their corresponding \\ue103ne-tuned GPT-3 models marginally.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"Chemical Science\\n\\nEdge Article\\n\\n8) or imides (9–11). In all cases, GNN models outperformed their corresponding \\ue103ne-tuned GPT-3 models marginally.\\n\\nTo further test the eﬀectiveness of \\ue103ne-tuning GPT-3, we excluded families 1–8 of molecules during \\ue103ne-tuning when predicting for quinone molecules belonging to families 6–8. Similarly, for the imide molecules belonging to families 9–11, we \\ue103ne-tuned GPT-3 models while excluding families 1–11. Despite further limiting the model's exposure to patterns shared between target molecules and similar ones, the \\ue103ne- tuned GPT-3 models performed robustly in predicting the properties of the unknown molecules. These more stringent tests further reinforce that \\ue103ne-tuning GPT-3 can be an eﬀective strategy for ML tasks involving molecular properties.\\n\\nFine-tuning GPT-3 with both canonical and non-canonical SMILES\\n\\nFig. 5 Examples of tetracene-containing molecules in the OSCs dataset, with their CSD reference codes labeled.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Fig. 5 Examples of tetracene-containing molecules in the OSCs dataset, with their CSD reference codes labeled.\\n\\ntuned without these speci\\ue103c families of molecules. The \\ue103ne- tuned GPT-3 models demonstrated notably better perfor- mance in predicting HOMO than LUMO. Even when all \\ue103ve families of molecules were excluded from the training process, the subsequently \\ue103ne-tuned GPT-3 models still demonstrated robust performance in predicting their HOMO and LUMO classes. The eﬃcacy of \\ue103ne-tuning GPT-3 was slightly reduced when predicting ‘unknown’ molecules containing quinones (6–'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='The results discussed so far have been derived from GPT-3 models \\ue103ne-tuned on canonical SMILES strings as generated by RDKit. However, multiple valid SMILES strings can represent a single molecule: for example, CCO, OCC, and C(O)C all represent the same ethanol molecule. To address this, canon- icalization algorithms are employed to ensure consistency, generating a singular SMILES string for a given molecule from the multitude of possibilities. This unique output, albeit dependent on the speci\\ue103c canonicalization algorithm applied, is referred to as the canonical SMILES.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='To assess the performance of GPT-3 that was \\ue103ne-tuned on canonical SMILES strings against models \\ue103ne-tuned with non- canonical ones, we employed a GPT-3 model \\ue103ne-tuned for ternary classi\\ue103cation of HOMO. This model, which used an 80 : 20 train-to-test data split for the OSCs dataset, is referenced as the \\ue103rst model in Table 1. For every molecule in the test set (20% of the OSCs dataset, equating to 8578 molecules), we\\n\\nTable 3 Ternary classiﬁcation accuracies of ﬁne-tuned GPT-3 and trained GNN models for “unknown” molecules\\n\\nHOMO accuracy\\n\\nLUMO accuracy\\n\\nConjugated fragment\\n\\nNumber of molecules\\n\\nGPT-3\\n\\nGNN\\n\\nGPT-3\\n\\nGNN'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='HOMO accuracy\\n\\nLUMO accuracy\\n\\nConjugated fragment\\n\\nNumber of molecules\\n\\nGPT-3\\n\\nGNN\\n\\nGPT-3\\n\\nGNN\\n\\nNaphthalene (1) Anthracene (2) Tetracene (3) Pyrene (4) Perylene (5) (1) + (2) + (3) + (4) + (5)a p-Benzoquinone (6) 1,4-Naphthoquinone (7) 9,10-Anthraquinone (8) (1) + (2) + (3) + (4) + (5) + (6) + (7) + (8)b 1,8-Naphthalimide (9) Naphthalenetetracarboxylic diimide (10) Perylenetetracarboxylic diimide (11) (1) + (2) + (3) + (4) + (5) + (6) + (7) + (8) + (9) + (10) + (11)c\\n\\n475 577 72 237 41 1402 295 282 186 2165 85 88 76 3177\\n\\n0.94 0.99 0.96 0.98 0.98 0.97 0.83 0.82 0.86 0.88 0.86 0.86 0.85 0.88\\n\\n0.95 1.00 1.00 1.00 1.00 0.98 0.91 0.91 0.91 0.91 0.93 0.88 0.89 0.88\\n\\n0.88 0.93 0.90 0.97 0.98 0.93 0.87 0.94 0.99 0.95 1.00 0.95 0.97 0.97\\n\\n0.91 0.97 0.99 0.99 0.95 0.95 0.87 0.96 1.00 0.96 1.00 0.98 0.97 0.97'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='0.88 0.93 0.90 0.97 0.98 0.93 0.87 0.94 0.99 0.95 1.00 0.95 0.97 0.97\\n\\n0.91 0.97 0.99 0.99 0.95 0.95 0.87 0.96 1.00 0.96 1.00 0.98 0.97 0.97\\n\\na All \\ue103ve families of molecules were excluded from model training. The HOMO/LUMO prediction accuracies reported in this row were measured on these \\ue103ve families of molecules. b All eight families of molecules were excluded from model training. The HOMO/LUMO prediction accuracies reported in this row were measured on the families 6–8 of molecules. c All 11 families of molecules were excluded from model training. The HOMO/LUMO prediction accuracies reported in this row were measured on the families 9–11 of molecules.\\n\\nChem. Sci.\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Creative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article\\n\\ngenerated 10 valid non-canonical SMILES strings.22 The \\ue103ne- tuned GPT-3 ternary classi\\ue103er was then applied to predict the HOMO class labels for all 10 SMILES strings of each molecule. This process was designed to evaluate the consistency of HOMO predictions between the canonical SMILES string and the 10 non-canonical versions for each molecule.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"Fig. 6a categorizes the molecules according to the consis- tency level, determined by the number of non-canonical strings receiving the same HOMO class label as the canonical string. A consistency level of 0 means that none of the non-canonical strings matched the canonical string's prediction; a level of 10 indicates a perfect match for all. Disappointingly, the GPT-3 model trained solely on canonical SMILES strings performed poorly with non-canonical strings (as shown by the blue bars): for just 1622 out of 8578 molecules, the model predicted consistent class labels across all 11 SMILES variations; for 344 molecules, there was a complete lack of prediction consistency across the SMILES variations.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Fig. 6 (a) Consistency of HOMO predictions between the canonical SMILES string and 10 non-canonical versions for each molecule in the test set; the consistency level is determined by the number of non- canonical strings receiving the same HOMO class label as the canonical string. (b) Number of erroneous responses to the non- canonical SMILES strings of a molecule; erroneous responses are completions that are not a HOMO class label (0, 1, or 2). In both (a) and (b), the blue bars represent the GPT-3 model ﬁne-tuned exclusively with canonical SMILES strings, while the orange bars represent the GPT-3 model ﬁne-tuned on a dataset augmented with non-canonical SMILES strings. Both vertical axes are in the logarithmic scale.\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nView Article Online\\n\\nChemical Science'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nView Article Online\\n\\nChemical Science\\n\\nFurthermore, Fig. 6b (blue bars) reveals that the GPT-3 model o\\ue09den provided erroneous responses to non-canonical SMILES strings, meaning that it completed the prompt with a character that was not a HOMO class label (0, 1, or 2), as it had been \\ue103ne-tuned to do. Only for 2436 out of 8578 molecules did the model respond with a class label, regardless of whether these were consistent with the canonical reference. For the remainder, the model's completions were incorrect to varying degrees.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"The underwhelming performance of the model \\ue103ne-tuned solely with canonical SMILES strings on non-canonical strings is attributable to the absence of certain patterns in canon- icalized SMILES—namely, the diﬀerent valid permutations of arranging the same group of atoms, as illustrated by the ethanol example above. To address this issue, we expanded the training dataset, which initially comprised only canonical strings, to include \\ue103ve valid non-canonical versions for each molecule. Subsequently, we \\ue103ne-tuned another GPT-3 model using this augmented dataset and evaluated its performance on the same test set that included each molecule's canonical SMILES string along with 10 non-canonical variants; we followed the same evaluation methodology as used for the model trained only on canonical SMILES strings (represented by the blue bars in Fig. 6).\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Fig. 6a demonstrates signi\\ue103cant improvements achieved by incorporating non-canonical SMILES strings into the \\ue103ne- tuning process of GPT-3. For 7243 of the 8578 molecules in the test set, the new GPT-3 model consistently predicted the same HOMO class label across all 11 SMILES versions of each molecule. Across all other consistency levels, this enhanced GPT-3 model—trained with a mix of canonical and non- canonical SMILES—uniformly surpassed the model trained solely on canonical SMILES. Likewise, Fig. 6b shows that the new model produced no erroneous responses for 8488 of the 8578 molecules and very few erroneous responses for the remainder. These results suggest that the enhanced GPT-3 model has bene\\ue103ted from exposure to diverse representations of molecules possible within SMILES notation. It appears that increasing the variety in the training data aids in the \\ue103ne-tuning process, allowing GPT-3 to develop a more robust under- standing of molecular structures and their properties. This'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='data aids in the \\ue103ne-tuning process, allowing GPT-3 to develop a more robust under- standing of molecular structures and their properties. This makes the model less sensitive to variations in SMILES repre- sentation and enhances its ability to generalize from learned the paramount patterns. Equally, importance of training data in determining the performance of a \\ue103ne-tuned model when deployed for predictions.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"these results highlight\\n\\nConclusions\\n\\nOur results suggest that \\ue103ne-tuning GPT-3, and perhaps other LLMs, can be an eﬀective ML approach to predicting electronic and functional properties of organic molecules, at least in terms of relatively coarse-grained classi\\ue103cation tasks. In all ML tasks that we conducted, the \\ue103ne-tuned GPT-3 model yielded accurate predictions for the hold-out data and even for ‘unknown’ classes of molecules. Moreover, our ablation tests demonstrated the models' resilience against loss of information (due to\\n\\nChem. Sci.\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nChemical Science\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"Creative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nChemical Science\\n\\nmissing atoms and chemical groups) and noise (random changes in atomic identities). Similar observations of GPT models' robustness and their resilience against noise have also been made by others.15 These results lead us to assert that the extensively pre-trained GPT-3, when properly \\ue103ne-tuned, can detect and distinguish chemically meaningful patterns and discern subtle diﬀerences among them, thus eﬀectively ‘specializing’ in the chemistry problems at hand.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='This approach has several potential advantages: for example, employing SMILES strings as direct prompts to GPT-3 requires signi\\ue103cantly less computational memory compared to many alternative ML input data forms, such as molecular graphs or numerical representations like the Smooth Overlap of Atomic Positions (SOAP) descriptors. Consequently, the GPT-3 \\ue103ne- tuning approach could be especially advantageous for large molecular datasets composed of millions or even tens of millions of data points.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"However, while our \\ue103ndings underscore the potential utility of GPT-3 \\ue103ne-tuning for predicting molecular properties, there are also certain inherent limitations. First, it does not seem obvious how one might enhance the performance of a \\ue103ne- tuned GPT-3 model beyond augmenting the training data with more volume and/or diversity, which may not be available for many research goals; indeed, the success of our method here relied on the existence of the large, pre-computed OSC dataset.18 This limitation stems from GPT-3's ‘black box’ nature. By like the contrast, with a molecular graph-based approach, directed message-passing neural network used as the baseline in this work, additional chemical information may be incor- porated into the graph representation to potentially enhance prediction performance.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Re\\ue104ecting on the generalized tokenization applied to SMILES in our work (Fig. 1), we hypothesize that a specialized tokens—while tokenizer respecting the chemical nature of the molecular structure and its fragments—could enhance performance in data eﬃciency and/or prediction accuracy. In a related note, the SELFIES (SELF-referencing Embedded Strings) representation, which o\\ue09den outperforms SMILES in ML tasks, did not show improved performance in our initial tests. This is likely because the generic tokenization applied to SELFIES diminished the extra chemical information that it conveyed compared to SMILES.\\n\\nthat creates chemically\\n\\nrelevant'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"that creates chemically\\n\\nrelevant\\n\\nSecond, while task-speci\\ue103c \\ue103ne-tuning enables GPT-3 to recognize chemically relevant patterns, thus enhancing its predictive performance, the model does not inherently grasp the chemical principles underpinning the molecular properties. Its predictions are entirely based on pattern recognition and do not imply a deep understanding of the underlying science. This predicament is further complicated because GPT-3 was not yet open-sourced at the time of these studies, which restricts any systematic interpretation of why speci\\ue103c predictions were made. This limits the applicability of this method in scenarios where understanding the reasoning behind a prediction is important. While our ablation tests did shed some light on the importance of certain chemical groups, the \\ue103ndings could be swayed by the underlying assumptions and certainly do not provide a thorough comprehension of the model's decision-\\n\\nChem. Sci.\\n\\nView Article Online\\n\\nEdge Article\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Chem. Sci.\\n\\nView Article Online\\n\\nEdge Article\\n\\nmaking process: in no sense does GPT ‘know’, for example, that a ketone group is prone to conjugation. These challenges relate to model interpretability and, again, greater under- standing might be possible to be addressed if a fully open- sourced GPT-3 model were available or if a diﬀerent, open- sourced LLM was used.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"Lastly, \\ue103ne-tuning LLMs such as GPT-3 can demand considerable resources, making the process both computa- tionally intensive and \\ue103nancially burdensome, particularly for large datasets. In this work, all the \\ue103ne-tuning tasks conducted via OpenAI's API resulted in a total cost of approximately 500 US dollars, excluding the initial exploratory exercises. With other major oﬀerings of LLMs, either \\ue103ne-tuning is not available, or a local GPU capacity is required for the \\ue103ne-tuning process (or even pre-training prior to \\ue103ne-tuning) when applied to chem- istry tasks. For now, these hurdles may impede broader testing and/or adoption of LLMs within the chemistry \\ue103eld, following the initial surge of eﬀorts that has centered primarily on GPT models.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content=\"In summary, our exploration of \\ue103ne-tuning GPT-3 demon- strates a promising new approach for predicting molecular properties and, more widely, for discerning patterns in large chemistry datasets. While this strategy has distinct limitations exist, future work in advancing tokenization techniques, improving model interpretability, and reducing computational demands could see LLMs such as GPT-3 becoming an integral part of the chemist's toolkit to complement more traditional computational predictions.\\n\\nData availability\\n\\nThe authors declare that data supporting the \\ue103ndings of this study are available within the paper. All python codes and datasets used in this study are available at https://github.com/ XieZikai/Chem-GPT-Finetune.\\n\\nAuthor contributions\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Author contributions\\n\\nL. C. and A. I. C. conceived and supervised the project. Z. X. carried out all the computational work, with support from L. C. and X. E. ¨O. H. O and A. T. provided the OSCs dataset and its subsets. L. C., Z. X., and A. I. C. led the preparation of the manuscript.\\n\\nConﬂicts of interest There are no con\\ue104icts to declare.\\n\\nAcknowledgements\\n\\nThe authors acknowledge funding from the Leverhulme Trust via the Leverhulme Research Centre for Functional Materials Design. Z. X. thanks the China Scholarship Council for a PhD studentship. The authors thank Dr Keith Butler for useful discussions. A. I. C. thanks the Royal Society for a Research Professorship.\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nOpen Access Article. Published on 05 December 2023. Downloaded on 1/1/2024 11:13:44 PM.\\n\\nCreative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article\\n\\nReferences'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Creative Commons Attribution 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article\\n\\nReferences\\n\\n1 K. T. Butler, D. W. Davies, H. Cartwright, O. Isayev and A. Walsh, Machine learning for molecular and materials science, Nature, 2018, 559, 547–555.\\n\\n2 N. Ree, A. H. Göller, J. H. Jensen and M. L. Regio, Predicting the regioselectivity of electrophilic aromatic substitution reactions using machine learning, Digital Discovery, 2022, 1, 108–114.\\n\\n3 P. Yang, E. A. Henle, X. Z. Fern and C. M. Simon, Classifying the toxicity of pesticides to honey bees via support vector machines with random walk graph kernels, J. Chem. Phys., 2022, 157, 034102.\\n\\n4 Z. Tu, T. Stuyver and C. W. Coley, Predictive chemistry: reaction machine development, and reaction discovery, Chem. Sci., 2023, 14, 226–244.\\n\\nlearning for\\n\\nreaction deployment,\\n\\n5 Q. Yuan, F. T. Szczypi´nski and K. E. Jelfs, Explainable graph neural networks for organic cages, Digital Discovery, 2022, 1, 127–138.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='5 Q. Yuan, F. T. Szczypi´nski and K. E. Jelfs, Explainable graph neural networks for organic cages, Digital Discovery, 2022, 1, 127–138.\\n\\n6 A. Nandy, et al., Computational Discovery of Transition- metal Complexes: From High-throughput Screening to Machine Learning, Chem. Rev., 2021, 121, 9927–10000.\\n\\n7 J. G. Freeze, H. R. Kelly and V. S. Batista, Search for Catalysts by Intelligence, Mountain Climbers, and Alchemists, Chem. Rev., 2019, 119, 6595–6612. 8 N. W. A. Gebauer, M. Gastegger, S. S. P. Hessmann, K.-R. Müller and K. T. Schütt, Inverse design of 3d molecular structures with conditional generative neural networks, Nat. Commun., 2022, 13, 973.\\n\\nInverse Design: Arti\\ue103cial'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='Inverse Design: Arti\\ue103cial\\n\\nInverse molecular design using machine learning: Generative models for matter engineering, Science, 2018, 361, 360–365. 10 A. Gupta, S. Chakraborty, D. Ghosh and R. Ramakrishnan, Data-driven modeling of S0 / S1 excitation energy in the BODIPY chemical space: High-throughput computation, quantum machine learning, and inverse design, J. Chem. Phys., 2021, 155, 244102.\\n\\n9 B.\\n\\nSanchez-Lengeling\\n\\nand A. Aspuru-Guzik,\\n\\n© 2023 The Author(s). Published by the Royal Society of Chemistry\\n\\nView Article Online\\n\\nChemical Science\\n\\n11 T. B. Brown, et al., Language Models are Few-Shot Learners, arXiv, 2020, preprint, DOI: 10.48550/arXiv.2005.14165. 12 OpenAI, GPT-4 Technical Report, arXiv, 2023, preprint,\\n\\narXiv:2303.08774, DOI: 10.48550/arXiv.2303.08774.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='arXiv:2303.08774, DOI: 10.48550/arXiv.2303.08774.\\n\\n13 Z. Zheng, et al., GPT-4 Reticular Chemist for MOF Discovery, arXiv, 2023, preprint, DOI: 10.48550/arXiv.2306.14915. 14 A. M. Bran, S. Cox, A. D. White and P. Schwaller, ChemCrow: Augmenting large-language models with chemistry tools, arXiv, 2023, preprint, arXiv:2304.05376, DOI: 10.48550/ arXiv.2304.05376.\\n\\n15 K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero and B. Smit, Is GPT all you need for low-data discovery in chemistry?, chemrXiv, 2023, DOI: 10.26434/chemrxiv-2023-fw8n4-v2. 16 K. M. Jablonka, et al., 14 examples of how LLMs can transform materials science and chemistry: a re\\ue104ection on a large language model hackathon, Digital Discovery, 2023, 2, 1233–1250. 17 D. Weininger,'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='and a information system. 1. Introduction to methodology and encoding rules, J. Chem. Inf. Comput. Sci., 1988, 28, 31–36. 18 ¨O. H. Omar, T. Nematiaram, A. Troisi and D. Padula, Organic materials repurposing, a data set for theoretical predictions of new applications for existing compounds, Sci. Data, 2022, 9, 54.\\n\\nSMILES,\\n\\nchemical\\n\\nlanguage\\n\\n19 J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals and G. E. Dahl, Neural Message Passing for Quantum the 34th International Chemistry, Conference on Machine Learning, ed. D. Precup and Y. W. Teh, PMLR, 2017, vol. 70, pp. 1263–1272.\\n\\nin Proceedings\\n\\nof\\n\\n20 K. Yang, et al., Analyzing Learned Molecular Representations for Property Prediction, J. Chem. Inf. Model., 2019, 59, 3370– 3388.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/Chen2023.pdf'}, page_content='of\\n\\n20 K. Yang, et al., Analyzing Learned Molecular Representations for Property Prediction, J. Chem. Inf. Model., 2019, 59, 3370– 3388.\\n\\n21 X. Li, et al., Combining machine learning and high- throughput experimentation to discover photocatalytically active organic molecules, Chem. Sci., 2021, 12, 10742–10754. 22 E. J. Bjerrum, SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules, arXiv, 2017, preprint, arXiv:1703.07076, DOI: 10.48550/arXiv.1703.07076.\\n\\nChem. Sci.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='3 2 0 2\\n\\nl u J\\n\\n4 1\\n\\n]\\n\\nG L . s c [\\n\\n1 v 3 4 4 7 0 . 7 0 3 2 : v i X r a\\n\\nCan Large Language Models Empower Molecular Property Prediction?\\n\\nCaption\\n\\nGraph\\n\\na molecule\\n\\nc1cc(c(cc1N)[N+](=O)[O-])N\\n\\n…thepresenceofthenitrogroupsgivesthismolecule…thismoleculeisknownas2-amino-4-nitrophenolandisacommonintermediateusedinthesynthesisofotherorganiccompounds…\\n\\nSMILES\\n\\nChen Qian1, Huayi Tang1, Zhirui Yang1, Hong Liang2, Yong Liu1 ∗ 1 Renmin University of China 2 Peking University {qianchen2022,huayitang,yangzhirui,liuyonggsai}@ruc.edu.cn, lho@stu.pku.edu.cn\\n\\nAbstract'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Molecular property prediction has gained sig- nificant attention due to its transformative po- tential in multiple scientific disciplines. Con- ventionally, a molecule graph can be repre- sented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revo- lutionized the field of NLP. Although it is nat- ural to utilize LLMs to assist in understand- ing molecules represented by SMILES, the ex- ploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objec- tive through two perspectives: zero/few-shot molecular classification, and using the new ex- planations generated by LLMs as representa- tions of molecules. To be specific, we first prompt LLMs to do in-context molecular clas- sification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM model for multiple downstream tasks. The experimental results highlight the superiority of text explanations as molecular representations across multiple benchmark datasets, and con- firm the immense potential of LLMs in molec- ular property prediction tasks. Codes are avail- able at https://github.com/ChnQ/LLM4Mol.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Figure 1: Different representation paradigms for a molecule.\\n\\n2020; Wang et al., 2022). In the previous litera- ture, on one hand, molecules can be naturally repre- sented as graphs with atoms as nodes and chemical bonds as edges. Therefore, Graph Neural Networks (GNNs) can be employed to handle the molecular data (Kipf and Welling, 2017; Xu et al., 2019; Sun et al., 2019; Rong et al., 2020). Simultaneously, the other line of research explores the utilization of NLP-like techniques to process molecular data (Wang et al., 2019; Honda et al., 2019; Wang et al., 2022), since in many chemical databases (Irwin and Shoichet, 2005; Gaulton et al., 2017), molecular data is commonly stored as SMILES (Simplified Molecular-Input Line-Entry System) (Weininger, 1988) strings, a textual representation of molecular structure following strict rules.\\n\\n1\\n\\nIntroduction'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='1\\n\\nIntroduction\\n\\nAs a cutting-edge research topic at the intersection of artificial intelligence and chemistry, molecular property prediction has drawn increasing interest due to its transformative potential in multiple sci- entific disciplines such as virtual screening , drug design and discovery (Zheng et al., 2019; Maia et al., 2020; Gentile et al., 2022), to name a few. Based on this, the effective modeling of molecular data constitutes a crucial prerequisite for AI-driven molecular property prediction tasks (Rong et al.,\\n\\n∗ Corresponding author.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='∗ Corresponding author.\\n\\nIn recent years, the rapid development of LLMs have sparked a paradigm shift and opened up un- precedented opportunities in the field of NLP (Zhao et al., 2023; Zhou et al., 2023). Those models demonstrate tremendous potential in addressing various NLP tasks and show surprising abilities (i.e., emergent abilities (Wei et al., 2022a)). No- tably, ChatGPT (OpenAI, 2023) is the state-of-the- art AI conversational system developed by OpenAI in 2022, which possesses powerful text understand- ing capabilities and has been widely applied across various vertical domains.\\n\\nNote that, since molecules can be represented as SMILES sequences, it is natural and intuitive to employ LLMs with rich world knowledge to handle molecular data. For instance, as depicted\\n\\nYour answer should follow the following format.Functional groups: ...Chemical characteristics: ...……\\n\\n(1) C(Cl)(Cl)Cl(2) c1ccc(c(c1)C(=O)O)N……'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Your answer should follow the following format.Functional groups: ...Chemical characteristics: ...……\\n\\n(1) C(Cl)(Cl)Cl(2) c1ccc(c(c1)C(=O)O)N……\\n\\nFunctionalgroups:Halogen,…Chemicalcharacteristics:Itisnon-flammableandnon-corrosive.Carbontetrachlorideisarelativelystablecompound,butishighlytoxicandcancauseliverandkidneydamageifingestedorinhaled.Fine-tune onDownstream tasksZero/Few-Shot ClassificationCaption as new Representation\\n\\nChatGPT\\n\\nLMs\\n\\nInstrustionInput TextExamplesPrompt\\n\\n(1) C(Cl)(Cl)Cl, Category 0(2) c1ccc(c(c1)C(=O)O)N,Category 1……\\n\\nSuppose you are an expert in the interdisciplinary field of chemistry and AI. Given the SMILES representation of a series of molecules, your job is to ...\\n\\nRoBERTa\\n\\nFigure 2: Overview of LLM4Mol.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='RoBERTa\\n\\nFigure 2: Overview of LLM4Mol.\\n\\nin Figure 1, given the SMILES line of a molecule, ChatGPT can accurately describe the functional groups, chemical properties, and potential pharma- ceutical applications w.r.t. the given molecule. We believe that such textual descriptions are meaning- ful for assisting in molecular-related tasks.\\n\\nrameter updates (Liu et al., 2022a; Lu et al., 2022; Wu et al., 2022; Wei et al., 2022b). Therefore, we attempt to leverage the ICL capability of Chat- GPT to assist in molecular classification task by well-designed prompts, as shown in Figure 2. This paradigm makes it much easier to incorporate hu- man knowledge into LLMs by changing the demon- stration and templates.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='However, the application of LLMs in molecu- lar property prediction tasks is still in its primary stages. In this paper, we move towards this goal from two perspectives: zero/few-shot molecular classification task, and generating new explana- tions for molecules with original SMILES. Con- cretely, inspired by the astonishing in-context learn- ing capabilities (Brown et al., 2020) of LLMs, we first prompt ChatGPT to perform in-context molecular classification. Then, we propose a novel molecular representation called Captions as new Representation (CaR), which leverages ChatGPT to generate informative and professional textual analyses for SMILES. Then the textual explanation can serving as new representation for molecules, as illustrated in Figure 1. Comprehensive experi- mental results highlight the remarkable capabilities and tremendous potential of LLMs in molecular property prediction tasks. We hope this work could shed new insights in model design of molecular property prediction'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='potential of LLMs in molecular property prediction tasks. We hope this work could shed new insights in model design of molecular property prediction tasks enpowered by LLMs.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Captions as New Representations. With vivid world knowledge and amazing reasoning ability, LLMs have been widely applied in various AI do- mains (He et al., 2023; Liu et al., 2023). Also, we reckon that LLMs can empower LLMs can greatly contribute to the understanding of molecular prop- erties. Taking a commonly used dataset in the field of molecular prediction for a toy example, PTC (Helma et al., 2001) is a collection of chemical molecules that reports their carcinogenicity in ro- dents. We conduct a keyword search using terms such as ‘toxicity’ ‘cancer’, and ‘harmful’ to re- trieve all explanations generated by ChatGPT for the originally SMILES-format PTC dataset. Inter- estingly, we observed that the majority of these key- words predominantly appeared in entries labeled as -1. This demonstrates that ChatGPT is capable of providing meaningful and distinctive professional explanations for the raw SMILES strings, thereby benefiting downstream tasks.\\n\\n2 Method'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='2 Method\\n\\nIn this section, we will elaborate on our prelimi- nary exploration of how LLMs can serve molecular property prediction tasks. Zero/Few-shot Classification. With the continu- ous advancement of LLMs, In-Context Learning (ICL) (Brown et al., 2020) has emerged as a new paradigm for NLP. Using a demonstration context that includes several examples written in natural language templates as input, LLMs can make pre- dictions for unseen input without additional pa-\\n\\nTowards this end, we propose to leverage Chat- GPT to understand the raw SMILES strings and generate textual descriptions that encompass var- ious aspects such as functional groups, chemical properties, pharmaceutical applications, and be- yond. Then, we fine-tune a pre-trained small-scale LM (e.g., RoBERTa (Liu et al., 2020)) on various downstream tasks, such as molecular classification and properties prediction.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Table 1: Testing evaluation results on several benchmark datasets with Random Splitting. For classification task reporting ACC and ROC-AUC (%, mean ± std), for regression tasks reporting RMSE (mean ± std). ↑ for higher is better, ↓ contrarily. ‡ denotes the results cited from origin paper. CoR with superior result is highlighted.\\n\\nMethod\\n\\nMUTAG\\n\\nACC ↑ PTC\\n\\nAIDS\\n\\nROC-AUC ↑\\n\\nSider\\n\\nClinTox\\n\\nEsol\\n\\nRMSE ↓\\n\\nLipo\\n\\nGCN\\n\\n90.00 ± 4.97\\n\\n62.57 ± 4.13\\n\\n78.68 ± 3.36 64.24 ± 5.61\\n\\n91.88 ± 1.45\\n\\n0.77 ± 0.05\\n\\n0.80 ± 0.04\\n\\ns N N G\\n\\nGIN\\n\\nChebyNet D-MPNN‡\\n\\n89.47 ± 4.71\\n\\n64.21 ± 5.16\\n\\n\\n\\n58.29 ± 5.88\\n\\n61.43 ± 4.29\\n\\n\\n\\n78.01 ± 1.77 66.19 ± 5.10\\n\\n79.74 ± 1.78 80.68 ± 5.10\\n\\n\\n\\n66.40 ± 2.10\\n\\n92.08 ± 1.11\\n\\n91.48 ± 1.50\\n\\n90.60 ± 4.30\\n\\n0.67 ± 0.04\\n\\n0.75 ± 0.04\\n\\n0.58 ± 0.05\\n\\n0.79 ± 0.03\\n\\n0.85 ± 0.04\\n\\n0.55 ± 0.07\\n\\ns ECFP4-MLP E L I M S\\n\\nSMILES-Transformer‡ MolR‡\\n\\n96.84 ± 3.49\\n\\n\\n\\n\\n\\n85.71 ± 7.67\\n\\n\\n\\n\\n\\n94.64 ± 3.14 90.19 ± 4.88\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n95.81 ± 2.09\\n\\n95.40\\n\\n91.60 ± 3.90\\n\\n0.60 ± 0.11\\n\\n0.72\\n\\n\\n\\n0.60 ± 0.16\\n\\n0.92'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='96.84 ± 3.49\\n\\n\\n\\n\\n\\n85.71 ± 7.67\\n\\n\\n\\n\\n\\n94.64 ± 3.14 90.19 ± 4.88\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n95.81 ± 2.09\\n\\n95.40\\n\\n91.60 ± 3.90\\n\\n0.60 ± 0.11\\n\\n0.72\\n\\n\\n\\n0.60 ± 0.16\\n\\n0.92\\n\\n\\n\\nM CaRRoberta ∆GN N s ∆N LP\\n\\nL L\\n\\n91.05 ± 3.37 93.14 ± 3.43 94.37 ± 1.19 88.81 ± 2.65 99.80 ± 0.43 0.45 ± 0.04 0.47 ± 0.03 +30% −2%\\n\\n+12% −6%\\n\\n+53% +9%\\n\\n+20% +0%\\n\\n+9% +6%\\n\\n−35% −32%\\n\\n−37% −38%\\n\\n0.3\\n\\n0.3\\n\\n0.1\\n\\n0.9\\n\\nMUTAG\\n\\n0.9\\n\\n0.0\\n\\n0.5\\n\\n235Shots\\n\\n1.0ACC\\n\\n1.0ACC\\n\\n0.1\\n\\n0.8\\n\\n0.8\\n\\nChatGPT\\n\\n0.4\\n\\nGCN\\n\\n0.6\\n\\nPTC\\n\\n0.4\\n\\n0.6\\n\\n235Shots\\n\\n0.5\\n\\n0.0\\n\\n0.7\\n\\n0.7\\n\\n0.2\\n\\n0.2\\n\\nECFP\\n\\nGIN\\n\\n95\\n\\n85\\n\\n100Accuracy\\n\\n90\\n\\nCLINTOX\\n\\ndeberta\\n\\nPTC\\n\\ndeberta(de novo)\\n\\nroberta\\n\\nmolecules\\n\\n80\\n\\nMUTAG\\n\\nFigure 3: Few-shot classification results on MUTAG and PTC by classical models and ChatGPT.\\n\\nFigure 4: Performance of CaR by replacing Small LMs.\\n\\n3 Experiments\\n\\n3.1 Setup'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Datasets. To comprehensively evaluate the per- formance of CaR, we conduct experiments on 9 datasets spanning molecular classification tasks and molecular regression tasks. i) 3 classifica- tion datasets from TUDataset (Morris et al., 2020): MUTAG, PTC, AIDS. ii) 4 classification datasets from MoleculeNet (Wu et al., 2018): Sider, Clin- Tox, Bace, BBBP. iii) 2 regression datasets from MoleculeNet: Esol, Lipophilicity. Baselines. We compare CaR with the following baselines: i) GNN-based methods, GCN (Kipf and Welling, 2017), GIN (Xu et al., 2019), ChebyNet (Defferrard et al., 2016), D-MPNN (Yang et al., 2019), GraphMVP (Liu et al., 2022b), InfoGraph (Sun et al., 2019), G-Motif (Rong et al., 2020), Mole-BERT (Xia et al., 2023). ii) SMILES- based methods, ECFP (Rogers and Hahn, 2010), SMILES-Transfor (Honda et al., 2019), MolR (Wang et al., 2022), ChemBERTa (Chithrananda et al., 2020), MolKD (Zeng et al., 2023). Settings. For all datasets, we perform a 8/1/1 splitting for'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='(Wang et al., 2022), ChemBERTa (Chithrananda et al., 2020), MolKD (Zeng et al., 2023). Settings. For all datasets, we perform a 8/1/1 splitting for train/validate/test, where the best av- erage performance (and standard variance) on the'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='test fold is reported. Specially, we perform a 10- fold cross-validation (CV) with a holdout fixed test for random split datasets; conduct experiments for scaffold splitting datasets with 5 random seeds. Small-scale LMs are implemented using the Hug- ging Face transformers library (Wolf et al., 2020) with default parameters.\\n\\n3.2 Main Results'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='3.2 Main Results\\n\\nHow does ChatGPT perform on zero/few-shot molecular classification? Figure 3 illustrates the few-shot learning capabilities of ChatGPT, tradi- tional GNNs, and ECFP on two datasets. It is observed that ChatGPT underperforms compared to traditional methods for MUTAG, whereas con- versely for PTC. Furthermore, see Figure 6, as the number of shots increases, ChatGPT demonstrates an upward trend in performance for both datasets. These results indicate that ChatGPT possesses a certain level of few-shot molecular classification capability. However, throughout the experiments, we find that ChatGPT’s classification performance was not consistent for the same prompt, and differ- ent prompts also have a significant impact on the results. Therefore, it is crucial to design effective prompts that incorporate rational prior information'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Table 2: Testing evaluation results of different methods on benchmark datasets with Scaffold Splitting. The remaining settings keep consistent with Table 1.\\n\\nMethod\\n\\nSider\\n\\nROC-AUC ↑\\n\\nClinTox\\n\\nBace\\n\\nBBBP\\n\\nEsol\\n\\nRMSE ↓\\n\\nLipo\\n\\nGCN\\n\\n55.81 ± 2.92\\n\\n50.32 ± 2.46\\n\\n76.78 ± 4.74\\n\\n71.90 ± 5.35\\n\\n1.09 ± 0.11\\n\\n0.88 ± 0.03\\n\\nGIN\\n\\n58.86 ± 2.57\\n\\n51.79 ± 5.18\\n\\n77.05 ± 5.68\\n\\n75.30 ± 4.66\\n\\n1.26 ± 0.49\\n\\n0.88 ± 0.02\\n\\ns N N G\\n\\nChebyNet InfoGraph‡ G-Motif‡ GraphMVP-C‡ Mole-BERT‡\\n\\n60.87 ± 1.68\\n\\n59.20 ± 0.20\\n\\n60.60 ± 1.10\\n\\n63.90 ± 1.20\\n\\n62.80 ± 1.10\\n\\n52.92 ± 9.36\\n\\n75.10 ± 5.00\\n\\n77.80 ± 2.00\\n\\n77.50 ± 4.20\\n\\n78.90 ± 3.00\\n\\n77.31 ± 3.55\\n\\n73.90 ± 2.50\\n\\n73.40 ± 4.00\\n\\n81.20 ± 0.90\\n\\n80.80 ± 1.40\\n\\n73.89 ± 4.95\\n\\n69.20 ± 0.80\\n\\n66.40 ± 3.40\\n\\n72.40 ± 1.60\\n\\n71.90 ± 1.60\\n\\n1.09 ± 0.08\\n\\n\\n\\n\\n\\n1.03\\n\\n1.02 ± 0.03\\n\\n0.89 ± 0.04\\n\\n\\n\\n\\n\\n0.68\\n\\n0.68 ± 0.02\\n\\ns ECFP4-MLP E L ChemBERTa‡ I M MolKD‡ S\\n\\n64.86 ± 3.45\\n\\n\\n\\n61.30 ± 1.20\\n\\n52.93 ± 5.92\\n\\n73.30\\n\\n83.80 ± 3.10\\n\\n81.58 ± 4.02\\n\\n\\n\\n80.10 ± 0.80\\n\\n73.37 ± 6.05\\n\\n64.30\\n\\n74.80 ± 2.30\\n\\n1.77 ± 0.25'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='64.86 ± 3.45\\n\\n\\n\\n61.30 ± 1.20\\n\\n52.93 ± 5.92\\n\\n73.30\\n\\n83.80 ± 3.10\\n\\n81.58 ± 4.02\\n\\n\\n\\n80.10 ± 0.80\\n\\n73.37 ± 6.05\\n\\n64.30\\n\\n74.80 ± 2.30\\n\\n1.77 ± 0.25\\n\\n\\n\\n\\n\\n1.03 ± 0.04\\n\\n\\n\\n\\n\\nM CaRRoberta ∆GN N s ∆N LP\\n\\nL L\\n\\n58.06 ± 1.80 −3% −9%\\n\\n84.16 ± 17.63 +30% +22%\\n\\n80.73 ± 1.42 +5% −1%\\n\\n81.99 ± 4.19 +15% +19%\\n\\n0.96 ± 0.09 −13% −46%\\n\\n1.02 ± 0.06 +27% −1%\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1c\\x00\\x15\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1c\\x00\\x19\\n\\n\\x00\\x13\\n\\n\\x007\\x00U\\x00D\\x00L\\x00Q\\x00L\\x00Q\\x00J\\x00\\x03\\x00F\\x00X\\x00U\\x00Y\\x00H\\x00V\\x00\\x03\\x00R\\x00Q\\x00\\x03\\x00F\\x00O\\x00L\\x00Q\\x00W\\x00R\\x00[\\n\\n\\x00\\x1b\\x00\\x13\\x00\\x13\\x006\\x00W\\x00H\\x00S\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1c\\x00\\x1b\\n\\n\\x00\\x14\\x00\\x11\\x00\\x13\\x00\\x13\\x00$\\x00&\\x00&\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x1b\\x00/\\x00R\\x00V\\x00V\\n\\n\\x00\\x13\\x00\\x11\\x00\\x17\\x00\\x19\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x15\\n\\n\\x00\\x13\\x00\\x11\\x00\\x17\\x00\\x1b\\n\\n\\x00$\\x00F\\x00F\\n\\n\\x00\\x13\\x00\\x11\\x00\\x17\\x00\\x17\\n\\n\\x00/\\x00R\\x00V\\x00V\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1c\\x00\\x17\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x19\\n\\n\\x00\\x17\\x00\\x13\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x17\\n\\n\\x00\\x19\\x00\\x13\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x13\\n\\n\\x00\\x15\\x00\\x13\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x17\\x00\\x15\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x13\\n\\n\\x00$\\x00F\\x00F\\n\\n\\x007\\x00U\\x00D\\x00L\\x00Q\\x00L\\x00Q\\x00J\\x00\\x03\\x00F\\x00X\\x00U\\x00Y\\x00H\\x00V\\x00\\x03\\x00R\\x00Q\\x00\\x03\\x00E\\x00D\\x00F\\x00H\\n\\n\\x00\\x14\\x00\\x13\\x00\\x13\\n\\n\\x00\\x17\\x00\\x13\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x19\\n\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x18\\n\\n\\x00\\x13\\x00\\x11\\x00\\x17\\x00\\x18\\n\\n\\x00\\x13\\n\\n\\x00\\x1a\\x00\\x13\\x00\\x13\\x006\\x00W\\x00H\\x00S\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\n\\n\\x00\\x15\\x00\\x13\\x00\\x13\\n\\n\\x00\\x16\\x00\\x13\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\n\\n\\x00\\x18\\x00\\x13\\x00\\x13\\n\\n\\x00/\\x00R\\x00V\\x00V\\n\\n\\x00\\x14\\x00\\x11\\x00\\x13\\x00$\\x00&\\x00&\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x13\\x00/\\x00R\\x00V\\x00V\\n\\n\\x00\\x13\\x00\\x11\\x00\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1b\\n\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x18\\n\\n\\x00\\x19\\x00\\x13\\x00\\x13\\n\\n\\x00\\x19\\x00\\x13\\x00\\x13\\n\\n\\x00\\x17\\x00\\x13\\x00\\x13\\n\\n\\x00\\x1b\\x00\\x13\\x00\\x13\\x006\\x00W\\x00H\\x00S\\n\\n\\x00\\x13\\x00\\x11\\x00\\n\\n\\x00\\x13\\n\\n\\x00\\x15\\x00\\x13\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x16\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x17\\x00\\x18\\n\\n\\x00\\x13\\x00\\x11\\x00\\x19\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\n\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x13\\n\\n\\x00$\\x00F\\x00F\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x18'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='\\x00\\x13\\x00\\x11\\x00\\x1b\\n\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x18\\n\\n\\x00\\x19\\x00\\x13\\x00\\x13\\n\\n\\x00\\x19\\x00\\x13\\x00\\x13\\n\\n\\x00\\x17\\x00\\x13\\x00\\x13\\n\\n\\x00\\x1b\\x00\\x13\\x00\\x13\\x006\\x00W\\x00H\\x00S\\n\\n\\x00\\x13\\x00\\x11\\x00\\n\\n\\x00\\x13\\n\\n\\x00\\x15\\x00\\x13\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x16\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x17\\x00\\x18\\n\\n\\x00\\x13\\x00\\x11\\x00\\x19\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\n\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x13\\n\\n\\x00$\\x00F\\x00F\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x18\\n\\n\\x007\\x00U\\x00D\\x00L\\x00Q\\x00L\\x00Q\\x00J\\x00\\x03\\x00F\\x00X\\x00U\\x00Y\\x00H\\x00V\\x00\\x03\\x00R\\x00Q\\x00\\x03\\x00E\\x00E\\x00E\\x00S\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x18\\n\\n\\x00\\x13\\x00\\x11\\x00\\x17\\n\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x18\\n\\n\\x00/\\x00R\\x00V\\x00V\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x13\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1b\\n\\n\\x00\\x13\\x00\\x11\\x00\\x18\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1b\\x00\\x13\\x00/\\x00R\\x00V\\x00V\\n\\n\\x00\\x14\\x00\\x11\\x00\\x13\\x00$\\x00&\\x00&\\n\\nFigure 5: The loss value (Loss) and accuracy value (ACC) during training process.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='to achieve better zero/few-shot classification. How does CaR perform compared with existing methods on common benchmarks? The main results for comparing the performance of different methods on several benchmark datasets are shown in Table 1 and Table 2. From the tables, we obtain the following observation: i) Under the random split setting, CaR achieves superior results on al- most all datasets, whether in classification or re- gression tasks. Remarkably, CaR exhibits a signifi- cant performance improvement of 53% compared to traditional methods on the PTC dataset. ii) For Scaffold splitting, one can observe that compared to other models, LLM demonstrates comparable results on Sider and Bace with slightly less supe- rior; in the Lipo regression task, CaR falls short compared to GNNs; However, CoR achieves no- table performance improvements on the remaining datasets. These observations indicate LLMs’ ef- fectiveness and potential in enhancing molecular predictions across various'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='on the remaining datasets. These observations indicate LLMs’ ef- fectiveness and potential in enhancing molecular predictions across various domains. Convergence Analysis. In Figure 5, we plot the ROC-AUC and loss curves on three datasets to verify CaR’s convergence. One can observe that the'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='loss value decreases rapidly in the first several steps and then continuously decrease in a fluctuation way until convergence. Also, the ROC-AUC curve exhibits an inverse and corresponding trend. These results demonstrate the convergence of CaR. Replace Small-scale LMs. To validate the effec- tiveness of CaR, we further fine-tune two addi- tional pre-trained LMs (DeBERTa (He et al., 2021), adaptive-lm-molecules (Blanchard et al., 2023)) and also train a non-pretrained DeBERTa from scratch. The results are plotted in Figure 4. One can observe that different pre-trained LMs exhibit similar performance, and generally outperform the LM trained from scratch, which validate the effec- tiveness of CaR.\\n\\n4 Conclusion'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='4 Conclusion\\n\\nIn this work, we explore how LLMs can contribute to molecular property prediction from two perspec- tives, in-context classification and generating new representation for molecules. This preliminary at- tempt highlights the immense potential of LLM in handling molecular data. In future work, we\\n\\nattempt to focus on more complex molecular down- stream tasks, such as generation tasks and 3D anti- body binding tasks.\\n\\nLimitations'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Lack of Diverse LLMs. In this work, we pri- marily utilized ChatGPT as a representative of LLMs. However, the performance of other LLMs on molecular data has yet to be explored, such as the more powerful GPT-4 (OpenAI, 2023) or domain-specific models like MolReGPT (Li et al., 2023). Insufficient Mining of Graph Structures. While we currently model molecular prediction tasks solely as NLP tasks, we acknowledge the cru- cial importance of the graph structure inherent in molecules for predicting molecular properties. How to further enhance the performance of our framework by mining graph structured information is worth exploring. Beyond SMILES. In this work, we focus on small molecule data that can be represented as SMILES strings. However, in practical biochemistry do- mains, there is a wide range of data, such as proteins, antibodies, and other large molecules, that cannot be represented using SMILES strings. Therefore, the design of reasonable sequential rep- resentations for the'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='and other large molecules, that cannot be represented using SMILES strings. Therefore, the design of reasonable sequential rep- resentations for the large molecules with 3D struc- ture to LLMs of is an important and urgent research direction to be addressed.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='References\\n\\nAndrew E Blanchard, Debsindhu Bhowmik, Zachary Fox, John Gounley, Jens Glaser, Belinda S Akpa, and Stephan Irle. 2023. Adaptive language model training for molecular design. Journal of Cheminfor- matics, 15(1):1–12.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901.\\n\\nSeyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2020. Chemberta: Large-scale self- supervised pretraining for molecular property pre- diction. arXiv preprint arXiv:2010.09885.\\n\\nMichaël Defferrard, Xavier Bresson, and Pierre Van- dergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. Ad- vances in Neural Information Processing Systems, 29:3837–3845.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Anna Gaulton, Anne Hersey, Michał Nowotka, A Patri- cia Bento, Jon Chambers, David Mendez, Prudence Mutowo, Francis Atkinson, Louisa J Bellis, Elena Cibrián-Uhalte, et al. 2017. The chembl database in 2017. Nucleic acids research, 45(D1):D945–D954.\\n\\nFrancesco Gentile, Jean Charle Yaacoub, James Gleave, Michael Fernandez, Anh-Tien Ton, Fuqiang Ban, Abraham Stern, and Artem Cherkasov. 2022. Ar- tificial intelligence–enabled virtual screening of ultra- large chemical libraries with deep docking. Nature Protocols, 17(3):672–697.\\n\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced In International bert with disentangled attention. Conference on Learning Representations.\\n\\nXiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi. 2023. Explanations as features: Llm- arXiv based features for text-attributed graphs. preprint arXiv:2305.19523.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Christoph Helma, Ross D. King, Stefan Kramer, and Ashwin Srinivasan. 2001. The predictive toxicology challenge 2000–2001. Bioinformatics, 17(1):107– 108.\\n\\nShion Honda, Shoi Shi, and Hiroki R Ueda. 2019. Smiles transformer: Pre-trained molecular finger- print for low data drug discovery. arXiv preprint arXiv:1911.04738.\\n\\nJohn J Irwin and Brian K Shoichet. 2005. Zinc- a free database of commercially available compounds for virtual screening. Journal of chemical information and modeling, 45(1):177–182.\\n\\nThomas N. Kipf and Max Welling. 2017.\\n\\nSemi- supervised classification with graph convolutional networks. In International Conference on Learning Representations.\\n\\nJiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and Qing Li. 2023. Empower- ing molecule discovery for molecule-caption transla- tion with large language models: A chatgpt perspec- tive. arXiv preprint arXiv:2306.06615.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022a. What makes good in-context examples for gpt-3? In Pro- ceedings of Deep Learning Inside Out: The 3rd Work- shop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100–114.\\n\\nJunling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is chatgpt a good recommender? a preliminary study. arXiv preprint arXiv:2304.10149.\\n\\nShengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. 2022b. Pre- training molecular graph representation with 3d ge- ometry. In International Conference on Learning Representations.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Ro{bert}a: A robustly optimized {bert} pretraining approach.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few- shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics, pages 8086–8098.\\n\\nEduardo Habib Bechelane Maia, Letícia Cristina Assis, Tiago Alves De Oliveira, Alisson Marques Da Silva, and Alex Gutterres Taranto. 2020. Structure-based virtual screening: from classical to artificial intelli- gence. Frontiers in chemistry, 8:343.\\n\\nChristopher Morris, Nils M. Kriege, Franka Bause, Kris- tian Kersting, Petra Mutzel, and Marion Neumann. 2020. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond.\\n\\nOpenAI. 2023.\\n\\nGpt-4 technical report.\\n\\nArXiv,\\n\\nabs/2303.08774.\\n\\nDavid Rogers and Mathew Hahn. 2010. Extended- connectivity fingerprints. Journal of chemical in- formation and modeling, 50(5):742–754.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='David Rogers and Mathew Hahn. 2010. Extended- connectivity fingerprints. Journal of chemical in- formation and modeling, 50(5):742–754.\\n\\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. 2020. Self-supervised graph transformer on large- scale molecular data. Advances in Neural Informa- tion Processing Systems, 33:12559–12571.\\n\\nFan-Yun Sun, Jordan Hoffman, Vikas Verma, and Jian Infograph: Unsupervised and semi- Tang. 2019. supervised graph-level representation learning via mutual information maximization. In International Conference on Learning Representations.\\n\\nHongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, and Martin D. Burke. 2022. Chemical-reaction-aware molecule representation learning. In International Conference on Learning Representations.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. 2019. Smiles-bert: large scale unsupervised pre-training for molecular property pre- diction. In Proceedings of the 10th ACM interna- tional conference on bioinformatics, computational biology and health informatics, pages 429–436.\\n\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.\\n\\nDavid Weininger. 1988. Smiles, a chemical language and information system. 1. introduction to methodol- ogy and encoding rules. Journal of chemical infor- mation and computer sciences, 28(1):31–36.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Em- pirical Methods in Natural Language Processing: System Demonstrations, pages 38–45.\\n\\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. Moleculenet: a benchmark for molecular machine learning. Chem- ical science, 9(2):513–530.\\n\\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling- peng Kong. 2022. Self-adaptive in-context learning. arXiv preprint arXiv:2212.10375.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling- peng Kong. 2022. Self-adaptive in-context learning. arXiv preprint arXiv:2212.10375.\\n\\nJun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. 2023. Mole-BERT: Rethinking pre-training graph neural networks for molecules. In The Eleventh In- ternational Conference on Learning Representations.\\n\\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural net- works? In International Conference on Learning Representations.\\n\\nKevin Yang, Kyle Swanson, Wengong Jin, Connor Co- ley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. 2019. Analyzing learned molecular representations for property prediction. Journal of chemical informa- tion and modeling, 59(8):3370–3388.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='Liang Zeng, Lanqing Li, and Jian Li. 2023. Molkd: Dis- tilling cross-modal knowledge in chemical reactions for molecular property prediction. arXiv preprint arXiv:2305.01912.\\n\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.\\n\\nLiangzhen Zheng, Jingrong Fan, and Yuguang Mu. 2019. Onionnet: a multiple-layer intermolecular-contact- based convolutional neural network for protein– ligand binding affinity prediction. ACS omega, 4(14):15956–15965.\\n\\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419.\\n\\nA N-shot Results\\n\\n8\\n\\n0.65\\n\\n4\\n\\nMutag\\n\\n0\\n\\n0.70\\n\\n0.60\\n\\n0.55\\n\\n0.75\\n\\n0.80acc\\n\\n2\\n\\nPTC\\n\\n6\\n\\n10#shots\\n\\nFew-shot classification on Mutag and PTC'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/2307.07443.pdf'}, page_content='A N-shot Results\\n\\n8\\n\\n0.65\\n\\n4\\n\\nMutag\\n\\n0\\n\\n0.70\\n\\n0.60\\n\\n0.55\\n\\n0.75\\n\\n0.80acc\\n\\n2\\n\\nPTC\\n\\n6\\n\\n10#shots\\n\\nFew-shot classification on Mutag and PTC\\n\\nFigure 6: The impact of #Shots on Few-shot classifica- tion on MUTAG and PTC by ChatGPT.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"Open Access Article. Published on 22 June 2018. Downloaded on 4/3/2023 5:18:49 PM.\\n\\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nChemical Science\\n\\nEDGE ARTICLE\\n\\nView Article Online\\n\\nView Journal\\n\\n| View Issue\\n\\nCite this: Chem. Sci., 2018, 9, 6091\\n\\n“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models†\\n\\nPhilippe Schwaller, and Teodoro Laino\\n\\n‡* Th´eophile Gaudin,‡ D´avid L´anyi, Costas Bekas\\n\\nThere is an intuitive analogy of an organic chemist's understanding of a compound and a language speaker's\\n\\nunderstanding of a word. Based on this analogy, it is possible to introduce the basic concepts and analyze\\n\\npotential impacts of linguistic analysis to the world of organic chemistry. In this work, we cast the reaction prediction task as a translation problem by introducing a template-free sequence-to-sequence model,\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='trained end-to-end and fully data-driven. We propose a tokenization, which is arbitrarily extensible with\\n\\nReceived 28th May 2018 Accepted 20th June 2018\\n\\nreaction information. Using an attention-based model borrowed from human language translation, we\\n\\nimprove the state-of-the-art solutions in reaction prediction on the top-1 accuracy by achieving 80.3%\\n\\nDOI: 10.1039/c8sc02339e\\n\\nwithout relying on auxiliary knowledge, such as reaction templates or explicit atomic features. Also,\\n\\nrsc.li/chemical-science\\n\\na top-1 accuracy of 65.4% is reached on a larger and noisier dataset.\\n\\n1\\n\\nIntroduction\\n\\nA\\ue09der nearly 200 years of documented research, the synthesis of organic molecules remains one of the most important tasks in organic chemistry. The construction of a target molecule from a set of existing reactants and reagents via chemical reactions is attracting much attention because of economical implications.\\n\\nits'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"Multiple eﬀorts have been made in the past 50 years to rationalize the large number of chemical compounds and reactions identi\\ue103ed, which form the large knowledge bases for In 1969, Corey and Wipke1 solving synthetic problems. demonstrated that both synthesis and retrosynthesis could be performed by a machine. Their pioneering contribution involved the use of handcra\\ue09ded rules made by experts, which are commonly known as reaction templates. The templates encode the local changes to the atoms' connectivity under certain conditions accounting for various subtleties of retro- synthesis. A similar algorithm emerged in the late 1970s2 which also requires a set of expert rules. Unfortunately, rules writing is a tedious task, both time and labor-intensive, and may not cover the entire domain for complex organic chemistry problems. In such cases, profound chemical expertise is still required, and the solutions are usually developed by trained organic chemists. However, it can be extremely\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='profound chemical expertise is still required, and the solutions are usually developed by trained organic chemists. However, it can be extremely challenging even for them to'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"IBM Research, Zurich, Switzerland. E-mail: {phs,tga,dla,bek,teo}@zurich.ibm.com\\n\\n† Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, con\\ue103dence and token probabilities. See DOI: 10.1039/c8sc02339e\\n\\nsynthesize a relatively complex molecule, which may take several reaction steps to construct. In fact, navigating the chemical space of drug-related compounds by relying only on intuition may turn a synthesis into a nearly impossible task, especially if the problem is slightly outside the expert's knowledge.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='Other approaches extract reaction templates directly from data.3–6 In this speci\\ue103c context, candidate products are gener- ated from the templates and then are ranked according to their likelihood. Satoh and Funatsu3,4 used various hard-coded criterion to perform the ranking whereas more recent approaches5,6 used a deep neural network. However, these types of approaches are fundamentally dependent on the rule-based system component and thus inherit some of its major limita- tions. In particular, these approaches do not produce suﬃ- ciently accurate predictions outside of the training domain.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='Nevertheless, the class of algorithms1–6 that is based on rules manually encoded by human experts or automatically derived from a reaction database is not the only way to approach the problem of organic synthesis. A second approach for predicting chemical reactions exploits the advancements in computational chemistry to evaluate the energy barriers of a reaction, based on \\ue103rst-principle calculations.7–9 Although it is possible to reach very accurate levels of predictions for small systems (chemical reactions involving few hundred atoms), it is still a very computationally daunting task which limits, among other things, the sampling of the solvent degrees of freedom, possibly resulting in unrealistic entropy contributions. Therefore, while computational chemistry may intrinsically solve the problem of reaction prediction, the systematic treatment of all those degrees of freedom that may\\n\\nits prohibitive cost does prevent\\n\\n‡ P. S. and T. G. contributed equally to this work.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='its prohibitive cost does prevent\\n\\n‡ P. S. and T. G. contributed equally to this work.\\n\\nThis journal is © The Royal Society of Chemistry 2018\\n\\nChem. Sci., 2018, 9, 6091–6098 | 6091\\n\\nOpen Access Article. Published on 22 June 2018. Downloaded on 4/3/2023 5:18:49 PM.\\n\\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nChemical Science'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"drive the chemical reaction along a speci\\ue103c route. For such reasons, its current \\ue103eld of applicability in industry is mainly limited to problems that may have a purely academic interest. One way to view the reaction prediction task is to cast it as a translation problem, where the objective is to map a text sequence that represents the reactants to a text sequence rep- resenting the product. Molecules can equivalently be expressed as text sequences in line notation format, such as the simpli\\ue103ed molecular-input line-entry system (SMILES).10 Intuitively, there is an analogy between a chemist's understanding of a compound and a language speaker's understanding of a word. No matter how imaginative such an analogy is, it was only very recently that a formal veri\\ue103cation was proved.11 Cadeddu et al.11 showed that organic molecules contain fragments whose rank distribution is essentially identical to that of sentence fragments. Moreover, it has already been shown that a text representation of\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='whose rank distribution is essentially identical to that of sentence fragments. Moreover, it has already been shown that a text representation of molecules has been eﬀective in chemoinformatics.12–16 This has strength- ened our belief that the methods of computational linguistics can have an immense impact on the analysis of organic mole- cules and reactions.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='In this work, we build on the idea of relating organic chemistry to a language and explore the application of state-of- the-art neural machine translation methods, which are sequence-to-sequence (seq2seq) models. We intend to solve the forward-reaction prediction problem, where the starting mate- rials are known and the interest is in generating the products. This approach was \\ue103rst pioneered by Nam and Kim.17 Here, we propose a model with higher capacity and a diﬀerent attention mechanism, which better captures the relation between reac- tants and products. For the tokenization, we combine an atom- wise tokenization for the reactants similar to the work of Nam and Kim17 with a one-hot reagent tokenization suggested by Schneider et al.18 Given that training data for reaction condition were available, the tokenization would be arbitrarily extensible with tokens describing those conditions. In this work, we only use a set of the most common reagents.19 The overall network architecture is'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='with tokens describing those conditions. In this work, we only use a set of the most common reagents.19 The overall network architecture is simple, and the model is trained end-to-end, fully data-driven and without additional external information. With this approach, we improved the top-1 accuracy by 0.7% compared to current template-free solutions, achieving a value of 80.3% using their own training and test data sets.20 The model presented set also a \\ue103rst score of 65.4% on a noisy single product reactions dataset extracted from US patents.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='2 Related work 2.1 Template-based reaction prediction\\n\\nTemplate-based reaction prediction methods have been widely researched in the past couple of years.5,6,21 Wei et al.21 used a graph-convolution neural network proposed by Duvenaud et al.22 to infer \\ue103ngerprints of the reactants and reagents. They trained a network on the \\ue103ngerprints to predict which reaction templates to apply to the reactants. Segler and Waller5 built a knowledge graph using reaction templates and discovered novel reactions by searching for missing nodes in the graph. Coley et al.6 generated for a given set of reactants all possible product candidates from a set of reaction templates extracted\\n\\n6092 | Chem. Sci., 2018, 9, 6091–6098\\n\\nView Article Online\\n\\nEdge Article'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='6092 | Chem. Sci., 2018, 9, 6091–6098\\n\\nView Article Online\\n\\nEdge Article\\n\\nfrom US patents23 and predicted the outcome of the reaction by ranking the candidates with a neural network. One major advancement by Segler and Waller5 and Coley et al.6 was to consider alternative products as negative examples. Recently, Segler and Waller24 introduced a neural-symbolic approach. They extracted reaction rules from the commercially available Reaxys database. Then, they trained a neural network on molecular \\ue103ngerprints to prioritize templates. The reaction products were generated using the top-ranked templates. In any case, template-based methods have the limitation that they cannot predict anything outside the space covered by the previously extracted templates.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='2.2 Template-free reaction prediction While template-free approaches existed for decades,25–28 a \\ue103rst rule-free approach was introduced by Kayala et al.29 Using \\ue103ngerprints and hand-cra\\ue09ded features, they predicted a series of mechanistic steps to obtain one reaction outcome. Owing to the sparsity of data on such mechanistic reaction steps, the dataset was self-generated with a template-based expert system. Recently, Jin et al.20 used a novel approach based on Weisfeiler– Lehman Networks (WLN). They trained two independent networks on a set of 400 000 reactions extracted from US patents. The \\ue103rst WLN scored the reactivity between atom pairs and predicted the reaction center. All possible bond con\\ue103gu- ration changes were enumerated to generate product candi- dates. The candidates that were not removed by hard-coded valence and connectivity rules are then ranked by a Weisfeiler– Lehman Diﬀerence Network (WLDN). Their method achieved a top-1 accuracy of 79.6% on a test set of 40 000'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='rules are then ranked by a Weisfeiler– Lehman Diﬀerence Network (WLDN). Their method achieved a top-1 accuracy of 79.6% on a test set of 40 000 reactions. Jin et al.20 claimed to outperform template-based approaches by a margin of 10% a\\ue09der augmenting the model with the unknown products of the initial prediction to have a product coverage of 100% on the test set. The dataset with the exact training, vali- dation and test split have been released.§ The complexity of the reaction prediction problem was signi\\ue103cantly reduced by removing the stereochemical information.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"2.3 Seq2seq models in organic reaction prediction and retrosynthesis\\n\\nThe closest work to ours is that of Nam and Kim,17 who also used a template-free seq2seq model reaction outcomes. Whereas their network was trained end-to-end on patent data and self-generated reaction examples, they limited their predictions to textbook reactions. Their model was based on the Tensor\\ue104ow translate model (v0.10.0),30 from which they took the default values for most of the hyperparameters. Compared to Nam and Kim,17 our model uses Luong's attention mechanism,31 through which a mapping between input and output tokens is obtained.\\n\\nto predict\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='to predict\\n\\nRetrosynthesis is the opposite of reaction prediction. Given a product molecule, the goal is to \\ue103nd possible reactants. In contrast to major product prediction, in retrosynthesis more than one target string might be correct, e.g. a product could be the result of two diﬀerent reactant pairs. Having no distinct target, the training of a seq2seq model can be more diﬃcult.\\n\\nThis journal is © The Royal Society of Chemistry 2018\\n\\nOpen Access Article. Published on 22 June 2018. Downloaded on 4/3/2023 5:18:49 PM.\\n\\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='Creative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article\\n\\nThe \\ue103rst attempt of using a seq2seq model in retrosynthesis was achieved by Liu et al.32 They used a set of 50 000 reactions extracted and curated by Schneider et al.19 The reactions from that set include stereochemical information and are classi\\ue103ed into ten diﬀerent reactions classes. Overall, none of the previous works was able to demonstrate the full potential of seq2seq models.\\n\\n3 Dataset'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"3 Dataset\\n\\nAll the openly available chemical reaction datasets were derived in some form from the patent text-mining work of Daniel M. Lowe.23 Lowe's dataset has recently been updated and contains data extracted from US patents grants and applications dating from 1976 to September 2016.33 What makes the dataset particularly interesting is that the quality and noise may be similar to the data a chemical company might own. The portion of granted patents is made of 1 808 938 reactions, which are described using SMILES.10\\n\\nLooking at the original patent data, it is surprising that a complex chemical synthesis process consisting of multiple steps, performed over hours or days, can be summarized in a simple string. Such reaction strings are composed of three groups of molecules: the reactants, the reagents, and the products, which are separated by a ‘>’ sign. The process actions and reaction conditions, for example, have been neglected so far.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"To date, there is no standard way of \\ue103ltering duplicates, incomplete or erroneous reactions in Lowe's dataset. We kept the \\ue103ltering to a minimum to show that our network is able to handle noisy data. We removed 720 768 duplicates by comparing reaction strings without atom mapping and an additional 780 reactions, because the SMILES string could not be canonicalized with RDKit,34 as the explicit number of valence electrons for one of the atoms was greater than permitted. We took only single product reactions, corresponding to 92% of the dataset, to have distinct prediction targets. Although this is a current limitation in the training procedure of our model, it could be easily overcome in the future, for example by de\\ue103ning a speci\\ue103c order for the product molecules. Finally, the dataset was randomly split into training, validation and test sets (18 : 1 : 1).{ Reactions with the same reactants, but diﬀerent reagents and products were kept in the same set.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"To compare our model and results with the current state of the art, we used the USPTO set recently published by Jin et al.20 It was extracted from Lowe's grants dataset33 and contains 479 035 atom-mapped reactions without stereochemical infor- mation. We restricted ourselves to single product reactions, corresponding to 97% of the reactions in Jin's USPTO set. An overview of the datasets taken as ground truths for this work is shown in Table 1.\\n\\nIn general, we observe that if reactions that do not work well with the model are removed under the assumption that they are erroneous, the model's accuracy will improve suggesting the presence of a speci\\ue103c tradeoﬀ between coverage and accuracy. This calls for open datasets. The only fair way to compare models is to use datasets to which identical \\ue103ltering was applied\\n\\nThis journal is © The Royal Society of Chemistry 2018\\n\\nView Article Online\\n\\nChemical Science\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"This journal is © The Royal Society of Chemistry 2018\\n\\nView Article Online\\n\\nChemical Science\\n\\nTable 1 Overview of the datasets used in this work. Jin's is derived from Lowe's grants dataset\\n\\nReactions in\\n\\nTrain\\n\\nValid\\n\\nTest\\n\\nTotal\\n\\nLowe's grants set33 no duplicates single product\\n\\nJin's USPTO set20 single product\\n\\n902 581 409 035 395 496\\n\\n50 131 30 000 29 075\\n\\n50 258 40 000 38 647\\n\\n1 808 938 1 088 170 1 002 970 479 035 463 218\\n\\nor where the reactions that the model is unable to predict are counted as false predictions.\\n\\n3.1 Data preprocessing To prepare the reactions, we \\ue103rst used the atom mappings to separate reagents from reactants. Input molecules with atoms appearing in the product were classi\\ue103ed as reactants and the others without atoms in the product as reagents. Then, we removed the hydrogen atoms and the atom mappings from the reaction string, and canonicalized the molecules. A\\ue09derwards, we tokenized reactants and products atom-wise using the following regular expression:\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='token_regex ¼ \"(\\\\[[^\\\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\\\(|\\\\)|\\\\. |¼|#|(cid:2)|\\\\+|\\\\\\\\\\\\\\\\\\\\/|:|(cid:3)|@|\\\\?|>|\\\\*|\\\\$|\\\\%[0–9]{2}|[0–9])\".\\n\\nAs reagent atoms are never mapped into product atoms, we employed a reagent-wise tokenization using a set of the 76 most common reagents, according to the analysis in ref. 19. Reagents belonging to this set were added as distinct tokens a\\ue09der the \\ue103rst ‘>’ sign, ordered by occurrence. Other reagents, which were not in the set, were neglected and removed completely from the reac- tion string. The separate tokenization would allow us to extend the reaction information and add tokens for reaction conditions without changing the model architecture. The \\ue103nal source sequences were made up of tokenized “reactants > common reagents” and the target sequence of a tokenized “product”. The tokens were separated by space characters. The preprocessing steps together with examples are summarized in Table 2. The same preprocessing steps were applied to all datasets.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='4 Model\\n\\nTo map the sequence of the reactants/reagents to the sequence of the products, we adapted an existing implementation35 with minor modi\\ue103cations. Our model architecture, illustrated in Fig. 1, consists of two distinct recurrent neural networks (RNN) working together: (1) an encoder that processes the input sequence and emits its context vector C, and (2) a decoder that uses this representation to output a probability over a predic- tion. For these two RNNs, we rely on speci\\ue103c variants of long short-term memory (LSTM)36 because they are able to handle long-range relations in sequences. An LSTM consists of units that process the input data sequentially. Each unit at each time\\n\\nChem. Sci., 2018, 9, 6091–6098 | 6093\\n\\nOpen Access Article. Published on 22 June 2018. Downloaded on 4/3/2023 5:18:49 PM.\\n\\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nView Article Online\\n\\nChemical Science\\n\\nEdge Article'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"This article is licensed under a\\n\\nView Article Online\\n\\nChemical Science\\n\\nEdge Article\\n\\nTable 2 Data preparation steps to obtain source and target sequences. The tokens are separated by a space and individual molecules by a point token\\n\\nStep\\n\\nExample (entry 23 738, Jin's USPTO test set20): reactants > reagents > products\\n\\n(1)\\n\\n(2)\\n\\n(3)\\n\\n(4)\\n\\nOriginal string\\n\\nReactants and reagent separation Atom-mapping removal and canonicalization Tokenization Source Target\\n\\n[Cl:1][c:2]1[cH:3][c:4]([CH3:8])[n:5][n:6]1[CH3:7].[OH:14][N+:15]([O(cid:2):16])¼[O:17].[S:9](¼[O:10])(¼[O:11]) ([OH:12])[OH:13][[Cl:1][c:2]1[c:3]([N+:15](¼[O:14])[O(cid:2):16])[c:4]([CH3:8])[n:5][n:6]1[CH3:7] [Cl:1][c:2]1[cH:3][c:4]([CH3:8])[n:5][n:6]1[CH3:7].[OH:14][N+:15]([O(cid:2):16])¼[O:17]>[S:9](¼[O:10])(¼[O:11]) ([OH:12])[OH:13]>[Cl:1][c:2]1[c:3]([N+:15](¼[O:14])[O(cid:2):16])[c:4]([CH3:8])[n:5][n:6]1[CH3:7] Cc1cc(Cl)n(C)n1.O¼[N+]([O(cid:2)])O>O¼S(¼O)(O)O>Cc1nn(C)c(Cl)c1[N+](¼O)[O(cid:2)]\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"C c 1 c c ( C l ) n ( C ) n 1 . O ¼ [ N+ ] ( [ O(cid:2) ] ) O> A1 > C c 1 n n ( C ) c ( C l ) c 1 [ N+ ] ( ¼ O ) [ O(cid:2)] C c 1 c c ( C l ) n ( C ) n 1 . O ¼ [ N+ ] ( [ O(cid:2) ] ) O> A1 C c 1 n n ( C ) c ( C l ) c 1 [ N+ ] ( ¼ O ) [ O(cid:2) ]\\n\\nwhere f is a multilayered BLSTM; ht ˛ ℝn are the hidden states at time t; xt is an element of an input sequence x ¼ {x0, ., xT}, which is a one-hot encoding of our vocabulary; and We are the learned embedding weights. Generally, C is simply the last of the encoder's hidden states:\\n\\nC ¼ hT\\n\\n(8)\\n\\nThe second part of the model – the decoder – predicts the\\n\\nprobability of observing a product ^y ¼ {^y1, ., ^yM}: YM\\n\\nPð^yÞ ¼\\n\\npð^yijf^y1\\n\\n; .; ^yi(cid:2)1gÞ\\n\\n(9)\\n\\ni¼0\\n\\nFig. 1\\n\\nIllustration of an attention-based seq2seq model.\\n\\nand for a single token ^yi:\\n\\nstep t processes an element of the input xt and the network's previous hidden state ht(cid:2)1. The output and the hidden state transition is de\\ue103ned by\\n\\nit ¼ s(Wixt + Uiht(cid:2)1 + bi)\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"it ¼ s(Wixt + Uiht(cid:2)1 + bi)\\n\\nft ¼ s(Wfxt + Ufht(cid:2)1 + bf)\\n\\not ¼ s(Woxt + Uoht(cid:2)1 + bo)\\n\\n(1)\\n\\n(2)\\n\\n(3)\\n\\np(^yi|{^y1, ., ^yi(cid:2)1},ci) ¼ g(^yi(cid:2)1,si,ci),\\n\\nwhere g is a stack of LSTM, which outputs the probability ^yt for a single token; si are the decoder's hidden states; and ci is a diﬀerent context vector for each target token yi. Bahdanau et al.38 and Luong et al.31 proposed attention mechanisms, i.e., diﬀerent ways for computing the ci vector rather than taking the last hidden state of the encoder ht. We performed experiments using both models and describe Luong's method, which yielded the best overall results.\\n\\n(10)\\n\\nct ¼ ft 5 ct(cid:2)1 + it 5 tanh(Wcxt + Ucht(cid:2)1 + bc)\\n\\n(4)\\n\\nht ¼ ot 5 tanh(ct(cid:2)1),\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"(10)\\n\\nct ¼ ft 5 ct(cid:2)1 + it 5 tanh(Wcxt + Ucht(cid:2)1 + bc)\\n\\n(4)\\n\\nht ¼ ot 5 tanh(ct(cid:2)1),\\n\\nwhere it, ft and ot are the input, forget, and output gates; c is the cell state vector; W, U and b are model parameters learned during training; s is the sigmoid function and 5 is the entry- wise product. For the encoder, we used a bidirectional LSTM (BLSTM).37 A BLSTM processes the input sequence in both directions, so they have context not only from the past but also from the future. They comprise two LSTMs: one that processes the sequence forward and the other backward, with their forward and backward hidden states ht for each time step. The hidden states of a BLSTM are de\\ue103ned as\\n\\n! ht and\\n\\n(5)\\n\\n4.1 Luong's attention mechanism To compute the context vector, we \\ue103rst have to compute the attention weights a:\\n\\nait ¼\\n\\n(cid:4) su i Waht (cid:4) exp\\n\\n(cid:5)\\n\\nexp\\n\\nXT\\n\\nsu i Waht0\\n\\n(cid:5)\\n\\nt0¼0\\n\\nci ¼\\n\\nXT\\n\\naitht;\\n\\nt¼0\\n\\n(11)\\n\\n(12)\\n\\nht ¼\\n\\n(cid:1) ! ht; ht\\n\\n(cid:3) :\\n\\n(6)\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='(cid:5)\\n\\nexp\\n\\nXT\\n\\nsu i Waht0\\n\\n(cid:5)\\n\\nt0¼0\\n\\nci ¼\\n\\nXT\\n\\naitht;\\n\\nt¼0\\n\\n(11)\\n\\n(12)\\n\\nht ¼\\n\\n(cid:1) ! ht; ht\\n\\n(cid:3) :\\n\\n(6)\\n\\nThe attention vector is then de\\ue103ned by\\n\\nai ¼ tanh(Wa{ci;si}).\\n\\n(13)\\n\\nThus we can formalize our encoder as\\n\\nBoth Wa and Wa are learned weights. Then a can be used to\\n\\nC ¼ f(Wext,ht(cid:2)1),\\n\\n(7)\\n\\ncompute the probability for a particular token:\\n\\n6094 | Chem. Sci., 2018, 9, 6091–6098\\n\\nThis journal is © The Royal Society of Chemistry 2018\\n\\nOpen Access Article. Published on 22 June 2018. Downloaded on 4/3/2023 5:18:49 PM.\\n\\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article\\n\\np(yi|{y1, ., yi(cid:2)1},ci) ¼ softmax(Wpai),\\n\\n(14)\\n\\nwhere Wp are also the learned projection weights.\\n\\n4.2 Training details\\n\\nDuring training, all parameters of the network were trained jointly using a stochastic gradient descent. The loss function was a cross-entropy function, expressed as\\n\\nX\\n\\nHð y; ^yÞ ¼ (cid:2)\\n\\nyi logð ^yiÞ\\n\\n(15)\\n\\ni'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='X\\n\\nHð y; ^yÞ ¼ (cid:2)\\n\\nyi logð ^yiÞ\\n\\n(15)\\n\\ni\\n\\nfor a particular training sequence. The loss was computed over an entire minibatch and then normalized. The weights were initialized using a random uniform distribution ranging from (cid:2)0.1 to 0.1. Every 3 epochs, the learning rate was multiplied by a decay factor. The minibatch size was 128. Gradient clipping was applied when the norm of the gradient exceeded 5.0. The teacher forcing method39 was used during training.\\n\\n5 Architecture & hyperparameter search'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='5 Architecture & hyperparameter search\\n\\nFinding the best-performing set of hyperparameters for a deep neural network is not trivial. As mentioned in Section 4, our model has numerous parameters that can in\\ue104uence both its training and its architecture. Depending on those parameters, the performance of the model can vary notably. In order to select the best parameters eﬃciently, we build a framework around scikit-optimize.40 A\\ue09der the evaluation of 10 random sets, a gradient-boosted tree model41 was used as surrogate model together with expected improvement as acquisition function42 to guide the hyperparameter search on a space de\\ue103ned in Table 3. The sets of hyperparameters were evaluated according to their accuracy on the validation set. In total, we trained 100 models for 30 epochs. The set of best hyperparameters found with this method is highlighted in bold. This model has been further trained to 80 epochs to improve its \\ue103nal accuracy.\\n\\n6 Experiments 6.1 Reaction prediction'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='6 Experiments 6.1 Reaction prediction\\n\\nWe evaluated our model on two data sets and compared the the performance with other state-of-the-art\\n\\nresults. A\\ue09der\\n\\nTable 3 Hyperparameters space, parameters for the best model in bold\\n\\nParameter\\n\\nPossible values\\n\\nNumber of units Number of layers Type of encoder Output dropout State dropout Learning rate Decay factor Type of attention\\n\\n128, 256, 512 or 1024 2, 4 or 6 LSTM, BLSTM 0–0.9 (0.7676) 0–0.9 (0.5374) 0.1–5 (0.355) 0.85–0.99 (0.854) “Luong” or “Badhanau”\\n\\nThis journal is © The Royal Society of Chemistry 2018\\n\\nView Article Online\\n\\nChemical Science'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"This journal is © The Royal Society of Chemistry 2018\\n\\nView Article Online\\n\\nChemical Science\\n\\nhyperparameter optimization, we continued to train our best model on the 395 496 reactions in Jin's USPTO train set and tested the fully trained model on Jin's USPTO test set. Addi- tionally, we trained a second model with the same hyper- parameters on 902 581 randomly chosen single-product reactions from the more complex and noisy Lowe dataset and tested it on a set of 50 258 reactions. As molecules are discrete data, changing a single character, such as in source code or arithmetic expressions, can lead to completely diﬀerent mean- ings or even invalidate the entire string. Therefore we use full- sequence accuracy, the strictest criteria possible, as our evalu- ation metric by which a test prediction is considered correct only if all tokens are identical to the ground truth.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='The network had to solve three major challenges. First, it had to memorize the SMILES grammar to predict synthetically correct sequences. Second, because we trained it on canon- icalized molecules, the network had to learn the canonical representation. Third, the network had to map the reactants plus reagents space to the product space.\\n\\nAlthough the training was performed without a beam search, we used a beam width of 10 without length penalty for the inference. Therefore the 10 most probable sequences were kept at every time step. This allowed us to know what probability the network assigned to each of the sequences. We used the top-1 probabilities to analyze the prediction con\\ue103dence of the network.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"The \\ue103nal step was to canonicalize the network output. This simple and deterministic reordering of the tokens improved the accuracy by 1.5%. Thus, molecules that were correctly pre- dicted, but whose tokens were not enumerated in the canonical order, were still counted as correct. The prediction accuracies of our model on diﬀerent datasets are reported in Table 4. For single product reactions, we achieved an accuracy of 83.2% on Jin's USPTO test dataset and 65.4% on Lowe's test set.\\n\\nAn additional validation can be found in the ESI,† were we used the model trained on Lowe's dataset to predict reactions from pistachio,45 a commercial database of chemical reactions extracted from the patent literature. Because the Lowe's dataset used to train the model contained reactions until September 2016, we only predicted the reactions from 2017 and hence, we had a time split.\\n\\n6.2 Comparison with the state of the art\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"6.2 Comparison with the state of the art\\n\\nTo the best of our knowledge, no previous work has attempted to predict reactions on the complete US patent dataset of Lowe.33 Table 5 shows a comparison with the Weisfeiler–Lehman diﬀer- ence networks (WLDN) from Jin et al.20 on their USPTO test set.\\n\\nTable 4 Scores of our model on diﬀerent single product datasets\\n\\nAccuracies in [%]\\n\\nTest set\\n\\nSize\\n\\nBLEU,43 ROUGE44\\n\\nTop-1\\n\\nTop-2\\n\\nTop-3\\n\\nJin's USPTO20 Lowe's33\\n\\n38 648 50 258\\n\\n95.9, 96.0 90.3, 90.9\\n\\n83.2 65.4\\n\\n87.7 71.8\\n\\n89.2 74.1\\n\\nChem. Sci., 2018, 9, 6091–6098 | 6095\\n\\nOpen Access Article. Published on 22 June 2018. Downloaded on 4/3/2023 5:18:49 PM.\\n\\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nView Article Online\\n\\nChemical Science\\n\\nEdge Article\\n\\nTable 5 Comparison with Jin et al.20 The 1352 multiple product reactions (3.4% of the test set) are counted as false predictions for our model\\n\\nJin's USPTO test set,20 accuracies in [%]\\n\\nMethod\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"Jin's USPTO test set,20 accuracies in [%]\\n\\nMethod\\n\\nTop-1\\n\\nTop-2\\n\\nTop-3\\n\\nWLDN20 Our model\\n\\n79.6 80.3\\n\\n84.7\\n\\n87.7 86.2\\n\\nTop-5\\n\\n89.2 87.5\\n\\nprobability was related to accuracy. Fig. 2a illustrates the distri- bution of the top-1 probability for Lowe's test set in cases where the top-1 prediction is correct (le\\ue09d) and where it is wrong (right). A clear diﬀerence can be observed and used to de\\ue103ne a threshold under which we determine that the network does not know what to predict. Fig. 2b shows the top-1 accuracy and coverage depending on the con\\ue103dence threshold. For example, for a con\\ue103- dence threshold of 0.83 the model would predict the outcome of 70.2% of the reactions with an accuracy of 83.0% and for the remaining 29.8% of the reaction it would not know the outcome.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='To make a fair comparison, we count all the multiple product reactions in the test set as false predictions for our model because we trained only on the single product reactions. By achieving 80.3% top-1 accuracy, we perform quantitatively nearly identical. As our model does not rank candidates, but was trained on accurately predicting the top-1 outcome, it is not surprising that the WLDN beats our model in the top-3 and top-5 accuracy. The decoding of the 38 648 USPTO test set reactions takes on average 25 ms per reaction, inferred with a beam search. Our model can therefore compete with the state of the art.\\n\\n6.3 Prediction con\\ue103dence\\n\\nWe analyzed the top-1 beam search probability to obtain infor- mation about prediction con\\ue103dence and to observe how this\\n\\n6.4 Attention'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"We analyzed the top-1 beam search probability to obtain infor- mation about prediction con\\ue103dence and to observe how this\\n\\n6.4 Attention\\n\\nAttention is the key to take into account complex long-range dependencies between multiple tokens. Speci\\ue103c functional groups, solvents or catalysts have an impact on the outcome of a reaction, even if they are far from the reaction center in the molecular graph and therefore also in the SMILES string. Fig. 3 shows how the network learned to focus \\ue103rst on the C[O(cid:2)] molecule, to map the [O(cid:2)] in the input correctly to the O in the target, and to ignore the Br, which is replaced in the target. A few more reaction predictions together with the attention weights, con\\ue103dence and token probabilities are found in the ESI.†\\n\\nFig. 2 Top-1 prediction conﬁdence plots for Lowe's test set inferred with a beam search of 10.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"Fig. 2 Top-1 prediction conﬁdence plots for Lowe's test set inferred with a beam search of 10.\\n\\nFig. 3 Attention weights of reaction 120 from Jin's USPTO test set. The atom mapping between reactants and product is highlighted. SMILES: Brc1cncc(Br)c1.C[O(cid:2)]>CN(C)C¼O.[Na+]>COc1cncc(Br)c1. Reaction plotted with RDKit.34\\n\\n6096 | Chem. Sci., 2018, 9, 6091–6098\\n\\nThis journal is © The Royal Society of Chemistry 2018\\n\\nOpen Access Article. Published on 22 June 2018. Downloaded on 4/3/2023 5:18:49 PM.\\n\\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nEdge Article\\n\\n6.5 Limitations\\n\\nOur model is not without limitations. An obvious disadvantage compared to template-based methods is that the strings are not guaranteed to be a valid SMILES. Incorporating a context-free grammar layer, as was done in ref. 12, could bring minor improvements. Fortunately, only 1.3% of the top-1 predictions are grammatically erroneous for our model.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"Another limitation of the training procedure are multiple product reactions. In contrast to words in a sentence, the exact order in which the molecules in the target string are enumerated does not matter. A viable option would be to include in the training set all possible permutations of the product molecules.\\n\\nspace during optimization was restricted to a maximum of 1024 units for the encoder. Using more units could have led to improvements. On Jin's USPTO dataset, the training plateaued because an accuracy of 99.9% was reached and the network had memorized almost the entire training set. Even on Lowe's noisier dataset, a training accuracy of 94.5% was observed. A hyperparameter optimization could be performed on Lowe's dataset to improve the prediction accuracy.\\n\\nOur hyperparameter\\n\\n7 Conclusion\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"Our hyperparameter\\n\\n7 Conclusion\\n\\nPredicting reaction outcomes is a routine task of many organic chemists trained to recognize structural and reactivity patterns reported in a wide number of publications. Not only did we show that a seq2seq model with correctly tuned hyper- parameters can learn the language of organic chemistry, our approach also improved the current state-of-the-art in patent reaction outcome prediction by achieving 80.3% on Jin's USPTO dataset and 65.4% on single product reactions of Lowe's data- set. Similar to the work of Nam and Kim,17 our approach is fully data driven and free of chemical knowledge/rules and compared to their work,17 we take full advantage of the attention mechanism. Also worth mentioning is the overall simplicity of our model that jointly trains the encoder, decoder and attention layers end-to-end. Our hope is that, with this type of model, chemists can codify and perhaps one day fully automate the art of organic synthesis.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='Conﬂicts of interest There are no con\\ue104icts to declare.\\n\\nAcknowledgements\\n\\nWe thank Nadine Schneider, Greg Landrum and Roger Sayle for the helpful discussions on RDKit and the datasets. We also would like to acknowledge Marwin Segler and Hiroko Satoh for useful feedback on our approach.\\n\\nNotes and references\\n\\n§ https://github.com/wengong-jin/nips17-rexgen.\\n\\n{ https://ibm.box.com/v/ReactionSeq2SeqDataset.\\n\\nThis journal is © The Royal Society of Chemistry 2018\\n\\nView Article Online\\n\\nChemical Science\\n\\n1 E. J. Corey and W. T. Wipke, Science, 1969, 166, 178–192. 2 T. D. Salatin and W. L. Jorgensen, J. Org. Chem., 1980, 45(11),\\n\\n2043–2051.\\n\\n3 H. Satoh and K. Funatsu, J. Chem. Inf. Comput. Sci., 1995, 35,\\n\\n34–44.\\n\\n4 H. Satoh and K. Funatsu, J. Chem. Inf. Comput. Sci., 1996, 36,\\n\\n173–184.\\n\\n5 M. H. S. Segler and M. P. Waller, Chem.–Eur. J., 2017, 23,\\n\\n6118–6128.\\n\\n6 C. W. Coley, R. Barzilay, T. S. Jaakkola, W. H. Green and\\n\\nK. F. Jensen, ACS Cent. Sci., 2017, 3, 434–443.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='6118–6128.\\n\\n6 C. W. Coley, R. Barzilay, T. S. Jaakkola, W. H. Green and\\n\\nK. F. Jensen, ACS Cent. Sci., 2017, 3, 434–443.\\n\\n7 W. R. J. Dolbier, K. Henryk, K. Houk and S. Chimin, Acc.\\n\\nChem. Res., 1996, 29, 471–477.\\n\\n8 D. Mondal, S. Y. Li, L. Bellucci, T. Laino, A. Ta\\ue103, S. Guccione\\n\\nand S. D. Lepore, J. Org. Chem., 2013, 78, 2118–2127.\\n\\n9 O. Engkvist, P.-O. Norrby, N. Selmi, Y.-h. Lam, Z. Peng, E. C. Sherer, W. Amberg, T. Erhard and L. A. Smyth, Drug Discovery Today, 2018, 23(6), 1203–1218.\\n\\n10 D. Weininger, J. Chem. Inf. Comput. Sci., 1988, 281413, 31–36. 11 A. Cadeddu, E. K. Wylie, J. Jurczak, M. Wampler-Doty and B. A. Grzybowski, Angew. Chem., Int. Ed., 2014, 53, 8108–8112. J. N. Wei, D. Duvenaud, J. M. Hern´andez-Lobato, B. S´anchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams and A. Aspuru-Guzik, ACS Cent. Sci., 2018, 4, 268–276.\\n\\n12 R. G´omez-Bombarelli,\\n\\n13 S. Jastrze˛bski, D. Le´sniak and W. M. Czarnecki, Learning to'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='12 R. G´omez-Bombarelli,\\n\\n13 S. Jastrze˛bski, D. Le´sniak and W. M. Czarnecki, Learning to\\n\\nSMILE(S), 2016, http://arxiv.org/abs/1602.06289.\\n\\n14 M. J. Kusner, B. Paige and J. M. Hern´andez-Lobato, ICML,\\n\\n2017.\\n\\n15 E. J. Bjerrum, SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules, 2017, http://arxiv.org/ abs/1703.07076.\\n\\n16 M. H. S. Segler, T. Kogej, C. Tyrchan and M. P. Waller, ACS\\n\\nCent. Sci., 2018, 4, 120–131.\\n\\n17 J. Nam and J. Kim, Linking the Neural Machine Translation and the Prediction of Organic Chemistry Reactions, 2016, https://arxiv.org/pdf/1612.09529.pdf.\\n\\n18 N. Schneider, D. M. Lowe, R. A. Sayle and G. A. Landrum,\\n\\nJ. Chem. Inf. Model., 2015, 55, 39–53.\\n\\n19 N. Schneider, N. Stie\\ue104 and G. A. Landrum, J. Chem. Inf.\\n\\nModel., 2016, 56, 2336–2346.\\n\\n20 W. Jin, C. Coley, R. Barzilay and T. Jaakkola, NIPS, 2017, pp.\\n\\n2607–2616.\\n\\n21 J. N. Wei, D. Duvenaud and A. Aspuru-Guzik, ACS Cent. Sci.,\\n\\n2016, 2, 725–723.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='2607–2616.\\n\\n21 J. N. Wei, D. Duvenaud and A. Aspuru-Guzik, ACS Cent. Sci.,\\n\\n2016, 2, 725–723.\\n\\n22 D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik and R. P. Adams, NIPS, 2015. 23 D. M. Lowe, Ph.D. thesis, University of Cambridge, 2012. 24 M. H. S. Segler and M. P. Waller, Chem.–Eur. J., 2017, 23,\\n\\n5966–5971.\\n\\n25 J. Bauer, E. Fontain, D. Forstmeyer and I. Ugi, Tetrahedron\\n\\nComput. Methodol., 1988, 1, 129–132.\\n\\n26 P. R¨ose and J. Gasteiger, Anal. Chim. Acta, 1990, 235, 163–\\n\\n168.\\n\\n27 W. L. Jorgensen, E. R. Laird, A. J. Gushurst, J. M. Fleischer, S. A. Gothe, H. E. Helson, G. D. Paderes and S. Sinclair, Pure Appl. Chem., 1990, 62, 1921–1932.\\n\\nChem. Sci., 2018, 9, 6091–6098 | 6097\\n\\nOpen Access Article. Published on 22 June 2018. Downloaded on 4/3/2023 5:18:49 PM.\\n\\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nChemical Science'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='Creative Commons Attribution-NonCommercial 3.0 Unported Licence.\\n\\nThis article is licensed under a\\n\\nChemical Science\\n\\n28 W. A. Warr, Mol. Inf., 2014, 33, 469–476. 29 M. A. Kayala and P. Baldi, J. Chem. Inf. Model., 2012, 52,\\n\\n2526–2540.\\n\\n30 M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, X. Zheng and G. Brain, OSDI, 2016.\\n\\n31 M.-T. Luong, H. Pham and C. D. Manning, EMNLP, 2015. 32 B. Liu, B. Ramsundar, P. Kawthekar, J. Shi, J. Gomes, Q. Luu Nguyen, S. Ho, J. Sloane, P. Wender and V. Pande, ACS Cent. Sci., 2017, 3, 1103–1113.\\n\\n33 D. M. Lowe, Chemical reactions from US pat. (1976-Sep2016), 2017, https://\\ue103gshare.com/articles/Chemical_reactions_from_ US_patents_1976-Sep2016_/5104873.'), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content=\"34 G. Landrum, B. Kelley, P. Tosco, S. Riniker, Gedeck, N. Schneider, R. Vianello, A. Dalke, S. Alexander, S. Turk, M. Swain, B. Cole, JLVarjo, A. Pahl, P. Fuller, G. Doliath, M. W´ojcikowski, D. Cosgrove, G. Sforna, M. Nowotka, J. H. Jensen, J. Doma´nski, D. Hall, N. O'Boyle, W.-G. Bolick, Nhfechner and S. Roughley,\\n\\nJ. P, Strets123,\\n\\n6098 | Chem. Sci., 2018, 9, 6091–6098\\n\\nView Article Online\\n\\nEdge Article\\n\\nRdkit/Rdkit: 2017_09_1 (Q3 2017) Release, 2017, https:// zenodo.org/record/1004356#.Wd3LDY6l2EI.\\n\\n35 R. Zhao, T. Luong and E. Brevdo, Neural Machine Translation (seq2seq) Tutorial, 2017, https://github.com/tensor\\ue104ow/nmt. 36 S. Hochreiter and J. Schmidhuber, Neural Comput., 1997, 9,\\n\\n1735–1780.\\n\\n37 A. Graves and J. Schmidhuber, Neural Network., 2005, 18,\\n\\n602–610.\\n\\n38 D. Bahdanau, K. Cho and Y. Bengio, ICLR, 2015. 39 R. J. Williams and D. Zipser, Neural Comput., 1989, 1, 270–\\n\\n280.\"), Document(metadata={'source': 'ai-science-training-series/05_llm_part2/PDFs/c8sc02339e.pdf'}, page_content='602–610.\\n\\n38 D. Bahdanau, K. Cho and Y. Bengio, ICLR, 2015. 39 R. J. Williams and D. Zipser, Neural Comput., 1989, 1, 270–\\n\\n280.\\n\\n40 T. Head, N. Campos, M. Cherti, A. Fabisch, T. Fan, M. Kumar, G. Louppe, K. Malone, M. Pak, I. Shcherbatyi, T. Smith and Z. Vin´ıcius, Scikit-Optimize, 2017, http://scikit- optimize.github.io/.\\n\\n41 J. H. Friedman, Ann. Stat., 2001, 29(5), 1189–1232. 42 J. Mockus, J. Global Optim., 1994, 4, 347–365. 43 K. Papineni, S. Roukos, T. Ward and W.-J. Zhu, ACL, 2001. 44 C.-Y. Lin, ACL, 2004. 45 N. Schneider, D. M. Lowe, R. A. Sayle, M. A. Tarselli and\\n\\nG. A. Landrum, J. Med. Chem., 2016, 59, 4385–4402.\\n\\nThis journal is © The Royal Society of Chemistry 2018')]\n"
          ]
        }
      ],
      "source": [
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJKCqpSCAlvv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfgRcG6g0XOG",
        "scrolled": true,
        "outputId": "13ef5c97-1562-4e0c-9122-9a04f72b9cac"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'page_content'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m      2\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_text_splitters/base.py:94\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     92\u001b[0m texts, metadatas \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 94\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m)\n\u001b[1;32m     95\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
          ]
        }
      ],
      "source": [
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "# docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyTRRvU6DboO"
      },
      "source": [
        "### 4. Then we embed the chunked texts using a Transformer.\n",
        "This allows us to encode the text into our search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXny33dcCHZc"
      },
      "source": [
        "Embedding converts text to a numerical representation in a vector space. RAG compares the embeddings of user queries within the vector of the knowledge library.\n",
        "\n",
        "In this example, we choose a simple embedding using the MiniLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kabjUo1I1Pev",
        "outputId": "1f6a55b9-adc5-4cc3-f22f-d87f8e12af1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "50ab3c6b796a417f82eae97fdff209a5",
            "92ead2409a454641895e0250bdcfef9e",
            "2736a3bcfeed4d0ca1e4b057b6ba263c",
            "96a48c244bde44f783e17d2fb09ab30a",
            "9d2dca32e0b04c7a830b2ea6b6709d3b",
            "1d0d1b4d8e664053b5ecc9ac18ae4d95",
            "a351e18bdd13415a8bfc2925792144e8",
            "0a51867e85bb4ca091b342d3b444ba0e",
            "767b536a3bc546119278b81576d373f4",
            "944d4d22802d48a79e95bd454b227af0",
            "08d8aa99df3a4eca9beb3795df5a9896",
            "391271ffe92c4c5f9293704f0e44baa7",
            "4b8d5a71094e4961a9bb589dbc50fc91",
            "5fc35bc7eb0c477bac62296252729b14",
            "99e27e3724b149daaea6635184b47c4c",
            "1ecc8143b4d84c9a890d2f0b90e3ed01",
            "f97a270a971f438483be1b55ad73108b",
            "683589aa5e8442e59a2fa2463cdb72d2",
            "bcc4bdcaf218483e95e14789c121d065",
            "a0dc6b70c3df41deaee6ffd21bac52c9",
            "5f8b307ac7954fe2a24179a409abb5fd",
            "0837819fda384c4cabd5276f62733c65",
            "c98d05af6a244d6fa8e362fb5ff3cf4f",
            "e59501eda6314551a58a62a376e2fb40",
            "0b03bd364ab74b63be48cd36638af1ad",
            "9481a37e853848fd95d65adffbdbdea6",
            "5df3f5f0058e48e1b9f0936989c10003",
            "cdefd91bd9024e7980ce1b2e4de212a7",
            "116b5c29f916401b8cda95433769823b",
            "3eeb2e05033c41648e2465bb597c8d94",
            "5b3c747f8a7e4326b3eae01c5b809851",
            "04c5866455bc460c91672fdf5e62af49",
            "3a51a4709d5a455ab588e1121890726d",
            "b956c7fa66e145bd9a6f1089b6fbd1e1",
            "5b328001cb684d2c9bfc91ed9f335141",
            "460195c92d83417191621463926bf878",
            "bdf62004be78469cb077718421936ea7",
            "6b671e2331d54ef1a19e1622352ca8f0",
            "94f738ea2ecb4d589c7bc188a974f875",
            "e01a0891183f41b9b8cb7df660ee6dcb",
            "209b5810fe114d6d8509265cf1aa42bb",
            "e8d09f7ae86e4082856f5244b27962a9",
            "6b57d16116a04fa0a7e4769c79e19480",
            "b0877b9497724e00bf4e539751a93cfe",
            "9516be91ee184931b721f27b5421fc39",
            "d689173163b8414dabae7c8e070189c8",
            "e1cd19fc345a46808ed2137c66e9f648",
            "114f8d5b692249a9a5e231409f42c347",
            "97df4be860e441ff82656478d73152d5",
            "a3d56be0e1ed4157939479c14eb25d59",
            "b307d2019b6548158dfd9eb2e2d81266",
            "f807016b612e4c01aa0b4698b370a722",
            "efa5edbd2d754094a1979c2b74f5d133",
            "6f57b73719b848c792e22ebed6301905",
            "41fc56daebe14c5baa8b36107d7175b8",
            "b5c579dcc4c84dfb83aaabc416c6a9e7",
            "22fcd9377d244129acd96f46e3bc17e7",
            "5febe76d866741bda6fded73a471049b",
            "b4119fda1ac14ca596654075c37ff420",
            "1c539a577435446ebf2d599e1f777630",
            "0b5751aeb9934365ae60003114e5f593",
            "e8a92a347fd942bd8776b6167571bcf1",
            "c7d7690e79c34e3b86fb898cecf2ca4c",
            "9584d33540a7442d937513363c72a0ab",
            "801dd358909843f0bbbada7d10099ca7",
            "f03aacd78c5040619e5d89a9475af1d5",
            "ab86031acdc749e9b31bc1698a635ce0",
            "e2d4d8658aa94155bd4a92ad114c51b8",
            "bed8e2726d8840d2b08a5f129dc19bc1",
            "b9a70ae5d92a465b980845e87a67f1e4",
            "c38415d2e45d4ff7ba871d99f9b1b523",
            "fa81521a40d540abbdd2564fe70a954d",
            "daca27b9821d4badb7243649d9a3c360",
            "ecf056fc7070415fb83ea4deb3887583",
            "1fb2b2c4b2954480955dc530cc0e7905",
            "403be7357b3f40d6a8a6f428aed102f2",
            "57afef0e704a48e5b57752f5092070a8",
            "51e3d6c4048348f9bd6d878360774fea",
            "2d132b6c6841423080b69c7ea91b4c05",
            "7166a3209cf544308dd965da2939380a",
            "35d99e80f961460588173e44abf8cef8",
            "b1a6e12f57b74c3892be626f1d088b49",
            "6accc604666140efad6a0b7b6054d3e3",
            "6b1cdd7ee89840a99f99c0bb865e58d2",
            "8062ce6a0fc1414e919c576e68dea7ce",
            "de8aba8f805b4c0fb5579f23e9a9de5c",
            "e7687eb21dec4e2cb049fd12a7a2b9a2",
            "cc12a4910eec435ba623c63822953be4",
            "ef2508cbbff842c28ca964a949c0f64a",
            "53478ed6b90c4e2195cd263f63923478",
            "1b03b0db59df438db1d73f0e893ab8da",
            "b5146d3bbdc14737bb71f62dcd71ea24",
            "dcbd16e6845a4eb68acf1f344bc2ff8f",
            "11fdd17b1f0e4a349e58f757586bd6c1",
            "b4a5837db14048f28a54c9ec825f3440",
            "d126a7834be44b6a81094298433aa126",
            "671f20dd28b2405f8fc8e3380d47f757",
            "3bb4bacbb11a4f40af7414cd10d3aec6",
            "2be6a69e298846f6ba91cfb3633eab1c",
            "6195f1f406a343e1b731efce4a5a95b1",
            "89e343f8574a4496931c22756f4bca83",
            "6e55bddd29af4579b721c16a2fc63e1c",
            "3441ed26851742819018b4abbe19e212",
            "807bca63da8e445099de076866aff172",
            "8ee3f6e207e04250b5320ff30088ab23",
            "be489b0db48a403e8cbf868c372c2e4e",
            "29b6641a6c9448498b444d14e668221a",
            "b412f6f12a84414eb0dc5e900671d436",
            "f2509b738f9f4d94928304c5a46d86f4",
            "1df2254fe3504ed3a86c593869b0f9db",
            "dcc44a136fbd412bac5fab9d4b06a40a",
            "508eba817d7d41b78808fbc040427596",
            "562538ee70f0404ca5c63c167e6d3a21",
            "4f7b9a64a59744d581f3bc26765643ff",
            "34285e769efe4e2e8095e15c739caf8e",
            "401b60be3839406987d29c995b3679da",
            "13fb95ab92de41b88347c7aed2e9441c",
            "85c7e3cb187448dca099364db4b805be",
            "a72fa2953d4c4f70bcd75ec88c2ea95d",
            "1b737b62f51e4b1095b7d3db1c060efe",
            "d31607e4830a4e0788624b459f809e96"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-adf0a18b0ba1>:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50ab3c6b796a417f82eae97fdff209a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "391271ffe92c4c5f9293704f0e44baa7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c98d05af6a244d6fa8e362fb5ff3cf4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b956c7fa66e145bd9a6f1089b6fbd1e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9516be91ee184931b721f27b5421fc39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5c579dcc4c84dfb83aaabc416c6a9e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab86031acdc749e9b31bc1698a635ce0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51e3d6c4048348f9bd6d878360774fea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef2508cbbff842c28ca964a949c0f64a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6195f1f406a343e1b731efce4a5a95b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcc44a136fbd412bac5fab9d4b06a40a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "model_kwargs = {'device':'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings':False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name = modelPath,\n",
        "  model_kwargs = model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvkGZSJvEFeK"
      },
      "source": [
        "### 5.Create a vector database\n",
        "Vector databases, also called vector storage, efficiently store and retrieve vector data, which are arrays of numerical values representing points in multi-dimensional space. They're useful for handling data like embeddings from deep learning models or numerical features. Unlike traditional relational databases, which aren't optimized for vectors, vector databases offer efficient storage, indexing, and querying for high-dimensional and variable-length vectors.\n",
        "\n",
        "There are various types of vector databases:\n",
        "1. Chroma\n",
        "2. FAISS\n",
        "3. Pinecone\n",
        "4. Weaviate\n",
        "5. Qdrant\n",
        "\n",
        "Here, we build this using the FAISS utility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqccXGwoEYP-"
      },
      "source": [
        "![vector_database](https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/04-rag/rag_images/vector_database.png?raw=1)\n",
        "\n",
        "Image credit: https://blog.gopenai.com/primer-on-vector-databases-and-retrieval-augmented-generation-rag-using-langchain-pinecone-37a27fb10546"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWu5YGPj1e6B",
        "outputId": "f5b9e4cd-f749-494a-ff0e-f6f9dbbbb3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFdiffusion, allowing it to efficiently target this site (right bar, pink). C-D) As well as conditioning on hotspot residue information, a fine-tuned RFdiffusion model can also condition on input fold information (secondary structure and block-adjacency information - see Supplementary Methods 4.5). This effectively allows the specification of a (for instance, particularly compatible) fold that the binder should adopt. C) Two examples showing binders can be specified to adopt either a ferredoxin fold (left) or a particular helical bundle fold (right). D) Quantification of the efficiency of fold-conditioning. Secondary structure inputs were accurately respected (top, pink). Note that in this design target and target site, RFdiffusion without fold-specification made generally helical\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "question = \"What is RF Fold?\"\n",
        "searchDocs = db.similarity_search(question)\n",
        "print(searchDocs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A56JddAEkCF"
      },
      "source": [
        "### 6. Initialize the LLM that will be used for question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdUokERIMnz3"
      },
      "source": [
        "Here, we use a pretrained model flan-t5-large as part of a HuggingFacePipeline. This will later be chained with the vector database for RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9CXf3N6c134r",
        "scrolled": true,
        "outputId": "edddcab0-093b-426a-c308-3ec43cf96bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "c4804d4a46df4aa1b9b089e48e54aa82",
            "9cbdb36c80f546c0b9e066c0e3ad85c1",
            "7b22e2c90d174f5dad664e5995c425b1",
            "88457ec03982476eb958efb1ea2d5085",
            "a4a99d57c30b44699f70a5b7dfe366a8",
            "609c953b06e14252ad4999f3e9c6aa2b",
            "5f37d315b0744d20b8b35e61eeda8e2c",
            "e87a73d92853477f9ac1bd767c11999a",
            "ee6c4bf481b04b3284116fc2849a14d5",
            "f8c0492545f54a3aac8b2bf6f68f146b",
            "715d28a3fb5f429dbe21a05513584aef",
            "db5a066686154f05997080e98ce59abc",
            "b23437e99a5a4e75830d4762ac89da9c",
            "3f5317ca9c0f4f09894326c9532c71cf",
            "95948e44862a441a819712cd71ca6eae",
            "89066b2795b44e50905e6e0544e094b0",
            "f1f08e9c066f46ee8e2352e144e26f0a",
            "2ca48ed368fc417f9483facb6a3a58ae",
            "d199fb112d4c4417a85efef894288068",
            "488cca4d9fb7456186b3fdc8a9e5a51a",
            "37cc1f7ee94f4424a4d4bca31caa09ec",
            "6e6182ce57bf4c61a87d800556392705",
            "90ba1c5997c54789b3e306db6bf7cdb6",
            "9836b480662a47919f33ee38ac5e6236",
            "94b701eceb2f4ee7a1c3eacd75676ae5",
            "5748cc4f24444c989993535513a427fb",
            "e0fd0716509740928f1543ec07b08b12",
            "c7978b7c1fca42a4a67b2532ebebbcd0",
            "b417dff54be9488fa6f94bc06be183f4",
            "7dd24d7223b243bbbb56dcd7ccd78c7a",
            "70afe58926c245f9be76995560405afc",
            "d45a45e2036f42d1984e047ebd63fa32",
            "57b0c1b425e34c3caa2d0cd8521a2911",
            "db16259e7cba44dfa636304774c2c6db",
            "cf2c8f83a6b34cd88f7cebd0175c72af",
            "847851e1f2214e1cb3d66f1cc2d3405e",
            "aa13b0a1cc8a4b709f4dd9479a472ed1",
            "966a23bf2cfd4619822d48be8c4701c6",
            "a2bec6964c5f4320929dd0ae77e56a41",
            "41e85494f1ec496d8fcd359017decbd0",
            "bf1bac252f6945df860a53417d1942d6",
            "73d92a1009b542fd8d919846785f380b",
            "9b5d4a5edbf640d58d554fb7ce8acc58",
            "04cc1b7ac3a34690b3851f47efb8815e",
            "68bd97c79f7b4bceb203828a758d107e",
            "800a7753b75744bdb02173766a825515",
            "6e5e135b276e4a4583f016851faf5d08",
            "96016c6eb931439aaa01a1a76e7bd376",
            "ec15c2c66d0e4c5e8fe32dc9ddfd5034",
            "08db420bd45e4f508676d784df456fbc",
            "1e51258337e148e7ae9a408d4286f2b9",
            "4edf8bda68ce45d9aaa266e83314e0f8",
            "fbd3f1b8b2104b56a792ac72504c40d2",
            "4fb225f5989c402fbb88bc95946fae47",
            "b7ab29f24a6743ee8e7dec6f6ad5725f",
            "4e5131daf3fb4645a35c098f3db3f74b",
            "35d4ac78182f4dbc8cd28f886998e2ba",
            "a3e334873908452fa418b7d39b933c20",
            "32d2ad1b1205484694970c7becefa6dd",
            "62f90487c5d94e7fbeca07bbfb77aca2",
            "6174d061c9a840928eb8e5a64d3764d8",
            "eb23c29ff7c74084ab1566389561a9cd",
            "a4f24f1fbe15430bb93621c45ef2cc95",
            "25c12ded834343ebbd53247a3bce686b",
            "9dfefb4431ff4989871b19e88e28cf33",
            "78e12ba3a9ab463ebca1bce5019d1edc",
            "ae96aaef5684415f818413c0b3f2a4a5",
            "f7ec65d9bb134a8eaec3e5476d406e13",
            "25c1bf7af8aa4c9886edcdb45ca1fe4d",
            "da29190b171641d78390ff070fe40701",
            "250e2c59d8cd4b12b74a03d84e6370af",
            "86470b106c59468e96056b638b2ce696",
            "9ac1876586274f9789ab2530d499aff5",
            "7c8aeae0703c43ea9b2c60dd07700099",
            "366d99b698f743b485ba40bafe7575e8",
            "1cce0cdc28e9442e8d334408e22c14d7",
            "0c1aaadc4d514c878f6be620ebb0d6e7"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4804d4a46df4aa1b9b089e48e54aa82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db5a066686154f05997080e98ce59abc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90ba1c5997c54789b3e306db6bf7cdb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db16259e7cba44dfa636304774c2c6db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68bd97c79f7b4bceb203828a758d107e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e5131daf3fb4645a35c098f3db3f74b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae96aaef5684415f818413c0b3f2a4a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "<ipython-input-10-a5162468da6c>:7: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "llm = HuggingFacePipeline(\n",
        "   pipeline = pipe,\n",
        "   model_kwargs={\"temperature\": 0, \"max_length\": 2048},\n",
        ")\n",
        "#'HuggingFaceH4/zephyr-7b-beta'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvJhhDzYEvhu"
      },
      "source": [
        "### 7. Retrieve data and use it to answer a question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOQG22pHOf5W"
      },
      "source": [
        "![rag_workflow](https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/04-rag/rag_images/rag_workflow.png?raw=1)\n",
        "\n",
        "Image credit: https://blog.gopenai.com/retrieval-augmented-generation-101-de05e5dc21ef"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWJHxBc9hzJB"
      },
      "source": [
        "Let's ask questions it would only be able to know if the model actually read the texts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nMrN1_3B2cA1"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLCz3eiX2e0g",
        "outputId": "76f26dce-cd2a-4e70-dc6f-e9820def37a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-8e78b448a1f8>:8: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa_chain ({ \"query\" : \"What technique proposed in 2023 can be used to predict protein folding?\" })\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFdiffusion\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "  llm=llm,\n",
        "  chain_type=\"stuff\",\n",
        "  retriever=db.as_retriever(),\n",
        "  chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "result = qa_chain ({ \"query\" : \"What technique proposed in 2023 can be used to predict protein folding?\" })\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynkOkS39K2FT"
      },
      "source": [
        "Now let's ask the chain where to find the article related to RFDiffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWP0ZJ0Cc7uw",
        "outputId": "2ced79f6-1509-4cad-ac0c-661c25f88f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Which scientific article should I read to learn about RFdiffusion for protein folding?',\n",
              " 'result': 'Nature | Vol 620 | 31 August 2023 | 1091'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "qa_chain ({ \"query\" : \"Which scientific article should I read to learn about RFdiffusion for protein folding?\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f-vDxEaMT2_"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Use any of the frameworks/models here to load in your favorite websites and ask the model a question regarding them.\n",
        "\n",
        "Hint:\n",
        "```\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\"https://www.espn.com/\", \"https://www.google.com\"])\n",
        "\n",
        "To bypass SSL verification errors during fetching, you can set the “verify” option:\n",
        "\n",
        "loader.requests_kwargs = {‘verify’:False}\n",
        "\n",
        "data = loader.load()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbqQ2UQ7PGY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iviJn8067KQX"
      },
      "source": [
        "## More applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTNUrAkH9rsE"
      },
      "source": [
        "### RAG using Llama 2, Langchain and ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "omDQJW4E-D8D",
        "outputId": "f9704a6b-7844-4be7-92f0-90143516f7db",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting einops\n",
            "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.15-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: numpy in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from xformers) (1.26.4)\n",
            "Collecting torch==2.5.0 (from xformers)\n",
            "  Downloading torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch==2.5.0->xformers) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch==2.5.0->xformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch==2.5.0->xformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch==2.5.0->xformers) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/avasan/.local/lib/python3.12/site-packages (from torch==2.5.0->xformers) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.0->xformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.0->xformers)\n",
            "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: setuptools in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch==2.5.0->xformers) (69.5.1)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.0->xformers)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.5.0->xformers) (1.3.0)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /home/avasan/.local/lib/python3.12/site-packages (from chromadb) (2.9.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /home/avasan/.local/lib/python3.12/site-packages (from chromadb) (1.19.2)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /home/avasan/.local/lib/python3.12/site-packages (from chromadb) (0.20.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from chromadb) (4.66.2)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /home/avasan/.local/lib/python3.12/site-packages (from chromadb) (1.67.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting typer>=0.9.0 (from chromadb)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /home/avasan/.local/lib/python3.12/site-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /home/avasan/.local/lib/python3.12/site-packages (from chromadb) (3.10.10)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from chromadb) (0.27.0)\n",
            "Collecting rich>=10.11.0 (from chromadb)\n",
            "  Downloading rich-13.9.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\n",
            "Requirement already satisfied: certifi in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: idna in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
            "Requirement already satisfied: sniffio in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /home/avasan/.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /home/avasan/.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.35.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.31.0)\n",
            "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.1.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: coloredlogs in /home/avasan/.local/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /home/avasan/.local/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /home/avasan/.local/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /home/avasan/.local/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/avasan/.local/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/avasan/.local/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /home/avasan/.local/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /home/avasan/.local/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/avasan/.local/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.26.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/avasan/.local/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /home/avasan/.local/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-13.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/avasan/.local/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/avasan/.local/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/avasan/.local/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from requests->kubernetes>=28.1.0->chromadb) (2.0.4)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /home/avasan/.local/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from jinja2->torch==2.5.0->xformers) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/avasan/.local/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "Downloading xformers-0.0.28.post2-cp312-cp312-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.15-py3-none-any.whl (607 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.9.3-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading starlette-0.41.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=0b1c054c4c43be9d58050c2a329101ac9831b93c3c18c8365d136f52b64067a4\n",
            "  Stored in directory: /home/avasan/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, triton, sympy, shellingham, pyproject_hooks, protobuf, opentelemetry-util-http, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mmh3, mdurl, httptools, einops, chroma-hnswlib, bcrypt, asgiref, watchfiles, starlette, requests-oauthlib, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, build, rich, opentelemetry-semantic-conventions, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, kubernetes, fastapi, typer, torch, opentelemetry-sdk, opentelemetry-instrumentation-asgi, xformers, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, bitsandbytes, chromadb\n",
            "\u001b[33m  WARNING: The script uvicorn is installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script isympy is installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.28.3\n",
            "    Uninstalling protobuf-5.28.3:\n",
            "      Successfully uninstalled protobuf-5.28.3\n",
            "\u001b[33m  WARNING: The script watchfiles is installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script markdown-it is installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script pyproject-build is installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts opentelemetry-bootstrap and opentelemetry-instrument are installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script fastapi is installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script typer is installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2, torchfrtrace and torchrun are installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script chroma is installed in '/home/avasan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.67.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 bcrypt-4.2.0 bitsandbytes-0.44.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.15 durationpy-0.9 einops-0.8.0 fastapi-0.115.3 httptools-0.6.4 kubernetes-31.0.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.0.1 monotonic-1.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 posthog-3.7.0 protobuf-4.25.5 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 rich-13.9.3 shellingham-1.5.4 starlette-0.41.0 sympy-1.13.1 torch-2.5.0 triton-3.1.0 typer-0.12.5 uvicorn-0.32.0 uvloop-0.21.0 watchfiles-0.24.0 websockets-13.1 xformers-0.0.28.post2\n"
          ]
        }
      ],
      "source": [
        "!pip install einops xformers \\\n",
        "bitsandbytes chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP_FBWvnLVBx",
        "outputId": "63856b56-230a-4c55-f46e-0c434dcda69e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.27.0-py3-none-any.whl (279 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/279.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m276.5/279.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.27.0\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SzBLZA6-H1i"
      },
      "source": [
        "Load in models and setup pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgK306ND-pLc"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "#import chromadb\n",
        "#from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=False,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368,
          "referenced_widgets": [
            "8df15ee07d5b4f319135edbd1336f6bc",
            "b23c31e839cb4eb2912511925fd89b9f",
            "351778cfee7548368c3efd2a9fa45c4d",
            "40b0a738218b4d0d9b0f0136b61f5475",
            "c3e6a516af6b413aa722862306800aec",
            "34e7d064f786428b90ef133c026c1996",
            "d4620904edf142a492f2d7952336161b",
            "29cb8b36ac7a4c29bb03635d3a327673",
            "7cb5dc7be34a4c1f978ae0276eb056e1",
            "0e6e6b9ea3e0413aa348c204209be5de",
            "db9629950d8e4c779195afc8e1fdd121",
            "c4255efba6014f098e33cfcf5c2b0baf",
            "e5234b26e1634d239033398d900856a6",
            "bbb80dc24edd474db1ad8d52a11aee55",
            "c806af04c8bd42f29d722bab4b446290",
            "bfa0651a56b447c3b273615630382196",
            "dadb95c3acfc4c2899ceccb6d4b27864",
            "b3d019eb94554f62827a6e971597a15c",
            "c13aec6e4d0e44bfa32acc18bdf3f203",
            "d50455156a5a4d45a73a50cb7f416531",
            "f62635f7b37c43dda0f4a30794e60ed2",
            "8c60c4ae54cd476cb5b916dd9be59020",
            "24f381d25e1246fcb1dda99772a86a6d",
            "52897dd8be92400799b2e8faafff0280",
            "6da5c9f6a2c3416699a8a2d42a16700b",
            "093543555c124ec3b83d67d81ec0a894",
            "e3dc4f7b5f434308b8a3979f367e5cb5",
            "c83fdc64f38447c8899d0d63e4838f5b",
            "7623d841ae044f4b8acc549e8a703c82",
            "6cd26551e9904703bfad3d2b285bd995",
            "68326900690d4293826fe4df969488b7",
            "a15a50db09f849389a40645d3632aa30",
            "588b2fe072634cf88afb8ffb7f5c193f",
            "7042a066086143d5b581cbeab2e5a28f",
            "82c936a81cd241138b620e485619fb24",
            "f6d86fda3aec497aa7acf4451db0ed54",
            "56da550bc1794a9b98a4cce8c6e10c29",
            "1e51efd76d014283afbbe40da2e6551f",
            "6d2ef60932cc4eb98cad2c597b4b4530",
            "2559f7a616504ecebb0044ec1011e99a",
            "a1d4ef4d59bc49f0941422717e9ebccc",
            "e8ada17a8a8d40a5b60b4921897907aa",
            "a9bd2ae265154b769be5b5a23d9aa23e",
            "89d1eae4a1df4ec3b778328f7686caba",
            "5f10b36d4d4d41ad94d6bf9da79c1156",
            "ec28be3ab6324055b4ecbdb0912bbe68",
            "34a1ceeaf38345a4a1893fc6e9c05db7",
            "309a455e823445828819289f656210ee",
            "648f07aa15784e0b9d87f773f57cb4d2",
            "7bd68f3b0bab4f548b2aa6f927650b7b",
            "9a4f9bfb7bc64e83b66757c34875c756",
            "4365308bdcf04d6f8f369093c2d321ba",
            "9bec35b479314cbcaae2bbe11b9f2bdf",
            "abadb02290ce45abbcf152949b3734a1",
            "176988f6991d4d71b5c9a33321d2153e",
            "055004f61d074c27bca1d7fc5acb793b",
            "98022d05b6754ac09eef55e0d99e24fc",
            "fc4865f0d11d4d83bf4871cdde6cd556",
            "34bffb01bbf947f6b4aa0fa1c74474ae",
            "d8d5cb60c8f2420e8ffed156dcb9ccff",
            "0f72aca74fa74f8eb9241636c390cb06",
            "b6d1b668476048fabab9faeb90a79fd2",
            "15675159e5614d3aaeb1ac9a09b35599",
            "e24c7a7154c74190954aa79c72424b4a",
            "84d0d9b2a32c4317936bcc508c19599d",
            "542b4ce411244f0581eb3c164807ab03"
          ]
        },
        "id": "GO0N4dvX-Gkm",
        "outputId": "4f04b051-3ab5-42e1-fabc-6080ba0b64d3",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8df15ee07d5b4f319135edbd1336f6bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4255efba6014f098e33cfcf5c2b0baf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24f381d25e1246fcb1dda99772a86a6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7042a066086143d5b581cbeab2e5a28f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f10b36d4d4d41ad94d6bf9da79c1156",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "055004f61d074c27bca1d7fc5acb793b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepare model, tokenizer: 73.339 sec.\n"
          ]
        }
      ],
      "source": [
        "time_1 = time()\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "time_2 = time()\n",
        "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX_J_eSa_F9-",
        "outputId": "f7571df7-a03c-43cf-f2d7-e1ed4106b9b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepare pipeline: 12.021 sec.\n"
          ]
        }
      ],
      "source": [
        "time_1 = time()\n",
        "query_pipeline = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",)\n",
        "time_2 = time()\n",
        "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXra3Pac_YjE"
      },
      "source": [
        "Prepare function to test pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK9y4p7-_X9t"
      },
      "outputs": [],
      "source": [
        "def test_model(tokenizer, pipeline, prompt_to_test):\n",
        "    \"\"\"\n",
        "    Perform a query\n",
        "    print the result\n",
        "    Args:\n",
        "        tokenizer: the tokenizer\n",
        "        pipeline: the pipeline\n",
        "        prompt_to_test: the prompt\n",
        "    Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
        "    time_1 = time()\n",
        "    sequences = pipeline(\n",
        "        prompt_to_test,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=200,)\n",
        "    time_2 = time()\n",
        "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
        "    for seq in sequences:\n",
        "        print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceE5C8mU_hB3",
        "outputId": "bd57b862-e434-4ce7-daa4-966cc16ead1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test inference: 732.623 sec.\n",
            "Result: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\n",
            "The State of the Union address is an annual speech delivered by the President of the United States to Congress, in which the President reports on the state of the union and outlines legislative priorities for the upcoming year.\n"
          ]
        }
      ],
      "source": [
        "test_model(tokenizer,\n",
        "           query_pipeline,\n",
        "           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK4yMFFN_dwA"
      },
      "source": [
        "Set up Huggingface pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cqXPe5mZ_quI",
        "outputId": "bc93929d-8982-42ae-c986-60a0950ed911"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'HuggingFacePipeline' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dbf3961f56d7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFacePipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_pipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# checking again that everything is working fine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'HuggingFacePipeline' is not defined"
          ]
        }
      ],
      "source": [
        "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
        "# checking again that everything is working fine\n",
        "llm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1vvVRwV_06d"
      },
      "source": [
        "Load in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVn0hr1K_2cw"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"llm-workshop/tutorials/04-rag/state_union2023.txt\",\n",
        "                    encoding=\"utf8\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXcI1FMTAVwn"
      },
      "source": [
        "Chunk data recursively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0yCLaqbAXrr"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPolpwX5bQGs",
        "outputId": "f581ce39-6686-42f7-911b-06f2df77f142",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\\n\\nLast year COVID-19 kept us apart. This year we are finally together again.\\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.\\n\\nWith a duty to one another to the American people to the Constitution.\\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny.\\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated.\\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined.\\n\\nHe met the Ukrainian people.\\n\\nBiden condemns Putin in scathing State of the Union speech, in 180 seconds', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='SharePlay Video\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight.\\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world.\\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people.\\n\\nThroughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.\\n\\nThey keep moving.\\n\\nAnd the costs and the threats to America and the world keep rising.\\n\\nThat’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='The United States is a member along with 29 other nations.\\n\\nIt matters. American diplomacy matters. American resolve matters.\\n\\nPutin’s latest attack on Ukraine was premeditated and unprovoked.\\n\\nHe rejected repeated efforts at diplomacy.\\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready. Here is what we did.\\n\\nWe prepared extensively and carefully.\\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin.\\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.\\n\\nWe countered Russia’s lies with truth.\\n\\nAnd now that he has acted the free world is holding him accountable.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content=\"Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland.\\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever.\\n\\nTogether with our allies –we are right now enforcing powerful economic sanctions.\\n\\nWe are cutting off Russia’s largest banks from the international financial system.\\n\\n MOST READ\\nGettyImages-1998529084.jpg\\nDemocrats Might Need a Plan B. Here’s What It Looks Like.\\nGOP senators defy Trump by advancing foreign aid bill\\n‘Enough to make Reagan ill': Trump’s NATO remarks under fire\\nTrump to Taylor Swift: Don’t be ungrateful\\nRFK Jr. apologizes to family for Super Bowl ad\\nPreventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.\", metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='We are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.\\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more.\\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.\\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains.\\n\\nAnd tonight I am announcing that we will join our allies in closing off American air space to all Russian flights – further isolating Russia – and adding an additional squeeze –on their economy. The Ruble has lost 30% of its value.\\n\\nThe Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance.\\n\\nWe are giving more than $1 Billion in direct assistance to Ukraine.\\n\\nAnd we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.\\n\\nLet me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.\\n\\nOur forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west.\\n\\nFor that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia.\\n\\nAs I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='And we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.\\n\\nPutin has unleashed violence and chaos. But while he may make gains on the battlefield – he will pay a continuing high price over the long run.\\n\\nAnd a proud Ukrainian people, who have known 30 years of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.\\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world.\\n\\nAnd I’m taking robust action to make sure the pain of our sanctions is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers.\\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.\\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming.\\n\\nBut I want you to know that we are going to be okay.\\n\\nWhen the history of this era is written Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger.\\n\\nWhile it shouldn’t have taken something so terrible for people around the world to see what’s at stake now everyone sees it clearly.\\n\\nWe see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.\\n\\nIn the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='This is a real test. It’s going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people.\\n\\nTo our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you.\\n\\nPutin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people.\\n\\nHe will never extinguish their love of freedom. He will never weaken the resolve of the free world.\\n\\nWe meet tonight in an America that has lived through two of the hardest years this nation has ever faced.\\n\\nThe pandemic has been punishing.\\n\\nAnd so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more.\\n\\nI understand.\\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it.\\n\\nThat’s why one of the first things I did as President was fight to pass the American Rescue Plan.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Because people were hurting. We needed to act, and we did.\\n\\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis.\\n\\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.\\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance.\\n\\nAnd as my Dad used to say, it gave people a little breathing room.\\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind.\\n\\nAnd it worked. It created jobs. Lots of jobs.\\n\\nIn fact—our economy created over 6.5 Million new jobs just last year, more jobs created in one year\\nthan ever before in the history of America.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Our economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasn’t worked for the working people of this nation for too long.\\n\\nFor the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else.\\n\\nBut that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century.\\n\\nVice President Harris and I ran for office with a new economic vision for America.\\n\\nInvest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up\\nand the middle out, not from the top down.\\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well.\\n\\nAmerica used to have the best roads, bridges, and airports on Earth.\\n\\nNow our infrastructure is ranked 13th in the world.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='We won’t be able to compete for the jobs of the 21st Century if we don’t fix that.\\n\\nThat’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history.\\n\\nThis was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen.\\n\\nWe’re done talking about infrastructure weeks.\\n\\nWe’re going to have an infrastructure decade.\\n\\nIt is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world—particularly with China.\\n\\nAs I’ve told Xi Jinping, it is never a good bet to bet against the American people.\\n\\nWe’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America.\\n\\nAnd we’ll do it all to withstand the devastating effects of the climate crisis and promote environmental justice.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='We’ll build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipes—so every child—and every American—has clean water to drink at home and at school, provide affordable high-speed internet for every American—urban, suburban, rural, and tribal communities.\\n\\n4,000 projects have already been announced.\\n\\nAnd tonight, I’m announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair.\\n\\nWhen we use taxpayer dollars to rebuild America – we are going to Buy American: buy American products to support American jobs.\\n\\nThe federal government spends about $600 Billion a year to keep the country safe and secure.\\n\\nThere’s been a law on the books for almost a century\\nto make sure taxpayers’ dollars support American jobs and businesses.\\n\\nEvery Administration says they’ll do it, but we are actually doing it.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='We will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America.\\n\\nBut to compete for the best jobs of the future, we also need to level the playing field with China and other competitors.\\n\\nThat’s why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing.\\n\\nLet me give you one example of why it’s so important to pass it.\\n\\nIf you travel 20 miles east of Columbus, Ohio, you’ll find 1,000 empty acres of land.\\n\\nIt won’t look like much, but if you stop and look closely, you’ll see a “Field of dreams,” the ground on which America’s future will be built.\\n\\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor “mega site”.\\n\\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Some of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives.\\n\\nSmartphones. The Internet. Technology we have yet to invent.\\n\\nBut that’s just the beginning.\\n\\nIntel’s CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from\\n$20 billion to $100 billion.\\n\\nThat would be one of the biggest investments in manufacturing in American history.\\n\\nAnd all they’re waiting for is for you to pass this bill.\\n\\nSo let’s not wait any longer. Send it to my desk. I’ll sign it.\\n\\nAnd we will really take off.\\n\\nAnd Intel is not alone.\\n\\nThere’s something happening in America.\\n\\nJust look around and you’ll see an amazing story.\\n\\nThe rebirth of the pride that comes from stamping products “Made In America.” The revitalization of American manufacturing.\\n\\nCompanies are choosing to build new factories here, when just a few years ago, they would have built them overseas.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='That’s what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country.\\n\\nGM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan.\\n\\nAll told, we created 369,000 new manufacturing jobs in America just last year.\\n\\nPowered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight.\\n\\nAs Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.”\\n\\nIt’s time.\\n\\nBut with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.\\n\\nInflation is robbing them of the gains they might otherwise feel.\\n\\nI get it. That’s why my top priority is getting prices under control.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Look, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories.\\n\\nThe pandemic also disrupted global supply chains.\\n\\nWhen factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up.\\n\\nLook at cars.\\n\\nLast year, there weren’t enough semiconductors to make all the cars that people wanted to buy.\\n\\nAnd guess what, prices of automobiles went up.\\n\\nSo—we have a choice.\\n\\nOne way to fight inflation is to drive down wages and make Americans poorer.\\n\\nI have a better plan to fight inflation.\\n\\nLower your costs, not your wages.\\n\\nMake more cars and semiconductors in America.\\n\\nMore infrastructure and innovation in America.\\n\\nMore goods moving faster and cheaper in America.\\n\\nMore jobs where you can earn a good living in America.\\n\\nAnd instead of relying on foreign supply chains, let’s make it in America.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Economists call it “increasing the productive capacity of our economy.”\\n\\nI call it building a better America.\\n\\nMy plan to fight inflation will lower your costs and lower the deficit.\\n\\n17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here’s the plan:\\n\\nFirst – cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.\\n\\nHe and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.\\n\\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua’s mom.\\n\\nImagine what it’s like to look at your child who needs insulin and have no idea how you’re going to pay for it.\\n\\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Joshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.\\n\\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.\\n\\nDrug companies will still do very well. And while we’re at it let Medicare negotiate lower prices for prescription drugs, like the VA already does.\\n\\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent.\\n\\nSix things to know about State of the Union 2022\\n\\nSharePlay Video\\nSecond - cut energy costs for families an average of $500 a year by combatting climate change.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Let’s provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America’s clean energy production in solar, wind, and so much more; lower the price of electric vehicles, saving you another $80 a month because you’ll never have to pay at the gas pump again.\\n\\nThird – cut the cost of child care. Many families pay up to $14,000 a year for child care per child.\\n\\nMiddle-class and working families shouldn’t have to pay more than 7% of their income for care of young children.\\n\\nMy plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldn’t afford child care, to be able to get back to work.\\n\\nMy plan doesn’t stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.\\n\\nAll of these will lower costs.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='And under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.\\n\\nThe one thing all Americans agree on is that the tax system is not fair. We have to fix it.\\n\\nI’m not looking to punish anyone. But let’s make sure corporations and the wealthiest Americans start paying their fair share.\\n\\nJust last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.\\n\\nThat’s simply not fair. That’s why I’ve proposed a 15% minimum tax rate for corporations.\\n\\nWe got more than 130 countries to agree on a global minimum tax rate so companies can’t get out of paying their taxes at home by shipping jobs and factories overseas.\\n\\nThat’s why I’ve proposed closing loopholes so the very wealthy don’t pay a lower tax rate than a teacher or a firefighter.\\n\\nSo that’s my plan. It will grow the economy and lower costs for families.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='So what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.\\n\\nMy plan will not only lower costs to give families a fair shot, it will lower the deficit.\\n\\nThe previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted.\\n\\nBut in my administration, the watchdogs have been welcomed back.\\n\\nWe’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.\\n\\nAnd tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud.\\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.\\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Lowering your costs also means demanding more competition.\\n\\nI’m a capitalist, but capitalism without competition isn’t capitalism.\\n\\nIt’s exploitation—and it drives up prices.\\n\\nWhen corporations don’t have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under.\\n\\nWe see it happening with ocean carriers moving goods in and out of America.\\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits.\\n\\nTonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers.\\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.\\n\\nThat ends on my watch.\\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='We’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees.\\n\\nLet’s pass the Paycheck Fairness Act and paid leave.\\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty.\\n\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.\\n\\nAnd let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.\\n\\nWhen we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America.\\n\\nFor more than two years, COVID-19 has impacted every decision in our lives and the life of the nation.\\n\\nAnd I know you’re tired, frustrated, and exhausted.\\n\\nBut I also know this.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Because of the progress we’ve made, because of your resilience and the tools we have, tonight I can say\\nwe are moving forward safely, back to more normal routines.\\n\\nWe’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.\\n\\nJust a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines.\\n\\nUnder these new guidelines, most Americans in most of the country can now be mask free.\\n\\nAnd based on the projections, more of the country will reach that point across the next couple of weeks.\\n\\nThanks to the progress we have made this past year, COVID-19 need no longer control our lives.\\n\\nI know some are talking about “living with COVID-19”. Tonight – I say that we will never just accept living with COVID-19.\\n\\nWe will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Here are four common sense steps as we move forward safely.\\n\\nFirst, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If you’re vaccinated and boosted you have the highest degree of protection.\\n\\nWe will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children.\\n\\nThe scientists are working hard to get that done and we’ll be ready with plenty of vaccines when they do.\\n\\nWe’re also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.\\n\\nWe’ve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.\\n\\nAnd we’re launching the “Test to Treat” initiative so people can get tested at a pharmacy, and if they’re positive, receive antiviral pills on the spot at no cost.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='If you’re immunocompromised or have some other vulnerability, we have treatments and free high-quality masks.\\n\\nWe’re leaving no one behind or ignoring anyone’s needs as we move forward.\\n\\nAnd on testing, we have made hundreds of millions of tests available for you to order for free.\\n\\nEven if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week.\\n\\nSecond – we must prepare for new variants. Over the past year, we’ve gotten much better at detecting new variants.\\n\\nIf necessary, we’ll be able to deploy new vaccines within 100 days instead of many more months or years.\\n\\nAnd, if Congress provides the funds we need, we’ll have new stockpiles of tests, masks, and pills ready if needed.\\n\\nI cannot promise a new variant won’t come. But I can promise you we’ll do everything within our power to be ready if it does.\\n\\nThird – we can end the shutdown of schools and businesses. We have the tools we need.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='It’s time for Americans to get back to work and fill our great downtowns again. People working from home can feel safe to begin to return to the office.\\n\\nWe’re doing that here in the federal government. The vast majority of federal workers will once again work in person.\\n\\nOur schools are open. Let’s keep it that way. Our kids need to be in school.\\n\\nAnd with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely.\\n\\nWe achieved this because we provided free vaccines, treatments, tests, and masks.\\n\\nOf course, continuing this costs money.\\n\\nI will soon send Congress a request.\\n\\nThe vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.\\n\\nFourth, we will continue vaccinating the world.\\n\\nWe’ve sent 475 Million vaccine doses to 112 countries, more than any other nation.\\n\\nAnd we won’t stop.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='And we won’t stop.\\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life.\\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.\\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.\\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together.\\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera.\\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun.\\n\\nOfficer Mora was 27 years old.\\n\\nOfficer Rivera was 22.\\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\n\\nI’ve worked on these issues a long time.\\n\\nI know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.\\n\\nSo let’s not abandon our streets. Or choose between safety and equal justice.\\n\\nLet’s come together to protect our communities, restore trust, and hold law enforcement accountable.\\n\\nThat’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers.\\n\\nThat’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='We should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities.\\n\\nI ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.\\n\\nAnd I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home—they have no serial numbers and can’t be traced.\\n\\nAnd I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon?\\n\\nBan assault weapons and high-capacity magazines.\\n\\nRepeal the liability shield that makes gun manufacturers the only industry in America that can’t be sued.\\n\\nThese laws don’t infringe on the Second Amendment. They save lives.\\n\\nThe most fundamental right in America is the right to vote – and to have it counted. And it’s under assault.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections.\\n\\nWe cannot let this happen.\\n\\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.\\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.\\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.\\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.\\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='We can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land—my forefathers and so many of yours.\\n\\nProvide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers.\\n\\nRevise our laws so businesses have the workers they need and families don’t wait decades to reunite.\\n\\nIt’s not only the right thing to do—it’s the economically smart thing to do.\\n\\nThat’s why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce.\\n\\nLet’s get it done once and for all.\\n\\nAdvancing liberty and justice also requires protecting the rights of women.\\n\\nThe constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before.\\n\\nIf we want to go forward—not backward—we must protect access to health care. Preserve a woman’s right to choose. And let’s continue to advance maternal health care in America.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong.\\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.\\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.\\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things.\\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.\\n\\nFirst, beat the opioid epidemic.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='There is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.\\n\\nGet rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers.\\n\\nIf you’re suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery.\\n\\nSecond, let’s take on mental health. Especially among our children, whose lives and education have been turned upside down.\\n\\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.\\n\\nI urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor.\\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='As Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they’re conducting on our children for profit.\\n\\nIt’s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children.\\n\\nAnd let’s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care.\\n\\nThird, support our veterans.\\n\\nVeterans are the best of us.\\n\\nI’ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home.\\n\\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.\\n\\nOur troops in Iraq and Afghanistan faced many dangers.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='One was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more.\\n\\nWhen they came home, many of the world’s fittest and best trained warriors were never the same.\\n\\nHeadaches. Numbness. Dizziness.\\n\\nA cancer that would put them in a flag-draped coffin.\\n\\nI know.\\n\\nOne of those soldiers was my son Major Beau Biden.\\n\\nWe don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops.\\n\\nBut I’m committed to finding out everything we can.\\n\\nCommitted to military families like Danielle Robinson from Ohio.\\n\\nThe widow of Sergeant First Class Heath Robinson.\\n\\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq.\\n\\nStationed near Baghdad, just yards from burn pits the size of football fields.\\n\\nHeath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='But cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body.\\n\\nDanielle says Heath was a fighter to the very end.\\n\\nHe didn’t know how to stop fighting, and neither did she.\\n\\nThrough her pain she found purpose to demand we do better.\\n\\nTonight, Danielle—we are.\\n\\nThe VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits.\\n\\nAnd tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers.\\n\\nI’m also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve.\\n\\nAnd fourth, let’s end cancer as we know it.\\n\\nThis is personal to me and Jill, to Kamala, and to so many of you.\\n\\nCancer is the #2 cause of death in America–second only to heart disease.\\n\\nLast month, I announced our plan to supercharge\\nthe Cancer Moonshot that President Obama asked me to lead six years ago.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Our goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.\\n\\nMore support for patients and families.\\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health.\\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.\\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more.\\n\\nA unity agenda for the nation.\\n\\nWe can do this.\\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy.\\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things.\\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror.\\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known.\\n\\nNow is the hour.\\n\\nOur moment of responsibility.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='Our test of resolve and conscience, of history itself.\\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged.\\n\\nWell I know this nation.\\n\\nWe will meet the test.\\n\\nTo protect freedom and liberty, to expand fairness and opportunity.\\n\\nWe will save democracy.\\n\\nAs hard as these times have been, I am more optimistic about America today than I have been my whole life.\\n\\nBecause I see the future that is within our grasp.\\n\\nBecause I know there is simply nothing beyond our capacity.\\n\\nWe are the only nation on Earth that has always turned every crisis we have faced into an opportunity.\\n\\nThe only nation that can be defined by a single word: possibilities.\\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union.\\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong.\\n\\nWe are stronger today than we were a year ago.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'}),\n",
              " Document(page_content='And we will be stronger a year from now than we are today.\\n\\nNow is our moment to meet and overcome the challenges of our time.\\n\\nAnd we will, as one people.\\n\\nOne America.\\n\\nThe United States of America.\\n\\nMay God bless you all. May God protect our troops.', metadata={'source': 'llm-workshop/tutorials/04-rag/state_union2023.txt'})]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMMRTYEAZEx"
      },
      "source": [
        "Embed and store in Chroma Vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPX2j0W0AnRS"
      },
      "outputs": [],
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device':'cpu'}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gZKaDW5BDDJ"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGKlWcClBJPX"
      },
      "source": [
        "Set up chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tHJsc-nAuyU"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEosQNUUBaEI"
      },
      "source": [
        "Test RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29bx7cTTBcBE"
      },
      "outputs": [],
      "source": [
        "def test_rag(qa, query):\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    time_1 = time()\n",
        "    result = qa.run(query)\n",
        "    time_2 = time()\n",
        "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
        "    print(\"\\nResult: \", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61QmD99hBtsh"
      },
      "source": [
        "Get sources..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5smrotNBwBG"
      },
      "outputs": [],
      "source": [
        "docs = vectordb.similarity_search(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Retrieved documents: {len(docs)}\")\n",
        "for doc in docs:\n",
        "    doc_details = doc.to_json()['kwargs']\n",
        "    print(\"Source: \", doc_details['metadata']['source'])\n",
        "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIPlwM3-BgYT"
      },
      "outputs": [],
      "source": [
        "query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\n",
        "test_rag(qa, query)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (pytorch)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "055004f61d074c27bca1d7fc5acb793b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98022d05b6754ac09eef55e0d99e24fc",
              "IPY_MODEL_fc4865f0d11d4d83bf4871cdde6cd556",
              "IPY_MODEL_34bffb01bbf947f6b4aa0fa1c74474ae"
            ],
            "layout": "IPY_MODEL_d8d5cb60c8f2420e8ffed156dcb9ccff"
          }
        },
        "093543555c124ec3b83d67d81ec0a894": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a15a50db09f849389a40645d3632aa30",
            "placeholder": "​",
            "style": "IPY_MODEL_588b2fe072634cf88afb8ffb7f5c193f",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 114kB/s]"
          }
        },
        "0e6e6b9ea3e0413aa348c204209be5de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f72aca74fa74f8eb9241636c390cb06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15675159e5614d3aaeb1ac9a09b35599": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "176988f6991d4d71b5c9a33321d2153e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e51efd76d014283afbbe40da2e6551f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24f381d25e1246fcb1dda99772a86a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52897dd8be92400799b2e8faafff0280",
              "IPY_MODEL_6da5c9f6a2c3416699a8a2d42a16700b",
              "IPY_MODEL_093543555c124ec3b83d67d81ec0a894"
            ],
            "layout": "IPY_MODEL_e3dc4f7b5f434308b8a3979f367e5cb5"
          }
        },
        "2559f7a616504ecebb0044ec1011e99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29cb8b36ac7a4c29bb03635d3a327673": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309a455e823445828819289f656210ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abadb02290ce45abbcf152949b3734a1",
            "placeholder": "​",
            "style": "IPY_MODEL_176988f6991d4d71b5c9a33321d2153e",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 2.45MB/s]"
          }
        },
        "34a1ceeaf38345a4a1893fc6e9c05db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4365308bdcf04d6f8f369093c2d321ba",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bec35b479314cbcaae2bbe11b9f2bdf",
            "value": 1842767
          }
        },
        "34bffb01bbf947f6b4aa0fa1c74474ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d0d9b2a32c4317936bcc508c19599d",
            "placeholder": "​",
            "style": "IPY_MODEL_542b4ce411244f0581eb3c164807ab03",
            "value": " 414/414 [00:00&lt;00:00, 28.6kB/s]"
          }
        },
        "34e7d064f786428b90ef133c026c1996": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351778cfee7548368c3efd2a9fa45c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29cb8b36ac7a4c29bb03635d3a327673",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cb5dc7be34a4c1f978ae0276eb056e1",
            "value": 2
          }
        },
        "40b0a738218b4d0d9b0f0136b61f5475": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e6e6b9ea3e0413aa348c204209be5de",
            "placeholder": "​",
            "style": "IPY_MODEL_db9629950d8e4c779195afc8e1fdd121",
            "value": " 2/2 [01:03&lt;00:00, 29.29s/it]"
          }
        },
        "4365308bdcf04d6f8f369093c2d321ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52897dd8be92400799b2e8faafff0280": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c83fdc64f38447c8899d0d63e4838f5b",
            "placeholder": "​",
            "style": "IPY_MODEL_7623d841ae044f4b8acc549e8a703c82",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "542b4ce411244f0581eb3c164807ab03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56da550bc1794a9b98a4cce8c6e10c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9bd2ae265154b769be5b5a23d9aa23e",
            "placeholder": "​",
            "style": "IPY_MODEL_89d1eae4a1df4ec3b778328f7686caba",
            "value": " 500k/500k [00:00&lt;00:00, 19.7MB/s]"
          }
        },
        "588b2fe072634cf88afb8ffb7f5c193f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f10b36d4d4d41ad94d6bf9da79c1156": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec28be3ab6324055b4ecbdb0912bbe68",
              "IPY_MODEL_34a1ceeaf38345a4a1893fc6e9c05db7",
              "IPY_MODEL_309a455e823445828819289f656210ee"
            ],
            "layout": "IPY_MODEL_648f07aa15784e0b9d87f773f57cb4d2"
          }
        },
        "648f07aa15784e0b9d87f773f57cb4d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68326900690d4293826fe4df969488b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cd26551e9904703bfad3d2b285bd995": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d2ef60932cc4eb98cad2c597b4b4530": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6da5c9f6a2c3416699a8a2d42a16700b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cd26551e9904703bfad3d2b285bd995",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68326900690d4293826fe4df969488b7",
            "value": 1618
          }
        },
        "7042a066086143d5b581cbeab2e5a28f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82c936a81cd241138b620e485619fb24",
              "IPY_MODEL_f6d86fda3aec497aa7acf4451db0ed54",
              "IPY_MODEL_56da550bc1794a9b98a4cce8c6e10c29"
            ],
            "layout": "IPY_MODEL_1e51efd76d014283afbbe40da2e6551f"
          }
        },
        "7623d841ae044f4b8acc549e8a703c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bd68f3b0bab4f548b2aa6f927650b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cb5dc7be34a4c1f978ae0276eb056e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82c936a81cd241138b620e485619fb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d2ef60932cc4eb98cad2c597b4b4530",
            "placeholder": "​",
            "style": "IPY_MODEL_2559f7a616504ecebb0044ec1011e99a",
            "value": "tokenizer.model: 100%"
          }
        },
        "84d0d9b2a32c4317936bcc508c19599d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89d1eae4a1df4ec3b778328f7686caba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c60c4ae54cd476cb5b916dd9be59020": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8df15ee07d5b4f319135edbd1336f6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b23c31e839cb4eb2912511925fd89b9f",
              "IPY_MODEL_351778cfee7548368c3efd2a9fa45c4d",
              "IPY_MODEL_40b0a738218b4d0d9b0f0136b61f5475"
            ],
            "layout": "IPY_MODEL_c3e6a516af6b413aa722862306800aec"
          }
        },
        "98022d05b6754ac09eef55e0d99e24fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f72aca74fa74f8eb9241636c390cb06",
            "placeholder": "​",
            "style": "IPY_MODEL_b6d1b668476048fabab9faeb90a79fd2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "9a4f9bfb7bc64e83b66757c34875c756": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bec35b479314cbcaae2bbe11b9f2bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a15a50db09f849389a40645d3632aa30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d4ef4d59bc49f0941422717e9ebccc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9bd2ae265154b769be5b5a23d9aa23e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abadb02290ce45abbcf152949b3734a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b23c31e839cb4eb2912511925fd89b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34e7d064f786428b90ef133c026c1996",
            "placeholder": "​",
            "style": "IPY_MODEL_d4620904edf142a492f2d7952336161b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b3d019eb94554f62827a6e971597a15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6d1b668476048fabab9faeb90a79fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbb80dc24edd474db1ad8d52a11aee55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c13aec6e4d0e44bfa32acc18bdf3f203",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d50455156a5a4d45a73a50cb7f416531",
            "value": 188
          }
        },
        "bfa0651a56b447c3b273615630382196": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13aec6e4d0e44bfa32acc18bdf3f203": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3e6a516af6b413aa722862306800aec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4255efba6014f098e33cfcf5c2b0baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5234b26e1634d239033398d900856a6",
              "IPY_MODEL_bbb80dc24edd474db1ad8d52a11aee55",
              "IPY_MODEL_c806af04c8bd42f29d722bab4b446290"
            ],
            "layout": "IPY_MODEL_bfa0651a56b447c3b273615630382196"
          }
        },
        "c806af04c8bd42f29d722bab4b446290": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f62635f7b37c43dda0f4a30794e60ed2",
            "placeholder": "​",
            "style": "IPY_MODEL_8c60c4ae54cd476cb5b916dd9be59020",
            "value": " 188/188 [00:00&lt;00:00, 10.8kB/s]"
          }
        },
        "c83fdc64f38447c8899d0d63e4838f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4620904edf142a492f2d7952336161b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d50455156a5a4d45a73a50cb7f416531": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8d5cb60c8f2420e8ffed156dcb9ccff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dadb95c3acfc4c2899ceccb6d4b27864": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9629950d8e4c779195afc8e1fdd121": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e24c7a7154c74190954aa79c72424b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3dc4f7b5f434308b8a3979f367e5cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5234b26e1634d239033398d900856a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dadb95c3acfc4c2899ceccb6d4b27864",
            "placeholder": "​",
            "style": "IPY_MODEL_b3d019eb94554f62827a6e971597a15c",
            "value": "generation_config.json: 100%"
          }
        },
        "e8ada17a8a8d40a5b60b4921897907aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec28be3ab6324055b4ecbdb0912bbe68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bd68f3b0bab4f548b2aa6f927650b7b",
            "placeholder": "​",
            "style": "IPY_MODEL_9a4f9bfb7bc64e83b66757c34875c756",
            "value": "tokenizer.json: 100%"
          }
        },
        "f62635f7b37c43dda0f4a30794e60ed2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6d86fda3aec497aa7acf4451db0ed54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d4ef4d59bc49f0941422717e9ebccc",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8ada17a8a8d40a5b60b4921897907aa",
            "value": 499723
          }
        },
        "fc4865f0d11d4d83bf4871cdde6cd556": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15675159e5614d3aaeb1ac9a09b35599",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e24c7a7154c74190954aa79c72424b4a",
            "value": 414
          }
        },
        "50ab3c6b796a417f82eae97fdff209a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92ead2409a454641895e0250bdcfef9e",
              "IPY_MODEL_2736a3bcfeed4d0ca1e4b057b6ba263c",
              "IPY_MODEL_96a48c244bde44f783e17d2fb09ab30a"
            ],
            "layout": "IPY_MODEL_9d2dca32e0b04c7a830b2ea6b6709d3b"
          }
        },
        "92ead2409a454641895e0250bdcfef9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d0d1b4d8e664053b5ecc9ac18ae4d95",
            "placeholder": "​",
            "style": "IPY_MODEL_a351e18bdd13415a8bfc2925792144e8",
            "value": "modules.json: 100%"
          }
        },
        "2736a3bcfeed4d0ca1e4b057b6ba263c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a51867e85bb4ca091b342d3b444ba0e",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_767b536a3bc546119278b81576d373f4",
            "value": 349
          }
        },
        "96a48c244bde44f783e17d2fb09ab30a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_944d4d22802d48a79e95bd454b227af0",
            "placeholder": "​",
            "style": "IPY_MODEL_08d8aa99df3a4eca9beb3795df5a9896",
            "value": " 349/349 [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "9d2dca32e0b04c7a830b2ea6b6709d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d0d1b4d8e664053b5ecc9ac18ae4d95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a351e18bdd13415a8bfc2925792144e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a51867e85bb4ca091b342d3b444ba0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "767b536a3bc546119278b81576d373f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "944d4d22802d48a79e95bd454b227af0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d8aa99df3a4eca9beb3795df5a9896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "391271ffe92c4c5f9293704f0e44baa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b8d5a71094e4961a9bb589dbc50fc91",
              "IPY_MODEL_5fc35bc7eb0c477bac62296252729b14",
              "IPY_MODEL_99e27e3724b149daaea6635184b47c4c"
            ],
            "layout": "IPY_MODEL_1ecc8143b4d84c9a890d2f0b90e3ed01"
          }
        },
        "4b8d5a71094e4961a9bb589dbc50fc91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f97a270a971f438483be1b55ad73108b",
            "placeholder": "​",
            "style": "IPY_MODEL_683589aa5e8442e59a2fa2463cdb72d2",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "5fc35bc7eb0c477bac62296252729b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcc4bdcaf218483e95e14789c121d065",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0dc6b70c3df41deaee6ffd21bac52c9",
            "value": 116
          }
        },
        "99e27e3724b149daaea6635184b47c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f8b307ac7954fe2a24179a409abb5fd",
            "placeholder": "​",
            "style": "IPY_MODEL_0837819fda384c4cabd5276f62733c65",
            "value": " 116/116 [00:00&lt;00:00, 8.35kB/s]"
          }
        },
        "1ecc8143b4d84c9a890d2f0b90e3ed01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f97a270a971f438483be1b55ad73108b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "683589aa5e8442e59a2fa2463cdb72d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcc4bdcaf218483e95e14789c121d065": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0dc6b70c3df41deaee6ffd21bac52c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f8b307ac7954fe2a24179a409abb5fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0837819fda384c4cabd5276f62733c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c98d05af6a244d6fa8e362fb5ff3cf4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e59501eda6314551a58a62a376e2fb40",
              "IPY_MODEL_0b03bd364ab74b63be48cd36638af1ad",
              "IPY_MODEL_9481a37e853848fd95d65adffbdbdea6"
            ],
            "layout": "IPY_MODEL_5df3f5f0058e48e1b9f0936989c10003"
          }
        },
        "e59501eda6314551a58a62a376e2fb40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdefd91bd9024e7980ce1b2e4de212a7",
            "placeholder": "​",
            "style": "IPY_MODEL_116b5c29f916401b8cda95433769823b",
            "value": "README.md: 100%"
          }
        },
        "0b03bd364ab74b63be48cd36638af1ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eeb2e05033c41648e2465bb597c8d94",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b3c747f8a7e4326b3eae01c5b809851",
            "value": 10659
          }
        },
        "9481a37e853848fd95d65adffbdbdea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04c5866455bc460c91672fdf5e62af49",
            "placeholder": "​",
            "style": "IPY_MODEL_3a51a4709d5a455ab588e1121890726d",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 418kB/s]"
          }
        },
        "5df3f5f0058e48e1b9f0936989c10003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdefd91bd9024e7980ce1b2e4de212a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "116b5c29f916401b8cda95433769823b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eeb2e05033c41648e2465bb597c8d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b3c747f8a7e4326b3eae01c5b809851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04c5866455bc460c91672fdf5e62af49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a51a4709d5a455ab588e1121890726d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b956c7fa66e145bd9a6f1089b6fbd1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b328001cb684d2c9bfc91ed9f335141",
              "IPY_MODEL_460195c92d83417191621463926bf878",
              "IPY_MODEL_bdf62004be78469cb077718421936ea7"
            ],
            "layout": "IPY_MODEL_6b671e2331d54ef1a19e1622352ca8f0"
          }
        },
        "5b328001cb684d2c9bfc91ed9f335141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f738ea2ecb4d589c7bc188a974f875",
            "placeholder": "​",
            "style": "IPY_MODEL_e01a0891183f41b9b8cb7df660ee6dcb",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "460195c92d83417191621463926bf878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_209b5810fe114d6d8509265cf1aa42bb",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8d09f7ae86e4082856f5244b27962a9",
            "value": 53
          }
        },
        "bdf62004be78469cb077718421936ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b57d16116a04fa0a7e4769c79e19480",
            "placeholder": "​",
            "style": "IPY_MODEL_b0877b9497724e00bf4e539751a93cfe",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.86kB/s]"
          }
        },
        "6b671e2331d54ef1a19e1622352ca8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f738ea2ecb4d589c7bc188a974f875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e01a0891183f41b9b8cb7df660ee6dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "209b5810fe114d6d8509265cf1aa42bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8d09f7ae86e4082856f5244b27962a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b57d16116a04fa0a7e4769c79e19480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0877b9497724e00bf4e539751a93cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9516be91ee184931b721f27b5421fc39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d689173163b8414dabae7c8e070189c8",
              "IPY_MODEL_e1cd19fc345a46808ed2137c66e9f648",
              "IPY_MODEL_114f8d5b692249a9a5e231409f42c347"
            ],
            "layout": "IPY_MODEL_97df4be860e441ff82656478d73152d5"
          }
        },
        "d689173163b8414dabae7c8e070189c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3d56be0e1ed4157939479c14eb25d59",
            "placeholder": "​",
            "style": "IPY_MODEL_b307d2019b6548158dfd9eb2e2d81266",
            "value": "config.json: 100%"
          }
        },
        "e1cd19fc345a46808ed2137c66e9f648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f807016b612e4c01aa0b4698b370a722",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efa5edbd2d754094a1979c2b74f5d133",
            "value": 612
          }
        },
        "114f8d5b692249a9a5e231409f42c347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f57b73719b848c792e22ebed6301905",
            "placeholder": "​",
            "style": "IPY_MODEL_41fc56daebe14c5baa8b36107d7175b8",
            "value": " 612/612 [00:00&lt;00:00, 27.4kB/s]"
          }
        },
        "97df4be860e441ff82656478d73152d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d56be0e1ed4157939479c14eb25d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b307d2019b6548158dfd9eb2e2d81266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f807016b612e4c01aa0b4698b370a722": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efa5edbd2d754094a1979c2b74f5d133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f57b73719b848c792e22ebed6301905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41fc56daebe14c5baa8b36107d7175b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5c579dcc4c84dfb83aaabc416c6a9e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22fcd9377d244129acd96f46e3bc17e7",
              "IPY_MODEL_5febe76d866741bda6fded73a471049b",
              "IPY_MODEL_b4119fda1ac14ca596654075c37ff420"
            ],
            "layout": "IPY_MODEL_1c539a577435446ebf2d599e1f777630"
          }
        },
        "22fcd9377d244129acd96f46e3bc17e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b5751aeb9934365ae60003114e5f593",
            "placeholder": "​",
            "style": "IPY_MODEL_e8a92a347fd942bd8776b6167571bcf1",
            "value": "model.safetensors: 100%"
          }
        },
        "5febe76d866741bda6fded73a471049b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d7690e79c34e3b86fb898cecf2ca4c",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9584d33540a7442d937513363c72a0ab",
            "value": 90868376
          }
        },
        "b4119fda1ac14ca596654075c37ff420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_801dd358909843f0bbbada7d10099ca7",
            "placeholder": "​",
            "style": "IPY_MODEL_f03aacd78c5040619e5d89a9475af1d5",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 212MB/s]"
          }
        },
        "1c539a577435446ebf2d599e1f777630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b5751aeb9934365ae60003114e5f593": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8a92a347fd942bd8776b6167571bcf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7d7690e79c34e3b86fb898cecf2ca4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9584d33540a7442d937513363c72a0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "801dd358909843f0bbbada7d10099ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f03aacd78c5040619e5d89a9475af1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab86031acdc749e9b31bc1698a635ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2d4d8658aa94155bd4a92ad114c51b8",
              "IPY_MODEL_bed8e2726d8840d2b08a5f129dc19bc1",
              "IPY_MODEL_b9a70ae5d92a465b980845e87a67f1e4"
            ],
            "layout": "IPY_MODEL_c38415d2e45d4ff7ba871d99f9b1b523"
          }
        },
        "e2d4d8658aa94155bd4a92ad114c51b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa81521a40d540abbdd2564fe70a954d",
            "placeholder": "​",
            "style": "IPY_MODEL_daca27b9821d4badb7243649d9a3c360",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "bed8e2726d8840d2b08a5f129dc19bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecf056fc7070415fb83ea4deb3887583",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fb2b2c4b2954480955dc530cc0e7905",
            "value": 350
          }
        },
        "b9a70ae5d92a465b980845e87a67f1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_403be7357b3f40d6a8a6f428aed102f2",
            "placeholder": "​",
            "style": "IPY_MODEL_57afef0e704a48e5b57752f5092070a8",
            "value": " 350/350 [00:00&lt;00:00, 18.5kB/s]"
          }
        },
        "c38415d2e45d4ff7ba871d99f9b1b523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa81521a40d540abbdd2564fe70a954d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daca27b9821d4badb7243649d9a3c360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecf056fc7070415fb83ea4deb3887583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb2b2c4b2954480955dc530cc0e7905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "403be7357b3f40d6a8a6f428aed102f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57afef0e704a48e5b57752f5092070a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51e3d6c4048348f9bd6d878360774fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d132b6c6841423080b69c7ea91b4c05",
              "IPY_MODEL_7166a3209cf544308dd965da2939380a",
              "IPY_MODEL_35d99e80f961460588173e44abf8cef8"
            ],
            "layout": "IPY_MODEL_b1a6e12f57b74c3892be626f1d088b49"
          }
        },
        "2d132b6c6841423080b69c7ea91b4c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6accc604666140efad6a0b7b6054d3e3",
            "placeholder": "​",
            "style": "IPY_MODEL_6b1cdd7ee89840a99f99c0bb865e58d2",
            "value": "vocab.txt: 100%"
          }
        },
        "7166a3209cf544308dd965da2939380a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8062ce6a0fc1414e919c576e68dea7ce",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de8aba8f805b4c0fb5579f23e9a9de5c",
            "value": 231508
          }
        },
        "35d99e80f961460588173e44abf8cef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7687eb21dec4e2cb049fd12a7a2b9a2",
            "placeholder": "​",
            "style": "IPY_MODEL_cc12a4910eec435ba623c63822953be4",
            "value": " 232k/232k [00:00&lt;00:00, 4.20MB/s]"
          }
        },
        "b1a6e12f57b74c3892be626f1d088b49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6accc604666140efad6a0b7b6054d3e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1cdd7ee89840a99f99c0bb865e58d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8062ce6a0fc1414e919c576e68dea7ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de8aba8f805b4c0fb5579f23e9a9de5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7687eb21dec4e2cb049fd12a7a2b9a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc12a4910eec435ba623c63822953be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef2508cbbff842c28ca964a949c0f64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53478ed6b90c4e2195cd263f63923478",
              "IPY_MODEL_1b03b0db59df438db1d73f0e893ab8da",
              "IPY_MODEL_b5146d3bbdc14737bb71f62dcd71ea24"
            ],
            "layout": "IPY_MODEL_dcbd16e6845a4eb68acf1f344bc2ff8f"
          }
        },
        "53478ed6b90c4e2195cd263f63923478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11fdd17b1f0e4a349e58f757586bd6c1",
            "placeholder": "​",
            "style": "IPY_MODEL_b4a5837db14048f28a54c9ec825f3440",
            "value": "tokenizer.json: 100%"
          }
        },
        "1b03b0db59df438db1d73f0e893ab8da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d126a7834be44b6a81094298433aa126",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_671f20dd28b2405f8fc8e3380d47f757",
            "value": 466247
          }
        },
        "b5146d3bbdc14737bb71f62dcd71ea24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bb4bacbb11a4f40af7414cd10d3aec6",
            "placeholder": "​",
            "style": "IPY_MODEL_2be6a69e298846f6ba91cfb3633eab1c",
            "value": " 466k/466k [00:00&lt;00:00, 5.51MB/s]"
          }
        },
        "dcbd16e6845a4eb68acf1f344bc2ff8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11fdd17b1f0e4a349e58f757586bd6c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a5837db14048f28a54c9ec825f3440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d126a7834be44b6a81094298433aa126": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "671f20dd28b2405f8fc8e3380d47f757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bb4bacbb11a4f40af7414cd10d3aec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2be6a69e298846f6ba91cfb3633eab1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6195f1f406a343e1b731efce4a5a95b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89e343f8574a4496931c22756f4bca83",
              "IPY_MODEL_6e55bddd29af4579b721c16a2fc63e1c",
              "IPY_MODEL_3441ed26851742819018b4abbe19e212"
            ],
            "layout": "IPY_MODEL_807bca63da8e445099de076866aff172"
          }
        },
        "89e343f8574a4496931c22756f4bca83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee3f6e207e04250b5320ff30088ab23",
            "placeholder": "​",
            "style": "IPY_MODEL_be489b0db48a403e8cbf868c372c2e4e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "6e55bddd29af4579b721c16a2fc63e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b6641a6c9448498b444d14e668221a",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b412f6f12a84414eb0dc5e900671d436",
            "value": 112
          }
        },
        "3441ed26851742819018b4abbe19e212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2509b738f9f4d94928304c5a46d86f4",
            "placeholder": "​",
            "style": "IPY_MODEL_1df2254fe3504ed3a86c593869b0f9db",
            "value": " 112/112 [00:00&lt;00:00, 6.48kB/s]"
          }
        },
        "807bca63da8e445099de076866aff172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee3f6e207e04250b5320ff30088ab23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be489b0db48a403e8cbf868c372c2e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29b6641a6c9448498b444d14e668221a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b412f6f12a84414eb0dc5e900671d436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2509b738f9f4d94928304c5a46d86f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1df2254fe3504ed3a86c593869b0f9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcc44a136fbd412bac5fab9d4b06a40a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_508eba817d7d41b78808fbc040427596",
              "IPY_MODEL_562538ee70f0404ca5c63c167e6d3a21",
              "IPY_MODEL_4f7b9a64a59744d581f3bc26765643ff"
            ],
            "layout": "IPY_MODEL_34285e769efe4e2e8095e15c739caf8e"
          }
        },
        "508eba817d7d41b78808fbc040427596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_401b60be3839406987d29c995b3679da",
            "placeholder": "​",
            "style": "IPY_MODEL_13fb95ab92de41b88347c7aed2e9441c",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "562538ee70f0404ca5c63c167e6d3a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85c7e3cb187448dca099364db4b805be",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a72fa2953d4c4f70bcd75ec88c2ea95d",
            "value": 190
          }
        },
        "4f7b9a64a59744d581f3bc26765643ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b737b62f51e4b1095b7d3db1c060efe",
            "placeholder": "​",
            "style": "IPY_MODEL_d31607e4830a4e0788624b459f809e96",
            "value": " 190/190 [00:00&lt;00:00, 11.6kB/s]"
          }
        },
        "34285e769efe4e2e8095e15c739caf8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "401b60be3839406987d29c995b3679da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13fb95ab92de41b88347c7aed2e9441c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85c7e3cb187448dca099364db4b805be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a72fa2953d4c4f70bcd75ec88c2ea95d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b737b62f51e4b1095b7d3db1c060efe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d31607e4830a4e0788624b459f809e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4804d4a46df4aa1b9b089e48e54aa82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cbdb36c80f546c0b9e066c0e3ad85c1",
              "IPY_MODEL_7b22e2c90d174f5dad664e5995c425b1",
              "IPY_MODEL_88457ec03982476eb958efb1ea2d5085"
            ],
            "layout": "IPY_MODEL_a4a99d57c30b44699f70a5b7dfe366a8"
          }
        },
        "9cbdb36c80f546c0b9e066c0e3ad85c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_609c953b06e14252ad4999f3e9c6aa2b",
            "placeholder": "​",
            "style": "IPY_MODEL_5f37d315b0744d20b8b35e61eeda8e2c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7b22e2c90d174f5dad664e5995c425b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e87a73d92853477f9ac1bd767c11999a",
            "max": 2539,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee6c4bf481b04b3284116fc2849a14d5",
            "value": 2539
          }
        },
        "88457ec03982476eb958efb1ea2d5085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8c0492545f54a3aac8b2bf6f68f146b",
            "placeholder": "​",
            "style": "IPY_MODEL_715d28a3fb5f429dbe21a05513584aef",
            "value": " 2.54k/2.54k [00:00&lt;00:00, 73.4kB/s]"
          }
        },
        "a4a99d57c30b44699f70a5b7dfe366a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "609c953b06e14252ad4999f3e9c6aa2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f37d315b0744d20b8b35e61eeda8e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e87a73d92853477f9ac1bd767c11999a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee6c4bf481b04b3284116fc2849a14d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8c0492545f54a3aac8b2bf6f68f146b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "715d28a3fb5f429dbe21a05513584aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db5a066686154f05997080e98ce59abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b23437e99a5a4e75830d4762ac89da9c",
              "IPY_MODEL_3f5317ca9c0f4f09894326c9532c71cf",
              "IPY_MODEL_95948e44862a441a819712cd71ca6eae"
            ],
            "layout": "IPY_MODEL_89066b2795b44e50905e6e0544e094b0"
          }
        },
        "b23437e99a5a4e75830d4762ac89da9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1f08e9c066f46ee8e2352e144e26f0a",
            "placeholder": "​",
            "style": "IPY_MODEL_2ca48ed368fc417f9483facb6a3a58ae",
            "value": "spiece.model: 100%"
          }
        },
        "3f5317ca9c0f4f09894326c9532c71cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d199fb112d4c4417a85efef894288068",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_488cca4d9fb7456186b3fdc8a9e5a51a",
            "value": 791656
          }
        },
        "95948e44862a441a819712cd71ca6eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37cc1f7ee94f4424a4d4bca31caa09ec",
            "placeholder": "​",
            "style": "IPY_MODEL_6e6182ce57bf4c61a87d800556392705",
            "value": " 792k/792k [00:00&lt;00:00, 7.02MB/s]"
          }
        },
        "89066b2795b44e50905e6e0544e094b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f08e9c066f46ee8e2352e144e26f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca48ed368fc417f9483facb6a3a58ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d199fb112d4c4417a85efef894288068": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "488cca4d9fb7456186b3fdc8a9e5a51a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37cc1f7ee94f4424a4d4bca31caa09ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e6182ce57bf4c61a87d800556392705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90ba1c5997c54789b3e306db6bf7cdb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9836b480662a47919f33ee38ac5e6236",
              "IPY_MODEL_94b701eceb2f4ee7a1c3eacd75676ae5",
              "IPY_MODEL_5748cc4f24444c989993535513a427fb"
            ],
            "layout": "IPY_MODEL_e0fd0716509740928f1543ec07b08b12"
          }
        },
        "9836b480662a47919f33ee38ac5e6236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7978b7c1fca42a4a67b2532ebebbcd0",
            "placeholder": "​",
            "style": "IPY_MODEL_b417dff54be9488fa6f94bc06be183f4",
            "value": "tokenizer.json: 100%"
          }
        },
        "94b701eceb2f4ee7a1c3eacd75676ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dd24d7223b243bbbb56dcd7ccd78c7a",
            "max": 2424064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70afe58926c245f9be76995560405afc",
            "value": 2424064
          }
        },
        "5748cc4f24444c989993535513a427fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d45a45e2036f42d1984e047ebd63fa32",
            "placeholder": "​",
            "style": "IPY_MODEL_57b0c1b425e34c3caa2d0cd8521a2911",
            "value": " 2.42M/2.42M [00:00&lt;00:00, 16.5MB/s]"
          }
        },
        "e0fd0716509740928f1543ec07b08b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7978b7c1fca42a4a67b2532ebebbcd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b417dff54be9488fa6f94bc06be183f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dd24d7223b243bbbb56dcd7ccd78c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70afe58926c245f9be76995560405afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d45a45e2036f42d1984e047ebd63fa32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57b0c1b425e34c3caa2d0cd8521a2911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db16259e7cba44dfa636304774c2c6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf2c8f83a6b34cd88f7cebd0175c72af",
              "IPY_MODEL_847851e1f2214e1cb3d66f1cc2d3405e",
              "IPY_MODEL_aa13b0a1cc8a4b709f4dd9479a472ed1"
            ],
            "layout": "IPY_MODEL_966a23bf2cfd4619822d48be8c4701c6"
          }
        },
        "cf2c8f83a6b34cd88f7cebd0175c72af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2bec6964c5f4320929dd0ae77e56a41",
            "placeholder": "​",
            "style": "IPY_MODEL_41e85494f1ec496d8fcd359017decbd0",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "847851e1f2214e1cb3d66f1cc2d3405e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf1bac252f6945df860a53417d1942d6",
            "max": 2201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73d92a1009b542fd8d919846785f380b",
            "value": 2201
          }
        },
        "aa13b0a1cc8a4b709f4dd9479a472ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b5d4a5edbf640d58d554fb7ce8acc58",
            "placeholder": "​",
            "style": "IPY_MODEL_04cc1b7ac3a34690b3851f47efb8815e",
            "value": " 2.20k/2.20k [00:00&lt;00:00, 17.3kB/s]"
          }
        },
        "966a23bf2cfd4619822d48be8c4701c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2bec6964c5f4320929dd0ae77e56a41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41e85494f1ec496d8fcd359017decbd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf1bac252f6945df860a53417d1942d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d92a1009b542fd8d919846785f380b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b5d4a5edbf640d58d554fb7ce8acc58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04cc1b7ac3a34690b3851f47efb8815e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68bd97c79f7b4bceb203828a758d107e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_800a7753b75744bdb02173766a825515",
              "IPY_MODEL_6e5e135b276e4a4583f016851faf5d08",
              "IPY_MODEL_96016c6eb931439aaa01a1a76e7bd376"
            ],
            "layout": "IPY_MODEL_ec15c2c66d0e4c5e8fe32dc9ddfd5034"
          }
        },
        "800a7753b75744bdb02173766a825515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08db420bd45e4f508676d784df456fbc",
            "placeholder": "​",
            "style": "IPY_MODEL_1e51258337e148e7ae9a408d4286f2b9",
            "value": "config.json: 100%"
          }
        },
        "6e5e135b276e4a4583f016851faf5d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4edf8bda68ce45d9aaa266e83314e0f8",
            "max": 662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbd3f1b8b2104b56a792ac72504c40d2",
            "value": 662
          }
        },
        "96016c6eb931439aaa01a1a76e7bd376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fb225f5989c402fbb88bc95946fae47",
            "placeholder": "​",
            "style": "IPY_MODEL_b7ab29f24a6743ee8e7dec6f6ad5725f",
            "value": " 662/662 [00:00&lt;00:00, 16.0kB/s]"
          }
        },
        "ec15c2c66d0e4c5e8fe32dc9ddfd5034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08db420bd45e4f508676d784df456fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e51258337e148e7ae9a408d4286f2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4edf8bda68ce45d9aaa266e83314e0f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbd3f1b8b2104b56a792ac72504c40d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fb225f5989c402fbb88bc95946fae47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7ab29f24a6743ee8e7dec6f6ad5725f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e5131daf3fb4645a35c098f3db3f74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35d4ac78182f4dbc8cd28f886998e2ba",
              "IPY_MODEL_a3e334873908452fa418b7d39b933c20",
              "IPY_MODEL_32d2ad1b1205484694970c7becefa6dd"
            ],
            "layout": "IPY_MODEL_62f90487c5d94e7fbeca07bbfb77aca2"
          }
        },
        "35d4ac78182f4dbc8cd28f886998e2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6174d061c9a840928eb8e5a64d3764d8",
            "placeholder": "​",
            "style": "IPY_MODEL_eb23c29ff7c74084ab1566389561a9cd",
            "value": "model.safetensors: 100%"
          }
        },
        "a3e334873908452fa418b7d39b933c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4f24f1fbe15430bb93621c45ef2cc95",
            "max": 3132668804,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25c12ded834343ebbd53247a3bce686b",
            "value": 3132668804
          }
        },
        "32d2ad1b1205484694970c7becefa6dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dfefb4431ff4989871b19e88e28cf33",
            "placeholder": "​",
            "style": "IPY_MODEL_78e12ba3a9ab463ebca1bce5019d1edc",
            "value": " 3.13G/3.13G [00:31&lt;00:00, 38.1MB/s]"
          }
        },
        "62f90487c5d94e7fbeca07bbfb77aca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6174d061c9a840928eb8e5a64d3764d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb23c29ff7c74084ab1566389561a9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4f24f1fbe15430bb93621c45ef2cc95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25c12ded834343ebbd53247a3bce686b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9dfefb4431ff4989871b19e88e28cf33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78e12ba3a9ab463ebca1bce5019d1edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae96aaef5684415f818413c0b3f2a4a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7ec65d9bb134a8eaec3e5476d406e13",
              "IPY_MODEL_25c1bf7af8aa4c9886edcdb45ca1fe4d",
              "IPY_MODEL_da29190b171641d78390ff070fe40701"
            ],
            "layout": "IPY_MODEL_250e2c59d8cd4b12b74a03d84e6370af"
          }
        },
        "f7ec65d9bb134a8eaec3e5476d406e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86470b106c59468e96056b638b2ce696",
            "placeholder": "​",
            "style": "IPY_MODEL_9ac1876586274f9789ab2530d499aff5",
            "value": "generation_config.json: 100%"
          }
        },
        "25c1bf7af8aa4c9886edcdb45ca1fe4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c8aeae0703c43ea9b2c60dd07700099",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_366d99b698f743b485ba40bafe7575e8",
            "value": 147
          }
        },
        "da29190b171641d78390ff070fe40701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cce0cdc28e9442e8d334408e22c14d7",
            "placeholder": "​",
            "style": "IPY_MODEL_0c1aaadc4d514c878f6be620ebb0d6e7",
            "value": " 147/147 [00:00&lt;00:00, 10.4kB/s]"
          }
        },
        "250e2c59d8cd4b12b74a03d84e6370af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86470b106c59468e96056b638b2ce696": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac1876586274f9789ab2530d499aff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c8aeae0703c43ea9b2c60dd07700099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "366d99b698f743b485ba40bafe7575e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cce0cdc28e9442e8d334408e22c14d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c1aaadc4d514c878f6be620ebb0d6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}