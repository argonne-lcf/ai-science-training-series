{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced62bc9",
   "metadata": {},
   "source": [
    "# Install Libraries if Needed (Only Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908937c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db221ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U jax[cuda] --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U deepxde\n",
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install -U tensorflow\n",
    "# !pip install ipympl\n",
    "# !pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52603e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fadb038",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Plotting\n",
    "\n",
    "# for jupyter notebooks\n",
    "%matplotlib notebook \n",
    "\n",
    "# for jupyter labs\n",
    "# %matplotlib widget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecaf932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import io\n",
    "import re\n",
    "\n",
    "\n",
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import torch\n",
    "from jax import random, grad, vmap, jit, hessian\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.optimizers import adam, exponential_decay\n",
    "from jax.experimental.ode import odeint\n",
    "from jax.nn import relu, elu, softplus\n",
    "from jax.config import config\n",
    "from jax.ops import index_update, index\n",
    "from jax import lax\n",
    "from jax.lax import while_loop, scan, cond\n",
    "from jax.flatten_util import ravel_pytree\n",
    "\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59acc6",
   "metadata": {},
   "source": [
    "# Intro To Jax\n",
    "\n",
    "This notebook is written in Jax.  Jax is essentially numpy on steroids.  It can map numpy function to GPU/TPU devices with only some slight differences.  It allow provides automatic differentiation.  Working with a low level library like Jax makes some physics informed deep learning applications easier.\n",
    "\n",
    "See https://github.com/google/jax for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be51cdf",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367eb254",
   "metadata": {},
   "source": [
    "## Jax Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define MLP\n",
    "def MLP(layers, activation=relu):\n",
    "  ''' Vanilla MLP'''\n",
    "  def init(rng_key):\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          glorot_stddev = 1. / jnp.sqrt((d_in + d_out) / 2.)\n",
    "          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "          b = jnp.zeros(d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return params\n",
    "  def apply(params, inputs):\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = jnp.dot(inputs, W) + b\n",
    "          inputs = activation(outputs)\n",
    "      W, b = params[-1]\n",
    "      outputs = jnp.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply\n",
    "\n",
    "# Define modified MLP\n",
    "def modified_MLP(layers, activation=relu):\n",
    "  def xavier_init(key, d_in, d_out):\n",
    "      glorot_stddev = 1. / jnp.sqrt((d_in + d_out) / 2.)\n",
    "      W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "      b = jnp.zeros(d_out)\n",
    "      return W, b\n",
    "\n",
    "  def init(rng_key):\n",
    "      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          W, b = xavier_init(k1, d_in, d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return (params, U1, b1, U2, b2) \n",
    "\n",
    "  def apply(params, inputs):\n",
    "      params, U1, b1, U2, b2 = params\n",
    "      U = activation(jnp.dot(inputs, U1) + b1)\n",
    "      V = activation(jnp.dot(inputs, U2) + b2)\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = activation(jnp.dot(inputs, W) + b)\n",
    "          inputs = jnp.multiply(outputs, U) + jnp.multiply(1 - outputs, V) \n",
    "      W, b = params[-1]\n",
    "      outputs = jnp.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply\n",
    "\n",
    "# Define Fourier feature net\n",
    "def FF_MLP(layers, freqs=50, activation=relu):\n",
    "   # Define input encoding function\n",
    "    def input_encoding(x, w):\n",
    "        out = jnp.hstack([jnp.sin(jnp.dot(x, w)),\n",
    "                         jnp.cos(jnp.dot(x, w))])\n",
    "        return out\n",
    "    FF = freqs * random.normal(random.PRNGKey(0), (layers[0], layers[1]//2))\n",
    "    def init(rng_key):\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          glorot_stddev = 1. / jnp.sqrt((d_in + d_out) / 2.)\n",
    "          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "          b = jnp.zeros(d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[1:-1], layers[2:]))\n",
    "      return params\n",
    "    def apply(params, inputs):\n",
    "        H = input_encoding(inputs, FF)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = jnp.dot(H, W) + b\n",
    "            H = activation(outputs)\n",
    "        W, b = params[-1]\n",
    "        outputs = jnp.dot(H, W) + b\n",
    "        return outputs\n",
    "    return init, apply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a512a5",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ba365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data generator\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, u, y, s, \n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.u = u # input sample\n",
    "        self.y = y # location\n",
    "        self.s = s # labeled data evulated at y (solution measurements, BC/IC conditions, etc.)\n",
    "        \n",
    "        self.N = u.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        inputs, outputs = self.__data_generation(subkey)\n",
    "        return inputs, outputs\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        if self.batch_size <= self.N:\n",
    "            idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        else:\n",
    "            idx = random.choice(key, self.N, (self.N,), replace=False)\n",
    "        s = self.s[idx,:]\n",
    "        y = self.y[idx,:]\n",
    "        u = self.u[idx,:]\n",
    "        # Construct batch\n",
    "        inputs = (u, y)\n",
    "        outputs = s\n",
    "        return inputs, outputs\n",
    "    \n",
    "    \n",
    "# Data generator\n",
    "class DataGeneratorEfficient(data.Dataset):\n",
    "    def __init__(self, u, y, s, ind,\n",
    "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        \n",
    "        self.u = u # input sample (Nsim, m)\n",
    "        self.y = y # location\n",
    "        self.s = s # labeled data evulated at y (solution measurements, BC/IC conditions, etc.)\n",
    "        self.ind = ind  # int array representing which coordinate belongs with which u\n",
    "        self.N = y.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        inputs, outputs = self.__data_generation(subkey)\n",
    "        return inputs, outputs\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        if self.batch_size <= self.N:\n",
    "            idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
    "        else:\n",
    "            idx = random.choice(key, self.N, (self.N,), replace=False)\n",
    "        s = self.s[idx,:]\n",
    "        y = self.y[idx,:]\n",
    "        ind = self.ind[idx,0] # reshapes ind correctly for indexing u\n",
    "        u = self.u[ind,:]\n",
    "        # Construct batch\n",
    "        inputs = (u, y)\n",
    "        outputs = s\n",
    "        return inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30278731",
   "metadata": {},
   "source": [
    "## DeepONetPI Model (Subclass this model for your particular PDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca30cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class DeepONetPI:\n",
    "    def __init__(self, \n",
    "                 branch_layers, \n",
    "                 trunk_layers, \n",
    "                 branch_net=MLP,\n",
    "                 trunk_net=MLP,\n",
    "                 branch_activation=jnp.tanh,\n",
    "                 trunk_activation=jnp.tanh,\n",
    "                 branch_rng_key=random.PRNGKey(1234),\n",
    "                 trunk_rng_key=random.PRNGKey(4321),\n",
    "                 optimizer=adam(exponential_decay(1e-3, decay_steps=5000,decay_rate=0.9)),\n",
    "                 operator_loss_const=1.0,\n",
    "                 physics_loss_const=1.0,\n",
    "                 bcs_loss_const=1.0,\n",
    "                 ics_loss_const=1.0,\n",
    "                 ckpt_dir='DeepONetPI',\n",
    "                 ckpt_file='params.npy',\n",
    "                 it_file='iterations.npy',\n",
    "                 loss_file='loss.npy',\n",
    "                 loss_operator_file='loss_operator.npy',\n",
    "                 loss_physics_file='loss_physics.npy',\n",
    "                 loss_bcs_file='loss_bcs.npy',\n",
    "                 loss_ics_file='loss_ics.npy',\n",
    "                 ):\n",
    "        # Network initialization and evaluation functions\n",
    "        # These are reserved for the flax case\n",
    "        self.branch_net = None\n",
    "        self.trunk_net = None\n",
    "        # Branch network\n",
    "        self.branch_init, self.branch_apply = branch_net(branch_layers, activation=branch_activation)\n",
    "        # Trunk network\n",
    "        self.trunk_init, self.trunk_apply = trunk_net(trunk_layers, activation=trunk_activation)\n",
    "\n",
    "        # Initialize\n",
    "        branch_params = self.branch_init(rng_key=branch_rng_key)\n",
    "        trunk_params = self.trunk_init(rng_key=trunk_rng_key)\n",
    "        params = (branch_params, trunk_params)\n",
    "\n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        self.opt_init, self.opt_update, self.get_params = optimizer\n",
    "        self.opt_state = self.opt_init(params)\n",
    "\n",
    "        # Initialize itercounter\n",
    "        self.itercount = itertools.count()\n",
    "        self.it = 0\n",
    "        \n",
    "        # Get Number of coordinate dimensions\n",
    "        self.dim = trunk_layers[0]\n",
    "        \n",
    "         # Used to restore the trained model parameters\n",
    "        _, self.unravel_params = ravel_pytree(params)\n",
    "        \n",
    "        \n",
    "        # Define loss constants used to weight the loss\n",
    "        self.operator_loss_const = operator_loss_const\n",
    "        self.physics_loss_const = physics_loss_const\n",
    "        self.bcs_loss_const = bcs_loss_const\n",
    "        self.ics_loss_const = ics_loss_const\n",
    "        \n",
    "        # Loggers\n",
    "        self.loss_log = []\n",
    "        self.loss_operator_log = []\n",
    "        self.loss_physics_log = []\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_ics_log = []\n",
    "        \n",
    "        # Checkpointing file names\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.ckpt_path = os.path.join(self.ckpt_dir, ckpt_file)\n",
    "        self.it_path = os.path.join(self.ckpt_dir, it_file)\n",
    "        self.loss_path = os.path.join(self.ckpt_dir, loss_file)\n",
    "        self.loss_operator_path = os.path.join(self.ckpt_dir, loss_operator_file)\n",
    "        self.loss_physics_path = os.path.join(self.ckpt_dir, loss_physics_file)\n",
    "        self.loss_bcs_path = os.path.join(self.ckpt_dir, loss_bcs_file)\n",
    "        self.loss_ics_path = os.path.join(self.ckpt_dir, loss_ics_file)\n",
    "\n",
    "        \n",
    "    # Define DeepONet architecture\n",
    "    def operator_net(self, params, u, *y):\n",
    "        # Apply DeepONet\n",
    "        # inputs: (u, *y), shape = (N, m), (N, 1) * dim or (N, dim)\n",
    "        # outputs: s, shape = (N, 1)\n",
    "        # Note that each coordinate dimension can be a separate input or a single array. This is done to make differentiation easier. In future we may allow u to be a tuple of shapes (N, m_i) * len(u) to allow for multiple branch networks to represent ics, bcs, \n",
    "        \n",
    "        y = jnp.stack(y)\n",
    "        branch_params, trunk_params = params\n",
    "        B = self.branch_apply(branch_params, u)\n",
    "        T = self.trunk_apply(trunk_params, y)\n",
    "        outputs = jnp.sum(B * T)\n",
    "        return outputs\n",
    "    \n",
    "    # Define ODE/PDE residual (initial definition)\n",
    "    def pde_net(self, params, u, y):\n",
    "        # s_ = self.operator_net # shorthand for operator_net function call\n",
    "        # s = s_(params, u, y) # actual value of s (may not be needed)\n",
    "        # # optionally split s up here       \n",
    "        # s_y = grad(s_, 2)(params, u, y) # the 2nd argument in grad specifies the argument number, here y is 2 for example\n",
    "        # res = 0.0 # here we output 0 because we want the user to specify the pde\n",
    "        # return res\n",
    "        pass\n",
    "    \n",
    "    # Define boundary condition (initial definition)\n",
    "    def bc_net(self, params, u, y):\n",
    "        pass\n",
    "    \n",
    "    # Define initial condition (initial definition)\n",
    "    def ic_net(self, params, u, y):\n",
    "        pass\n",
    "    \n",
    "    # Define operator loss\n",
    "    def loss_operator(self, params, batch):\n",
    "        # Fetch data\n",
    "        # inputs: (u, y), shape = (N, m), (N, dim)\n",
    "        # outputs: s, shape = (N, 1)\n",
    "        inputs, outputs = batch\n",
    "        u, y = inputs\n",
    "        # Compute forward pass\n",
    "        pred = vmap(self.operator_net, (None, 0, 0))(params, u, y)\n",
    "        # Compute loss\n",
    "        loss = jnp.mean((outputs.flatten() - pred.flatten())**2)\n",
    "        return loss\n",
    "\n",
    "    # Define physics loss\n",
    "    def loss_physics(self, params, batch):\n",
    "        # Fetch data\n",
    "        # inputs: (u_pde, y_pde), shape = (N_pde, m), (N_pde, dim)\n",
    "        # outputs: s_pde, shape = (N_pde, 1)\n",
    "        inputs, outputs = batch\n",
    "        u, y = inputs\n",
    "        # Compute forward pass\n",
    "        pred = vmap(self.pde_net, (None, 0, 0))(params, u, y)\n",
    "        # Compute loss\n",
    "        loss = jnp.mean((outputs.flatten() - pred.flatten())**2)\n",
    "        return loss\n",
    "    \n",
    "    def loss_bcs(self, params, batch):\n",
    "        # Fetch data\n",
    "        # inputs: (u_bc, y_bc), shape = (N_bc, m), (N_bc, dim)\n",
    "        # outputs: s_bc, shape = (N_bc, 1)\n",
    "        inputs, outputs = batch\n",
    "        u, y = inputs\n",
    "        # Compute forward pass\n",
    "        pred = vmap(self.bc_net, (None, 0, 0))(params, u, y)\n",
    "        # Compute loss\n",
    "        loss = jnp.mean((outputs.flatten() - pred.flatten())**2)\n",
    "        return loss\n",
    "    \n",
    "    def loss_ics(self, params, batch):\n",
    "        # Fetch data\n",
    "        # inputs: (u_ic, y_ic), shape = (N_ic, m), (N_ic, dim)\n",
    "        # outputs: s_ic, shape = (N_ic, 1)\n",
    "        inputs, outputs = batch\n",
    "        u, y = inputs\n",
    "        # Compute forward pass\n",
    "        pred = vmap(self.ic_net, (None, 0, 0))(params, u, y)\n",
    "        # Compute loss\n",
    "        loss = jnp.mean((outputs.flatten() - pred.flatten())**2)\n",
    "        return loss\n",
    "    \n",
    "    # Define total loss\n",
    "    def loss(self, params, operator_batch=None, physics_batch=None, bcs_batch=None, ics_batch=None):\n",
    "        loss = loss_operator = loss_physics = loss_bcs = loss_ics = 0.0\n",
    "        # lax.cond(operator_batch is not None, )\n",
    "        if operator_batch is not None:\n",
    "            loss_operator = self.loss_operator(params, operator_batch)\n",
    "        if physics_batch is not None:\n",
    "            loss_physics = self.loss_physics(params, physics_batch)\n",
    "        if bcs_batch is not None:\n",
    "            loss_bcs = self.loss_bcs(params, bcs_batch)\n",
    "        if ics_batch is not None:\n",
    "            loss_ics = self.loss_ics(params, ics_batch)\n",
    "        # losses = jnp.array([loss_operator, loss_physics, loss_bcs, loss_ics])\n",
    "        # loss = jnp.nansum(losses)\n",
    "        \n",
    "        loss = self.operator_loss_const*loss_operator + self.physics_loss_const*loss_physics + self.bcs_loss_const*loss_bcs + self.ics_loss_const*loss_ics\n",
    "        return loss\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, operator_batch=None, physics_batch=None, bcs_batch=None, ics_batch=None):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params, operator_batch, physics_batch, bcs_batch, ics_batch)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, \n",
    "              operator_dataset=None, \n",
    "              physics_dataset=None, \n",
    "              bcs_dataset=None, \n",
    "              ics_dataset=None,\n",
    "              operator_val_dataset=None, \n",
    "              physics_val_dataset=None,\n",
    "              bcs_val_dataset=None,\n",
    "              ics_val_dataset=None,\n",
    "              operator_loss_const=None,\n",
    "              physics_loss_const=None,\n",
    "              bcs_loss_const=None,\n",
    "              ics_loss_const=None,\n",
    "              nIter=10000, \n",
    "              log_freq=10, \n",
    "              val_freq=10,\n",
    "              ckpt_freq=1000,\n",
    "              history_freq=1000):\n",
    "        \n",
    "        # Update loss constants if desired\n",
    "        if operator_loss_const is not None:\n",
    "            self.operator_loss_const = operator_loss_const\n",
    "        if physics_loss_const is not None:\n",
    "            self.physics_loss_const = physics_loss_const\n",
    "        if bcs_loss_const is not None:\n",
    "            self.bcs_loss_const = bcs_loss_const\n",
    "        if ics_loss_const is not None:\n",
    "            self.ics_loss_const = ics_loss_const\n",
    "            \n",
    "        # Define the data iterator\n",
    "        operator_data = physics_data = ics_data = bcs_data = None\n",
    "        if operator_dataset is not None:\n",
    "            operator_data = iter(operator_dataset)\n",
    "        if physics_dataset is not None:\n",
    "            physics_data = iter(physics_dataset)\n",
    "        if bcs_dataset is not None:\n",
    "            bcs_data = iter(bcs_dataset)\n",
    "        if ics_dataset is not None:\n",
    "            ics_data = iter(ics_dataset)\n",
    "\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            operator_batch = physics_batch = bcs_batch = ics_batch = None\n",
    "            if operator_data is not None:\n",
    "                operator_batch = next(operator_data)\n",
    "            if physics_data is not None:\n",
    "                physics_batch = next(physics_data)\n",
    "            if bcs_data is not None:\n",
    "                bcs_batch = next(bcs_data)\n",
    "            if ics_data is not None:\n",
    "                ics_batch = next(ics_data)\n",
    "            # current iteration including past training\n",
    "            self.it = next(self.itercount)\n",
    "            self.opt_state = self.step(self.it, self.opt_state, operator_batch, physics_batch, bcs_batch, ics_batch)\n",
    "            \n",
    "            if (log_freq != 0) and (it % log_freq == 0):\n",
    "                params = self.get_params(self.opt_state)\n",
    "\n",
    "                # Compute losses\n",
    "                loss_value = self.loss(params, operator_batch, physics_batch, bcs_batch, ics_batch)\n",
    "                \n",
    "                if operator_batch is not None:\n",
    "                    loss_operator_value = self.loss_operator(params, operator_batch)\n",
    "                else:\n",
    "                    loss_operator_value = None\n",
    "                if physics_batch is not None:\n",
    "                    loss_physics_value = self.loss_physics(params, physics_batch)\n",
    "                else:\n",
    "                    loss_physics_value = None\n",
    "                if bcs_batch is not None:\n",
    "                    loss_bcs_value = self.loss_bcs(params, bcs_batch)\n",
    "                else:\n",
    "                    loss_bcs_value = None\n",
    "                if ics_batch is not None:\n",
    "                    loss_ics_value = self.loss_ics(params, ics_batch)\n",
    "                else:\n",
    "                    loss_ics_value = None\n",
    "                \n",
    "\n",
    "                # Store losses\n",
    "                loss_dict = {} # for printing losses in pbar\n",
    "                if loss_value is not None:\n",
    "                    self.loss_log.append([self.it, loss_value])\n",
    "                    loss_dict['loss'] = loss_value                    \n",
    "                if loss_operator_value is not None:\n",
    "                    self.loss_operator_log.append([self.it, loss_operator_value])\n",
    "                    loss_dict['loss_operator'] = loss_operator_value\n",
    "                if loss_physics_value is not None:\n",
    "                    self.loss_physics_log.append([self.it, loss_physics_value])\n",
    "                    loss_dict['loss_physics'] = loss_physics_value\n",
    "                if loss_bcs_value is not None:\n",
    "                    self.loss_bcs_log.append([self.it, loss_bcs_value])\n",
    "                    loss_dict['loss_bcs'] = loss_bcs_value\n",
    "                if loss_ics_value is not None:\n",
    "                    self.loss_ics_log.append([self.it, loss_ics_value])\n",
    "                    loss_dict['loss_ics'] = loss_ics_value\n",
    "\n",
    "                # Print losses during training\n",
    "                pbar.set_postfix(loss_dict)\n",
    "            \n",
    "            if (ckpt_freq != 0) and (it % ckpt_freq == 0):\n",
    "                # may want to add an iteration number to ckpt_path in future\n",
    "                self.save(ckpt_path=self.ckpt_path, it_path=self.it_path)\n",
    "                \n",
    "            if (history_freq != 0) and (it % history_freq == 0):\n",
    "                # may want to add an itteration number to the loss logs in future\n",
    "                self.save_history()\n",
    "                \n",
    "    def save(self, ckpt_path=None, it_path=None):\n",
    "        if ckpt_path is None:\n",
    "            ckpt_path = self.ckpt_path\n",
    "        if it_path is None:\n",
    "            it_path = self.it_path\n",
    "        ckpt_dir = os.path.split(ckpt_path)[0]\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        params = self.get_params(self.opt_state) \n",
    "        flat_params = flat_params, _  = ravel_pytree(params)\n",
    "        jnp.save(ckpt_path, flat_params)\n",
    "        jnp.save(it_path, self.it)\n",
    "        \n",
    "    def restore(self, ckpt_path=None, it_path=None):\n",
    "        if ckpt_path is None:\n",
    "            ckpt_path = self.ckpt_path\n",
    "        if it_path is None:\n",
    "            it_path = self.it_path\n",
    "        try:\n",
    "            flat_params = jnp.load(ckpt_path)\n",
    "        except:\n",
    "            print(f'Failed to load file {ckpt_path}')\n",
    "            traceback.print_exc()\n",
    "            return\n",
    "        try:\n",
    "            self.it = int(jnp.load(it_path))\n",
    "        except:\n",
    "            print(f'Failed to load file {it_path}')\n",
    "            traceback.print_exc()\n",
    "        params = self.unravel_params(flat_params)\n",
    "        self.opt_state = self.opt_init(params)\n",
    "        # note that we neet to initialize itercount value (initializes to current iteration, but we want next iteration for training)\n",
    "        self.itercount = itertools.count(self.it)\n",
    "        self.it = next(self.itercount)\n",
    "\n",
    "    def save_history(self, loss_path=None, loss_operator_path=None, loss_physics_path=None, loss_bcs_path=None, loss_ics_path=None):\n",
    "        if loss_path is None:\n",
    "            loss_path = self.loss_path\n",
    "        if loss_operator_path is None:\n",
    "            loss_operator_path = self.loss_operator_path\n",
    "        if loss_physics_path is None:\n",
    "            loss_physics_path = self.loss_physics_path\n",
    "        if loss_bcs_path is None:\n",
    "            loss_bcs_path = self.loss_bcs_path\n",
    "        if loss_ics_path is None:\n",
    "            loss_ics_path = self.loss_ics_path\n",
    "        \n",
    "        loss_dir = os.path.split(loss_path)[0]\n",
    "        loss_operator_dir = os.path.split(loss_operator_path)[0]\n",
    "        loss_physics_dir = os.path.split(loss_physics_path)[0]\n",
    "        loss_bcs_dir = os.path.split(loss_bcs_path)[0]\n",
    "        loss_ics_dir = os.path.split(loss_ics_path)[0]\n",
    "        \n",
    "        if self.loss_log:\n",
    "            os.makedirs(loss_dir, exist_ok=True)\n",
    "            jnp.save(loss_path, self.loss_log)\n",
    "        if self.loss_operator_log:\n",
    "            os.makedirs(loss_operator_dir, exist_ok=True)\n",
    "            jnp.save(loss_operator_path, self.loss_operator_log)\n",
    "        if self.loss_physics_log:\n",
    "            os.makedirs(loss_physics_dir, exist_ok=True)\n",
    "            jnp.save(loss_physics_path, self.loss_physics_log)\n",
    "        if self.loss_bcs_log:\n",
    "            os.makedirs(loss_bcs_dir, exist_ok=True)\n",
    "            jnp.save(loss_bcs_path, self.loss_bcs_log)\n",
    "        if self.loss_ics_log:\n",
    "            os.makedirs(loss_ics_dir, exist_ok=True)\n",
    "            jnp.save(loss_ics_path, self.loss_ics_log)\n",
    "            \n",
    "    def restore_history(self, loss_path=None, loss_operator_path=None, loss_physics_path=None, loss_bcs_path=None, loss_ics_path=None):\n",
    "        if loss_path is None:\n",
    "            loss_path = self.loss_path\n",
    "        if loss_operator_path is None:\n",
    "            loss_operator_path = self.loss_operator_path\n",
    "        if loss_physics_path is None:\n",
    "            loss_physics_path = self.loss_physics_path\n",
    "        if loss_bcs_path is None:\n",
    "            loss_bcs_path = self.loss_bcs_path\n",
    "        if loss_ics_path is None:\n",
    "            loss_ics_path = self.loss_ics_path\n",
    "        \n",
    "        # Try to load the loss files and add them to loss logs.  It is ok if they fail beacause the loss files might not exist.\n",
    "        try:\n",
    "            loss_log = jnp.load(loss_path)\n",
    "            self.loss_log = list(list(l) for l in loss_log)\n",
    "        except:\n",
    "            print(f'Failed to load file {loss_path}')\n",
    "        try:\n",
    "            loss_operator_log = jnp.load(loss_operator_path)\n",
    "            self.loss_operator_log = list(list(l) for l in loss_operator_log)\n",
    "        except:\n",
    "            print(f'Failed to load file {loss_operator_path}')\n",
    "        try:\n",
    "            loss_physics_log = jnp.load(loss_physics_path)\n",
    "            self.loss_physics_log = list(list(l) for l in loss_physics_log)\n",
    "        except:\n",
    "            print(f'Failed to load file {loss_physics_path}')\n",
    "        try:\n",
    "            loss_bcs_log = jnp.load(loss_bcs_path)\n",
    "            self.loss_bcs_log = list(list(l) for l in loss_bcs_log)\n",
    "        except:\n",
    "            print(f'Failed to load file {loss_bcs_path}')\n",
    "        try:\n",
    "            loss_ics_log = jnp.load(loss_ics_path)\n",
    "            self.loss_ics_log = list(list(l) for l in loss_ics_log)\n",
    "        except:\n",
    "            print(f'Failed to load file {loss_ics_path}')\n",
    "        \n",
    "    def restart_counter(self, i=0):\n",
    "        self.itercount = itertools(i)\n",
    "       \n",
    "           \n",
    "    # Evaluates predictions at test points  \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def predict_s(self, params, U_star, Y_star):\n",
    "        s_pred = vmap(self.operator_net, (None, 0, 0))(params, U_star, Y_star)\n",
    "        return s_pred\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def predict_pde(self, params, U_star, Y_star):\n",
    "        pde_pred = vmap(self.pde_net, (None, 0, 0))(params, U_star, Y_star)\n",
    "        return pde_pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d440bb2",
   "metadata": {},
   "source": [
    "## Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cf0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use double precision to generate data (due to GP sampling)\n",
    "def RBF(x1, x2, params):\n",
    "    output_scale, lengthscales = params\n",
    "    diffs = jnp.expand_dims(x1 / lengthscales, 1) - \\\n",
    "            jnp.expand_dims(x2 / lengthscales, 0)\n",
    "    r2 = jnp.sum(diffs**2, axis=2)\n",
    "    return output_scale * jnp.exp(-0.5 * r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d002725",
   "metadata": {},
   "source": [
    "# Diffusion Reaction Equation\n",
    "$$\n",
    "\\frac{\\partial s}{\\partial t}=D \\frac{\\partial^{2} s}{\\partial x^{2}}+k s^{2}+u(x), \\quad x \\in(0,1), t \\in(0,1],\\\\\n",
    "s(0,t) = s(1,t) = s(x,0) = 0, \\\\\n",
    "k=D=0.01\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0eb409",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRDeepONetPI(DeepONetPI):\n",
    "    def __init__(self,\n",
    "                 branch_layers, \n",
    "                 trunk_layers, \n",
    "                 branch_net=MLP,\n",
    "                 trunk_net=MLP,\n",
    "                 branch_activation=jnp.tanh,\n",
    "                 trunk_activation=jnp.tanh,\n",
    "                 branch_rng_key=random.PRNGKey(1234),\n",
    "                 trunk_rng_key=random.PRNGKey(4321),\n",
    "                 optimizer=adam(exponential_decay(1e-3, decay_steps=5000, decay_rate=0.9)),\n",
    "                 operator_loss_const=1.0,\n",
    "                 physics_loss_const=1.0,\n",
    "                 bcs_loss_const=1.0,\n",
    "                 ics_loss_const=1.0,\n",
    "                 ckpt_dir='DiffusionReaction',\n",
    "                 ckpt_file='params.npy',\n",
    "                 loss_file='loss.npy',\n",
    "                 loss_operator_file='loss_operator.npy',\n",
    "                 loss_physics_file='loss_physics.npy',\n",
    "                 loss_bcs_file='loss_bcs.npy',\n",
    "                 loss_ics_file='loss_ics.npy',   \n",
    "                 ):\n",
    "        super().__init__(branch_layers, \n",
    "                         trunk_layers, \n",
    "                         branch_net, \n",
    "                         trunk_net, \n",
    "                         branch_activation, \n",
    "                         trunk_activation, \n",
    "                         branch_rng_key, \n",
    "                         trunk_rng_key, \n",
    "                         optimizer, \n",
    "                         operator_loss_const, \n",
    "                         physics_loss_const, \n",
    "                         bcs_loss_const, \n",
    "                         ics_loss_const, \n",
    "                         ckpt_dir, \n",
    "                         ckpt_file, \n",
    "                         loss_file, \n",
    "                         loss_operator_file, \n",
    "                         loss_physics_file, \n",
    "                         loss_bcs_file, \n",
    "                         loss_ics_file)\n",
    "        \n",
    "    def pde_net(self, params, u, y):\n",
    "        # note that y here can include additional coord not passed to the original operator network\n",
    "        # print(y.shape)\n",
    "        x = y[0]\n",
    "        t = y[1]\n",
    "        s_ = self.operator_net # shorthand for operator_net function call\n",
    "        s = s_(params, u, x, t) # actual value of s (may not be needed)\n",
    "        s_x = grad(s_, 2)(params, u, x, t) \n",
    "        s_t = grad(s_, 3)(params, u, x, t)\n",
    "        s_xx = grad(grad(s_, 2), 2)(params, u, x, t)\n",
    "        res = s_t - 0.01 * s_xx - 0.01 * s**2 # this is equal to u(x), this avoids the interpolation\n",
    "        return res\n",
    "    \n",
    "    # here we are provided with values for BC/IC (zero), so for this case just call the operator net.  If we had Robin BC for example, we would output an array with outputs of [value, derivative]\n",
    "    # Also, if we had the case where BC/IC = u, we would return res = s - u\n",
    "    def bc_net(self, params, u, y):\n",
    "        s_ = self.operator_net # shorthand for operator_net function call\n",
    "        s = s_(params, u, y) # actual value of s\n",
    "        return s\n",
    "    \n",
    "    def ic_net(self, params, u, y):\n",
    "        s_ = self.operator_net # shorthand for operator_net function call\n",
    "        s = s_(params, u, y) # actual value of s\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e59f6ac",
   "metadata": {},
   "source": [
    "# Define Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef7bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use double precision to generate data (due to GP sampling)\n",
    "\n",
    "# A diffusion-reaction numerical solver\n",
    "def solve_ADR(key, Nx, Nt, N_op, length_scale):\n",
    "    \"\"\"Solve 1D\n",
    "    u_t = (k(x) u_x)_x - v(x) u_x + g(u) + f(x)\n",
    "    with zero initial and boundary conditions.\n",
    "    \"\"\"\n",
    "    xmin, xmax = 0, 1\n",
    "    tmin, tmax = 0, 1\n",
    "    k = lambda x: 0.01*jnp.ones_like(x)\n",
    "    v = lambda x: jnp.zeros_like(x)\n",
    "    g = lambda u: 0.01*u ** 2\n",
    "    dg = lambda u: 0.02 * u\n",
    "    u0 = lambda x: jnp.zeros_like(x)\n",
    "\n",
    "    # Generate subkeys\n",
    "    subkeys = random.split(key, 2)\n",
    "\n",
    "    # Generate a GP sample\n",
    "    N = 512\n",
    "    gp_params = (1.0, length_scale)\n",
    "    jitter = 1e-10\n",
    "    X = jnp.linspace(xmin, xmax, N)[:,None]\n",
    "    K = RBF(X, X, gp_params)\n",
    "    L = jnp.linalg.cholesky(K + jitter*jnp.eye(N))\n",
    "    gp_sample = jnp.dot(L, random.normal(subkeys[0], (N,)))\n",
    "    # Create a callable interpolation function  \n",
    "    f_fn = lambda x: jnp.interp(x, X.flatten(), gp_sample)\n",
    "\n",
    "    # Create grid\n",
    "    x = jnp.linspace(xmin, xmax, Nx)\n",
    "    t = jnp.linspace(tmin, tmax, Nt)\n",
    "    h = x[1] - x[0]\n",
    "    dt = t[1] - t[0]\n",
    "    h2 = h ** 2\n",
    "\n",
    "    # Compute coefficients and forcing\n",
    "    k = k(x)\n",
    "    v = v(x)\n",
    "    f = f_fn(x)\n",
    "\n",
    "    # Compute finite difference operators\n",
    "    D1 = jnp.eye(Nx, k=1) - jnp.eye(Nx, k=-1)\n",
    "    D2 = -2 * jnp.eye(Nx) + jnp.eye(Nx, k=-1) + jnp.eye(Nx, k=1)\n",
    "    D3 = jnp.eye(Nx - 2)\n",
    "    M = -jnp.diag(D1 @ k) @ D1 - 4 * jnp.diag(k) @ D2\n",
    "    m_bond = 8 * h2 / dt * D3 + M[1:-1, 1:-1]\n",
    "    v_bond = 2 * h * jnp.diag(v[1:-1]) @ D1[1:-1, 1:-1] + 2 * h * jnp.diag(\n",
    "        v[2:] - v[: Nx - 2]\n",
    "    )\n",
    "    mv_bond = m_bond + v_bond\n",
    "    c = 8 * h2 / dt * D3 - M[1:-1, 1:-1] - v_bond\n",
    "\n",
    "    # Initialize solution and apply initial condition\n",
    "    u = jnp.zeros((Nx, Nt))\n",
    "    # u = index_update(u, index[:,0], u0(x)) # this index_update syntax has been decapitated and the new syntax is better\n",
    "    u = u.at[:, 0].set(u0(x))\n",
    "    # Time-stepping update\n",
    "    def body_fn(i, u):\n",
    "        gi = g(u[1:-1, i])\n",
    "        dgi = dg(u[1:-1, i])\n",
    "        h2dgi = jnp.diag(4 * h2 * dgi)\n",
    "        A = mv_bond - h2dgi\n",
    "        b1 = 8 * h2 * (0.5 * f[1:-1] + 0.5 * f[1:-1] + gi)\n",
    "        b2 = (c - h2dgi) @ u[1:-1, i].T\n",
    "        # u = index_update(u, index[1:-1, i + 1], jnp.linalg.solve(A, b1 + b2)) # this index_update syntax has been decapitated and the new syntax is better\n",
    "        u = u.at[1:-1, i + 1].set(jnp.linalg.solve(A, b1 + b2))\n",
    "        return u\n",
    "    # Run loop\n",
    "    S = lax.fori_loop(0, Nt-1, body_fn, u)\n",
    "\n",
    "    # Input sensor locations and measurements\n",
    "    xx = jnp.linspace(xmin, xmax, m)\n",
    "    u = f_fn(xx)\n",
    "    # Output sensor locations and measurements\n",
    "    idx = random.randint(subkeys[1], (N_op, 2), 0, max(Nx,Nt))\n",
    "    y = jnp.concatenate([x[idx[:,0]][:,None], t[idx[:,1]][:,None]], axis = 1)\n",
    "    s = S[idx[:,0], idx[:,1]]\n",
    "    # x, t: sampled points on grid\n",
    "    return (x, t, S), (u, y, s)\n",
    "\n",
    "# Geneate training data corresponding to one input sample\n",
    "def generate_one_training_data_efficient(key, index, N_op, N_pde, N_bcs, N_ics):\n",
    "    # Numerical solution\n",
    "    (x, t, S), (u, y, s) = solve_ADR(key, Nx , Nt, N_op, length_scale)\n",
    "\n",
    "    # Geneate subkeys\n",
    "    subkeys = random.split(key, 4)\n",
    "    \n",
    "    # Sample the data\n",
    "    # u_op = jnp.tile(u, (N_op, 1))\n",
    "    u_op = u\n",
    "    num_op = jnp.tile(index, (N_op, 1))\n",
    "    y_op = y\n",
    "    s_op = s\n",
    "    \n",
    "    # Generate data for PDE constraints\n",
    "    # Sample collocation points\n",
    "    # Chooses N_pde random points in domain of (x, t).  We don't need to have a correct prediction for solution s, only that our prediction by the ANN satisfies the pde at the specified (x,t). For s, we take set s(x) = u(x) assuming that u is defined at each coordinate x (guarenteed in solve_ADR).\n",
    "    x_pde_idx = random.choice(subkeys[0], jnp.arange(Nx), shape=(N_pde,1))\n",
    "    x_pde = x[x_pde_idx]\n",
    "    t_pde = random.uniform(subkeys[1], minval=0, maxval=1, shape=(N_pde,1))\n",
    "\n",
    "    # u_pde = jnp.tile(u, (N_pde, 1))\n",
    "    u_pde = u\n",
    "    num_pde = jnp.tile(index, (N_pde, 1))\n",
    "    y_pde = jnp.hstack((x_pde, t_pde))\n",
    "    s_pde = u[x_pde_idx]\n",
    "    \n",
    "    # Sample points from the boundary conditions\n",
    "    # handle odd number of bc\n",
    "    N_bc_half1 = N_bcs // 2\n",
    "    N_bc_half2 = N_bcs - N_bc_half1\n",
    "    x_bc1 = jnp.zeros((N_bc_half1, 1))\n",
    "    x_bc2 = jnp.ones((N_bc_half2, 1))\n",
    "    x_bcs = jnp.vstack((x_bc1, x_bc2))\n",
    "    t_bcs = random.uniform(subkeys[2], shape=(N_bcs, 1))\n",
    "    \n",
    "    u_bcs = u\n",
    "    num_bcs = jnp.tile(index, (N_bcs, 1))\n",
    "    y_bcs = jnp.hstack([x_bcs, t_bcs])\n",
    "    s_bcs = jnp.zeros((N_bcs, 1))\n",
    "    \n",
    "    # Sample points from initial conditions\n",
    "    x_ics = random.uniform(subkeys[3], shape=(N_ics, 1))\n",
    "    t_ics = jnp.zeros((N_ics, 1))\n",
    "\n",
    "    # u_ics = jnp.tile(u, (N_ics, 1))\n",
    "    u_ics = u\n",
    "    num_ics = jnp.tile(index, (N_ics, 1))\n",
    "    y_ics = jnp.hstack([x_ics, t_ics])\n",
    "    s_ics = jnp.zeros((N_ics, 1))\n",
    "    \n",
    "    \n",
    "    # Shorthand tuples to pass fewer outputs\n",
    "    train_op = (u_op, y_op, s_op, num_op)\n",
    "    train_pde = (u_pde, y_pde, s_pde, num_pde)\n",
    "    train_bcs = (u_bcs, y_bcs, s_bcs, num_bcs)\n",
    "    train_ics = (u_ics, y_ics, s_ics, num_ics)\n",
    "    \n",
    "\n",
    "    return train_op, train_pde, train_bcs, train_ics\n",
    "\n",
    "# Geneate test data corresponding to one input sample\n",
    "def generate_one_test_data_efficient(key, index, N_op):\n",
    "    Nx = N_op\n",
    "    Nt = N_op\n",
    "    (x, t, S), (u, y, s) = solve_ADR(key, Nx , Nt, N_op, length_scale)\n",
    "\n",
    "    XX, TT = jnp.meshgrid(x, t)\n",
    "\n",
    "    # u_test = jnp.tile(u, (N_op**2,1))\n",
    "    u_test = u\n",
    "    num_test = jnp.tile(index, (N_op**2, 1))\n",
    "    y_test = jnp.hstack([XX.flatten()[:,None], TT.flatten()[:,None]])\n",
    "    s_test = S.T.flatten()\n",
    "\n",
    "    return u_test, y_test, s_test, num_test\n",
    "\n",
    "# Geneate test data corresponding to one input sample\n",
    "def generate_one_test_data(key, N_op):\n",
    "    Nx = N_op\n",
    "    Nt = N_op\n",
    "    (x, t, S), (u, y, s) = solve_ADR(key, Nx , Nt, N_op, length_scale)\n",
    "\n",
    "    XX, TT = jnp.meshgrid(x, t)\n",
    "\n",
    "    u_test = jnp.tile(u, (N_op**2,1))\n",
    "    y_test = jnp.hstack([XX.flatten()[:,None], TT.flatten()[:,None]])\n",
    "    s_test = S.T.flatten()\n",
    "\n",
    "    return u_test, y_test, s_test\n",
    "\n",
    "# Geneate training data corresponding to N input sample\n",
    "def generate_training_data_efficient(key, N, N_op, N_pde, N_ics, N_bcs):\n",
    "    config.update(\"jax_enable_x64\", True)\n",
    "    keys = random.split(key, N)\n",
    "    indices = jnp.arange(N)\n",
    "    train_op, train_pde, train_bcs, train_ics = vmap(generate_one_training_data_efficient, (0, 0, None, None, None, None))(keys, indices, N_op, N_pde, N_bcs, N_ics)\n",
    "    \n",
    "    u_op, y_op, s_op, num_op = train_op\n",
    "    u_pde, y_pde, s_pde, num_pde = train_pde\n",
    "    u_bcs, y_bcs, s_bcs, num_bcs = train_bcs\n",
    "    u_ics, y_ics, s_ics, num_ics = train_ics\n",
    "\n",
    "    u_op = u_op.reshape(N,-1)\n",
    "    y_op = y_op.reshape(N * N_op,-1)\n",
    "    s_op = s_op.reshape(N * N_op,-1)\n",
    "    num_op = num_op.reshape(N * N_op, -1)\n",
    "\n",
    "    u_pde = u_pde.reshape(N,-1)\n",
    "    y_pde = y_pde.reshape(N * N_pde,-1)\n",
    "    s_pde = s_pde.reshape(N * N_pde,-1)\n",
    "    num_pde = num_pde.reshape(N * N_pde, -1)\n",
    "\n",
    "    u_bcs = u_bcs.reshape(N,-1)\n",
    "    y_bcs = y_bcs.reshape(N * N_bcs,-1)\n",
    "    s_bcs = s_bcs.reshape(N * N_bcs,-1)\n",
    "    num_bcs = num_bcs.reshape(N * N_bcs, -1)\n",
    "\n",
    "    u_ics = u_ics.reshape(N,-1)\n",
    "    y_ics = y_ics.reshape(N * N_ics,-1)\n",
    "    s_ics = s_ics.reshape(N * N_ics,-1)\n",
    "    num_ics = num_ics.reshape(N * N_ics, -1)\n",
    "    \n",
    "    config.update(\"jax_enable_x64\", False)\n",
    "    \n",
    "    # Shorthand tuples to pass fewer outputs\n",
    "    train_op = (u_op, y_op, s_op, num_op)\n",
    "    train_pde = (u_pde, y_pde, s_pde, num_pde)\n",
    "    train_bcs = (u_bcs, y_bcs, s_bcs, num_bcs)\n",
    "    train_ics = (u_ics, y_ics, s_ics, num_ics)\n",
    "\n",
    "\n",
    "    return train_op, train_pde, train_bcs, train_ics\n",
    "\n",
    "# Geneate test data corresponding to N input sample\n",
    "def generate_test_data_efficient(key, N, N_op):\n",
    "\n",
    "    config.update(\"jax_enable_x64\", True)\n",
    "    keys = random.split(key, N)\n",
    "    indices = jnp.arange(N)\n",
    "\n",
    "    u_test, y_test, s_test, num_test = vmap(generate_one_test_data_efficient, (0, 0, None))(keys, indices, N_op)\n",
    "\n",
    "    # u_test = jnp.float32(u_test.reshape(N * N_op**2,-1))\n",
    "    u_test = jnp.float32(u_test.reshape(N,-1))\n",
    "    num_test = jnp.float32(num_test.reshape(N * N_op**2,-1))\n",
    "    y_test = jnp.float32(y_test.reshape(N * N_op**2,-1))\n",
    "    s_test = jnp.float32(s_test.reshape(N * N_op**2,-1))\n",
    "\n",
    "    config.update(\"jax_enable_x64\", False)\n",
    "    return u_test, y_test, s_test, num_test\n",
    "\n",
    "# Geneate test data corresponding to N input sample\n",
    "def generate_test_data(key, N, N_op):\n",
    "\n",
    "    config.update(\"jax_enable_x64\", True)\n",
    "    keys = random.split(key, N)\n",
    "\n",
    "    u_test, y_test, s_test = vmap(generate_one_test_data, (0, None))(keys, N_op)\n",
    "\n",
    "    u_test = jnp.float32(u_test.reshape(N * N_op**2,-1))\n",
    "    y_test = jnp.float32(y_test.reshape(N * N_op**2,-1))\n",
    "    s_test = jnp.float32(s_test.reshape(N * N_op**2,-1))\n",
    "\n",
    "    config.update(\"jax_enable_x64\", False)\n",
    "    return u_test, y_test, s_test\n",
    "\n",
    "# Compute relative l2 error over N_op test samples.\n",
    "def compute_error(key, N_op):\n",
    "    # Generate one test sample\n",
    "    u_test, y_test, s_test = generate_test_data(key, 1, N_op)\n",
    "    # Predict  \n",
    "    s_pred = model.predict_s(params, u_test, y_test)[:,None]\n",
    "    # Compute relative l2 error\n",
    "    error_s = jnp.linalg.norm(s_test - s_pred) / jnp.linalg.norm(s_test) \n",
    "    return error_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ab333",
   "metadata": {},
   "source": [
    "# Define Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "\n",
    "# GRF length scale\n",
    "length_scale = 0.1 #0.2\n",
    "\n",
    "# Resolution of the solution\n",
    "Nx = 100\n",
    "Nt = 100\n",
    "\n",
    "N = 5000 # number of input samples (different u values)\n",
    "m = Nx   # number of input sensors\n",
    "N_op_train = 100 # number data outputs per sample \n",
    "N_pde_train = 100  # number of points for PDE constrains pe sample\n",
    "N_bcs_train = 100 # number of BC points \n",
    "N_ics_train = 100 # number of IC points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279168e",
   "metadata": {},
   "source": [
    "# Plot Evolution of a Single Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(123)\n",
    "_, _, S0 = generate_test_data(key, 1, Nx)\n",
    "S0 = S0.reshape((Nx,Nt))\n",
    "x = jnp.linspace(0, 1, Nx)\n",
    "t = jnp.linspace(0, 1, Nt)\n",
    "# S = s_test.reshape(Nx,Nt)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "ax.plot(x, S0[-1])\n",
    "ylim = plt.ylim()\n",
    "xlim = [0, 1]\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(Nt):\n",
    "    ax.clear()\n",
    "    ax.plot(x, S0[i])\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "    plt.xlabel(f'$x$')\n",
    "    plt.ylabel(f'$s$')\n",
    "    plt.title(f'Diffusion Reaction Equation')\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be0b36",
   "metadata": {},
   "source": [
    "# Generate the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op, train_pde, train_bcs, train_ics = generate_training_data_efficient(key, N, N_op_train, N_pde_train, N_bcs_train, N_ics_train)\n",
    "\n",
    "u_op, y_op, s_op, num_op  = train_op\n",
    "u_pde, y_pde, s_pde, num_pde = train_pde\n",
    "u_bcs, y_bcs, s_bcs, num_bcs = train_bcs\n",
    "u_ics, y_ics, s_ics, num_ics  = train_ics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5bc67",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_layers = [m, 50, 50, 50, 50, 50]\n",
    "trunk_layers =  [2, 50, 50, 50, 50, 50]\n",
    "model = DRDeepONetPI(branch_layers, trunk_layers, branch_net=modified_MLP, trunk_net=modified_MLP, ckpt_dir='DiffusionReaction/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508a415",
   "metadata": {},
   "source": [
    "# Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.restore()\n",
    "model.restore_history()\n",
    "model.loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6372376",
   "metadata": {},
   "source": [
    "# Create data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b44eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "# op_dataset = DataGenerator(u_op, y_op, s_op, batch_size)\n",
    "# pde_dataset = DataGenerator(u_pde, y_pde, s_pde, batch_size)\n",
    "# bcs_dataset = DataGenerator(u_bcs, y_bcs, s_bcs, batch_size)\n",
    "# ics_dataset = DataGenerator(u_ics, y_ics, s_ics, batch_size)\n",
    "op_dataset = DataGeneratorEfficient(u_op, y_op, s_op, num_op, batch_size)\n",
    "pde_dataset = DataGeneratorEfficient(u_pde, y_pde, s_pde, num_pde, batch_size)\n",
    "bcs_dataset = DataGeneratorEfficient(u_bcs, y_bcs, s_bcs, num_bcs, batch_size)\n",
    "ics_dataset = DataGeneratorEfficient(u_ics, y_ics, s_ics, num_ics, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ceb35b",
   "metadata": {},
   "source": [
    "# Train then Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cdcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(op_dataset, pde_dataset, bcs_dataset, ics_dataset, nIter=35000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cc2b7",
   "metadata": {},
   "source": [
    "# Save the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c891cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save()\n",
    "model.save_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162e0ed",
   "metadata": {},
   "source": [
    "# Generate and Evaluate Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83914041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e33900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of samples and RNG\n",
    "N_test = 100 # number of input samples \n",
    "m_test = m   # number of sensors \n",
    "key_test = random.PRNGKey(1234567)\n",
    "keys_test = random.split(key_test, N_test)\n",
    "\n",
    "# Predict\n",
    "params = model.get_params(model.opt_state)\n",
    "\n",
    "# Compute error\n",
    "error_s = vmap(compute_error, (0, None))(keys_test, m_test) \n",
    "print('mean of relative L2 error of s: {:.2e}'.format(error_s.mean()))\n",
    "print('std of relative L2 error of s: {:.2e}'.format(error_s.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81de111",
   "metadata": {},
   "source": [
    "# Plot for loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,5))\n",
    "loss_operator_log = jnp.array(model.loss_operator_log)\n",
    "loss_physics_log = jnp.array(model.loss_physics_log)\n",
    "loss_bcs_log = jnp.array(model.loss_bcs_log)\n",
    "loss_ics_log = jnp.array(model.loss_ics_log)\n",
    "loss_log = jnp.array(model.loss_log)\n",
    "\n",
    "plt.plot(loss_operator_log[:,0], loss_operator_log[:,1], 'b-', lw=2, label='operator')\n",
    "plt.plot(loss_physics_log[:,0], loss_physics_log[:,1], 'r-', lw=2, label='physics')\n",
    "plt.plot(loss_bcs_log[:,0], loss_bcs_log[:,1], 'y-', lw=2, label='bcs')\n",
    "plt.plot(loss_ics_log[:,0], loss_ics_log[:,1], 'c-', lw=2, label='ics')\n",
    "plt.plot(loss_log[:,0], loss_log[:,1], 'k-', lw=2, label='total loss')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662765c",
   "metadata": {},
   "source": [
    "# Generate one test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(12345)\n",
    "# key = random.PRNGKey(112312347)\n",
    "P_test = 100\n",
    "Nx = m\n",
    "u_test, y_test, s_test = generate_test_data(key, 1, P_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48f697",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.get_params(model.opt_state)\n",
    "s_pred = model.predict_s(params, u_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa85fb",
   "metadata": {},
   "source": [
    "# Compute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an uniform mesh\n",
    "x = jnp.linspace(0, 1, Nx)\n",
    "t = jnp.linspace(0, 1, Nt)\n",
    "XX, TT = jnp.meshgrid(x, t)\n",
    "\n",
    "# Grid data\n",
    "S_pred = griddata(y_test, s_pred.flatten(), (XX,TT), method='cubic')\n",
    "S_test = griddata(y_test, s_test.flatten(), (XX,TT), method='cubic')\n",
    "\n",
    "# Compute the relative l2 error \n",
    "error = jnp.linalg.norm(S_pred - S_test, 2) / jnp.linalg.norm(S_test, 2) \n",
    "print('Relative l2 error: {:.3e}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8cb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = random.PRNGKey(123)\n",
    "# _, _, S0 = generate_test_data(key, 1, Nx)\n",
    "# S0 = S0.reshape((Nx,Nt))\n",
    "# x = jnp.linspace(0, 1, Nx)\n",
    "# t = jnp.linspace(0, 1, Nt)\n",
    "# S = s_test.reshape(Nx,Nt)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "ax.plot(x, S_test[-1], 'b-', label='Exact')\n",
    "ax.plot(x, S_pred[-1], 'r--', label='DeepONetPI Prediction')\n",
    "ylim = plt.ylim()\n",
    "xlim = [0, 1]\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(Nt):\n",
    "    ax.clear()\n",
    "    ax.plot(x, S_test[i], 'b-', label='Exact')\n",
    "    ax.plot(x, S_pred[i], 'r--', label='DeepONetPI Prediction')\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "    plt.xlabel(f'$x$')\n",
    "    plt.ylabel(f'$s$')\n",
    "    plt.title(f'Diffusion Reaction Equation')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7712cc5",
   "metadata": {},
   "source": [
    "# Plot Predictions and True Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "plt.subplot(1,3,1)\n",
    "# plt.pcolor(XX,TT, S_test, cmap='jet')\n",
    "plt.pcolormesh(XX,TT, S_test, cmap='jet', shading='gouraud')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$t$')\n",
    "plt.title('Exact $s(x,t)$')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "# plt.pcolor(XX,TT, S_pred, cmap='jet')\n",
    "plt.pcolormesh(XX,TT, S_pred, cmap='jet', shading='gouraud')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$t$')\n",
    "plt.title('Predict $s(x,t)$')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "# plt.pcolor(XX,TT, S_pred - S_test, cmap='jet')\n",
    "plt.pcolormesh(XX,TT, S_pred - S_test, cmap='jet', shading='gouraud')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$t$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(key, S_test, Nx=100, Nt=100, save_path=None):\n",
    "#     if print_index:\n",
    "#         display(sind)\n",
    "    u_test, y_test, s_test = generate_test_data(key, 1, Nx)\n",
    "    params = model.get_params(model.opt_state)\n",
    "    s_pred = model.predict_s(params, u_test, y_test)\n",
    "#     s_test_ = S_test[sind]\n",
    "\n",
    "    # Predict\n",
    "    params = model.get_params(model.opt_state)\n",
    "    s_pred = model.predict_s(params, u_test, y_test)\n",
    "\n",
    "    # Generate an uniform mesh\n",
    "    x = jnp.linspace(0, 1, Nx)\n",
    "    t = jnp.linspace(0, 1, Nt)\n",
    "    XX, TT = jnp.meshgrid(x, t)\n",
    "\n",
    "    # Grid data\n",
    "    # S_pred = griddata(y_test, s_pred.flatten(), (XX,TT), method='cubic')\n",
    "    # S_true = griddata(y_test, s_test.flatten(), (XX,TT), method='cubic')\n",
    "    # S_true = s_test_\n",
    "    S_true = s_test.reshape(Nx,Nt)\n",
    "    S_pred = s_pred.reshape(Nx,Nt)\n",
    "    u = u_test[0]\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(23,5))\n",
    "\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.plot(x, u)\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$u$')\n",
    "    plt.title('Source Term $u(x)$')\n",
    "    plt.xlim([0,1])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.pcolormesh(XX,TT, S_true, cmap='jet', shading='gouraud')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$t$')\n",
    "    plt.title('Exact $s(x,t)$')\n",
    "    plt.tight_layout()\n",
    "    plt.axis('square')\n",
    "\n",
    "\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.pcolormesh(XX,TT, S_pred, cmap='jet', shading='gouraud')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$t$')\n",
    "    plt.title('Predict $s(x,t)$')\n",
    "    plt.tight_layout()\n",
    "    plt.axis('square')\n",
    "\n",
    "    plt.subplot(1,4,4)\n",
    "    # plt.pcolor(XX,TT, S_pred - S_test, cmap='jet')\n",
    "    plt.pcolormesh(XX,TT, S_pred - S_true, cmap='jet', shading='gouraud')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$t$')\n",
    "    plt.title('Absolute error')\n",
    "    plt.tight_layout()\n",
    "    plt.axis('square')\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(f'{save_path}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "figures_dir = 'DiffusionReaction/figures/'\n",
    "os.makedirs(figures_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 5\n",
    "key = random.PRNGKey(0)\n",
    "keys = random.split(key, num_plots)\n",
    "for i, subkey in enumerate(keys):\n",
    "    save_path = os.path.join(figures_dir, f'DiffusionReaction{i}')\n",
    "#     plot_predictions(subkey, S_test, Nx=100, Nt=100, save_path=save_path)\n",
    "    plot_predictions(subkey, S_test, Nx=100, Nt=100, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab71ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
