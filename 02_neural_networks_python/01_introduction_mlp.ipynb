{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits classification with MLPs, by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Tanwi Mallick adapting notebooks by Bethany Lusch, Prasanna Balaprakash and Taylor Childers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll train a multi-layer perceptron model (multi-layer neural network) to classify MNIST digits. We'll build up the code by hand. In the next notebook, we show how this can be done using existing libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load the MNIST handwritten digits data set. First time we may have to download the data, which can take a while.\n",
    "\n",
    "<img src=\"images/MnistExamples.png\"  align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(numpy.float32)\n",
    "x_test  = x_test.astype(numpy.float32)\n",
    "\n",
    "x_train /= 255.\n",
    "x_test  /= 255.\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data (x_train) is a matrix of size (60000, 28*28), i.e. it consists of 60000 images that were each of size 28x28 pixels but are now flattened into vectors. y_train is a 60000-dimensional vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], numpy.prod(x_train[0,:,:].shape))\n",
    "x_test = x_test.reshape(x_test.shape[0], numpy.prod(x_test[0,:,:].shape))\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "y_train = y_train.astype(numpy.int32)\n",
    "y_test  = y_test.astype(numpy.int32)\n",
    "\n",
    "print()\n",
    "print('MNIST data loaded: train:',len(x_train),'test:',len(x_test))\n",
    "print('X_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look. Here are the first 10 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(numpy.reshape(x_train[i,:], (28, 28)), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a simple linear model: linear regression, like in the previous module. \n",
    "We add these extras:\n",
    "- Each example is a vector (flattened image), so the \"slope\" multiplication becomes a dot product.\n",
    "- We consider multiple examples at once. input_images is a matrix where each row is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(W, input_images):\n",
    "    # f(x) = xW returns m-length vector, where m is the number of examples\n",
    "    return numpy.dot(input_images, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y - \\hat{y})^{2}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(W, input_images, true_labels):\n",
    "    predicted_label = linear_model(W, input_images)\n",
    "    MSE = numpy.mean((true_labels - predicted_label)**2) \n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update weights using gradient decent \n",
    "$\\large{W = W - \\eta \\frac{\\partial J(W)}{\\partial W} }$,  where, $W$ is the network weight, $\\eta$ is the learning rate and $J(W)$ is the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(input_images, true_labels, current_W, learning_rate=0.000001):\n",
    "    # first we need dJW/dW where JW = MSE \n",
    "    n = input_images.shape[0] # get number of examples to average over\n",
    "    label_predictions = linear_model(current_W, input_images)\n",
    "    # calculate gradient: one entry per partial derivative for an entry in vector W\n",
    "    dJW_dW = (2./n) * numpy.dot(input_images.transpose(), label_predictions - true_labels)\n",
    "    # now we update W\n",
    "    new_W = current_W - (learning_rate * dJW_dW)  # gradient update step\n",
    "    return new_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we usually don't use all of the training data to calculate each step. We use a random subset. This makes the steps faster and noisier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the moment we take the simple route and use a fixed subset. \n",
    "batch_size = 100\n",
    "\n",
    "x_train_batch = x_train[:batch_size, :]\n",
    "y_train_batch = y_train[:batch_size,numpy.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = x_train.shape[1]\n",
    "W = .01 * numpy.random.rand(num_features,1)\n",
    "learning_rate = 0.0005  \n",
    "\n",
    "num_iters = 5000\n",
    "losses = numpy.zeros(num_iters,)\n",
    "\n",
    "for i in range(0, num_iters):\n",
    "    # all the magic here\n",
    "    W = learn(x_train_batch, y_train_batch, W, learning_rate)\n",
    "    losses[i] = evaluate(W, x_train_batch, y_train_batch)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this simple linear model f(x) = xW is not very accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "predicted_labels = linear_model(W, x_train[:10,:])\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(numpy.reshape(x_train[i,:], (28, 28)), cmap=\"gray\")\n",
    "    plt.title('%1.2f' % predicted_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of basic ways to improve:\n",
    "- Add bias term: f(x) = xW + b\n",
    "- Reformulate as classification (output integers, not real numbers), like logistic regression\n",
    "- Minimize something other than mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    z = 1/(1 + numpy.exp(-x))\n",
    "    return(z)\n",
    "\n",
    "def classification_model(A, b, input_images):\n",
    "    # f(x) = sigmoid(xA + b) returns m-length vector, where m is the number of examples\n",
    "    return sigmoid(numpy.dot(input_images, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function encourages outputs of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.arange(-10, 10, step=.1)\n",
    "plt.plot(x, sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle multiple classes, it's common to use a one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding:\n",
    "nb_classes = 10\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(numpy.reshape(x_train[i,:], (28, 28)), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(y_train[i]))\n",
    "    print('Training sample',i,': class:',y_train[i], ', one-hot encoded:', y_train_onehot[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To hand multi-class classification, it's common to use softmax instead of sigmoid. It's related but forces the outputs to sum to 1, like a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for multi-class classification problem, it is common to minimize a different \"loss\" function instead of mean squared error, like categorical cross-entropy. You can read more [here](https://gombru.github.io/2018/05/23/cross_entropy_loss/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above improvements are not enough to classify these images. We move to a nonlinear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network has multiple layers. A basic layer is $\\sigma(xA + b)$. An example neural network with two layers adds another affine transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\sigma(\\sigma(xW_1 + b_1)W_2 + b_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is often drawn as a network like this:\n",
    "\n",
    "\n",
    "<img src=\"images/tiny_network.png\" width=\"300\" hight=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing it in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_model(W1, W2, b1, b2, input_images):\n",
    "    return sigmoid(numpy.dot(sigmoid(numpy.dot(input_images, W1) + b1), W2) + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another layer:\n",
    "\n",
    "$f(x) = \\sigma(\\sigma(\\sigma(xW_1 + b_1)W_2 + b_2)W_3 + b_3)$\n",
    "\n",
    "\n",
    "<img src=\"images/three_layer_network.png\" width=\"300\" hight=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_model(A1, A2, A3, b1, b2, b3, input_images):\n",
    "    return sigmoid(numpy.dot(sigmoid(numpy.dot(sigmoid(numpy.dot(input_images, A1) + b1), A2) + b2), A3) + b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, deep learning training can be tricky. \n",
    "\n",
    "Unlike linear regression, the objective function that you're minimizing (some measure of error) is non-convex, so there can be many local optima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multiple local minima](images/nonconvex.png)\n",
    "\n",
    "Image source: [firsttimeprogrammer.blogspot.com](http://firsttimeprogrammer.blogspot.com/2014/09/multivariable-gradient-descent.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
    "    - $\\large{W = W - \\eta \\frac{\\partial J(W)}{\\partial W} }$\n",
    "\n",
    "    - A high learning rate can overshoot, causing the system to become unstable and diverge.\n",
    "    - A low learning rate converges slowly and becomes stuck in false local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image source: [saugatbhattarai](https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/learning-rate-gradient-descent/)\n",
    "<img src=\"images/learning-rate-gradient-descent.jpeg\" width=\"500\" hight=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent optimization algorithms \n",
    "- SGD\n",
    "- Adam \n",
    "- RMSprop\n",
    "- Adadelta\n",
    "- Adagrad\n",
    "\n",
    "Image source: [Sebastian Ruder](https://ruder.io/optimizing-gradient-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Neural networks can be overly flexible/complicated and \"overfit\" your data. This is like what happens if you fit a high-degree polynomial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/bias_vs_variance.png\" width=\"800\" hight=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To improve the generalization of our model on previously unseen data, we employ a technique known as regularization, which constrains our optimization problem in order to discourage complex models. Dropout is the commonly used regularization technique\n",
    "\n",
    "    - The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dropout.png\" width=\"600\" hight=\"600\" align=\"left\"/>\n",
    "\n",
    "\n",
    "Image source: [Amine ben khalifa](https://www.researchgate.net/figure/Dropout-neural-network-model-a-is-a-standard-neural-network-b-is-the-same-network_fig3_309206911)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/test_data_rule.png\" width=\"800\" hight=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the basics, we can experiment with more complicated networks. Rather than implement these all by hand, we will move to using existing Python packages in the next notebook. To continue by hand, check out [this blog post](https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
