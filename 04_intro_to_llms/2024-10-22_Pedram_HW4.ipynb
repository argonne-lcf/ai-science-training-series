{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Text Generation Parameters with HuggingFace Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how different generative parameters affect text generation using HuggingFace's `pipeline`. We will experiment with temperature, `max_new_tokens`, and different models to observe their effect on the output text's creativity, coherence, and legibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a default model (using GPT-2 here, as it’s smaller for initial runs)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Text Generation (Default Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of AI is\"\n",
    "output = generator(prompt, max_new_tokens=50)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 1: Increasing Temperature (0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(prompt, max_new_tokens=50, temperature=0.9)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 2: Higher Temperature (1.2) and Fewer Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(\"In a world where robots have taken over\", max_new_tokens=20, temperature=1.2)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 3: Switching to Another Model (`microsoft/Phi-3-mini-4k-instruct`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a different model\n",
    "generator = pipeline(\"text-generation\", model=\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Generate text with the new model and adjusted temperature\n",
    "output = generator(\"The future of AI is\", max_new_tokens=50, temperature=0.7)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explored how adjusting the temperature, `max_new_tokens`, and the model impacts the legibility and creativity of text generation. Here’s a quick summary:\n",
    "\n",
    "- **Lower temperatures (e.g., 0.7)** tend to produce more predictable, coherent outputs.\n",
    "- **Higher temperatures (1.0–1.2)** introduce more randomness, leading to creative but sometimes less coherent text.\n",
    "- **Different models** provide varied outputs, with some focusing on more factual, instructional content.\n",
    "\n",
    "By fine-tuning these parameters, you can adjust the output to better fit your specific needs, whether that’s creativity, factual accuracy, or coherence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
