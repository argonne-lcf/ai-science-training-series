Below is the combined .ipynb content that merges the two previous notebooks: the text generation using multiple models with different parameters, and the attention analysis using BertViz for two models of different sizes.

You can save this into a single .ipynb file, such as text_generation_and_attention_analysis.ipynb.

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation and Attention Mechanism Analysis\n",
    "\n",
    "In this notebook, we'll cover two tasks:\n",
    "1. **Text Generation** using multiple models with different parameters like temperature and max tokens.\n",
    "2. **Attention Visualization** using BertViz for models of different sizes to analyze how their attention mechanisms differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text Generation with HuggingFace Models\n",
    "We'll experiment with different models and generation parameters, including temperature and max tokens, to see how they affect the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models and Tokenizers\n",
    "We'll use a few models for text generation and tweak generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load GPT-2\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load LLaMA-2-7B\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text with Different Parameters\n",
    "We'll use the GPT-2 model and experiment with the following generation parameters:\n",
    "- `temperature`\n",
    "- `max_new_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate text\n",
    "def generate_text(model, tokenizer, prompt, temperature=1.0, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the function with GPT-2 and different parameters\n",
    "prompt = \"The future of AI is\"\n",
    "gpt2_output1 = generate_text(gpt2_model, gpt2_tokenizer, prompt, temperature=0.7)\n",
    "gpt2_output2 = generate_text(gpt2_model, gpt2_tokenizer, prompt, temperature=1.5, max_new_tokens=100)\n",
    "\n",
    "print(\"Output with temperature 0.7:\")\n",
    "print(gpt2_output1)\n",
    "print(\"\nOutput with temperature 1.5:\")\n",
    "print(gpt2_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll generate text using the LLaMA-2 model with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with LLaMA-2 and different parameters\n",
    "llama_output1 = generate_text(llama_model, llama_tokenizer, prompt, temperature=0.8)\n",
    "llama_output2 = generate_text(llama_model, llama_tokenizer, prompt, temperature=1.2, max_new_tokens=100)\n",
    "\n",
    "print(\"LLaMA-2 output with temperature 0.8:\")\n",
    "print(llama_output1)\n",
    "print(\"\nLLaMA-2 output with temperature 1.2:\")\n",
    "print(llama_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Attention Mechanism Analysis with BertViz\n",
    "We'll use BertViz to analyze how the attention mechanisms differ between a smaller model (GPT-2) and a larger model (LLaMA-2-7B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Models\n",
    "We'll load GPT-2 and LLaMA-2 for the attention visualization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the smaller model (GPT-2)\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "small_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "\n",
    "# Load the larger model (LLaMA-2-7B)\n",
    "large_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "large_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Input for Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The future of AI holds immense potential.\"\n",
    "\n",
    "# Tokenize for GPT-2\n",
    "small_inputs = small_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize for LLaMA-2\n",
    "large_inputs = large_tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Outputs with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention from GPT-2\n",
    "small_outputs = small_model(**small_inputs)\n",
    "\n",
    "# Get attention from LLaMA-2\n",
    "large_outputs = large_model(**large_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attention using BertViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view\n",
    "\n",
    "# GPT-2 visualization\n",
    "head_view(small_outputs.attentions, small_inputs.input_ids, small_tokenizer)\n",
    "\n",
    "# LLaMA-2 visualization\n",
    "head_view(large_outputs.attentions, large_inputs.input_ids, large_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Attention Mechanisms\n",
    "Now that we have visualized the attention heads of both models, let's analyze the differences:\n",
    "\n",
    "- **GPT-2 (Small model)**: With fewer parameters, GPT-2 has fewer attention heads. These heads tend to focus on a limited context, making the model more likely to attend to recent tokens or single-word relationships.\n",
    "\n",
    "- **LLaMA-2-7B (Large model)**: The larger model has more attention heads, which are capable of attending to a broader context, allowing it
