{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJJ8sjMZf2rk"
      },
      "source": [
        "# HW4: Text Generation and Attention Mechanism Analysis\n",
        "\n",
        "This homework has two section:\n",
        "1. **Text Generation:** using multiple models with different parameters like temperature and max tokens.\n",
        "2. **Understanding Attention Mechanisms:** using BertViz for models of different sizes to analyze how their attention mechanisms differ.\n"
      ],
      "id": "tJJ8sjMZf2rk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9gyzVGqf2rl"
      },
      "source": [
        "## Part 1: Text Generation with HuggingFace Models\n",
        "We'll experiment with different models and generation parameters, including temperature and max tokens, to see how they affect the model's responses.\n"
      ],
      "id": "c9gyzVGqf2rl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwqCHc-Hf2rl",
        "outputId": "b00ea0d4-51c8-4d1e-d05f-97d19dbfd0a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n",
        "!pip install huggingface_hub"
      ],
      "id": "rwqCHc-Hf2rl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwDyZD8Jf2rl"
      },
      "source": [
        "### Load Models and Tokenizers\n",
        "We'll use a few models for text generation and tweak generation parameters."
      ],
      "id": "WwDyZD8Jf2rl"
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n"
      ],
      "metadata": {
        "id": "X0d1SJBhhrIl"
      },
      "id": "X0d1SJBhhrIl",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Swbe8Wrjf2rl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load GPT-2\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n"
      ],
      "id": "Swbe8Wrjf2rl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBY_MbnHf2rm"
      },
      "source": [
        "### Generate Text with Different Parameters\n",
        "We'll use the GPT-2 model and experiment with the following generation parameters:\n",
        "- `temperature`\n",
        "- `max_new_tokens`"
      ],
      "id": "tBY_MbnHf2rm"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l4bT6vkcf2rm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49cc7e00-d65a-47d0-cb12-e2e237734b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output with temperature 0.7:\n",
            "Fall in Chicago is the newest chapter in the history of the Chicago Cubs. The Cubs have had two World Series titles in their past two years, but the team recently lost its first playoff game. The Cubs are currently in the NL Wild Card hunt, and while it's\n",
            "Output with temperature 1.5:\n",
            "Fall in Chicago is the first installment in her current series I am not saying it because to be honest how I felt is entirely out the window due to getting my body over with (but we were just supposed to be doing business this weekend and did not feel like I was out doing business or anything with our brand and no, none of this has been coming around now at all with the way they handle some of these projects and our new video game franchise seems very excited about me trying to start making my own next one).\n"
          ]
        }
      ],
      "source": [
        "# Define a function to generate text\n",
        "def generate_text(model, tokenizer, prompt, temperature=1.0, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        temperature=temperature,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Test the function with GPT-2 and different parameters\n",
        "prompt = \"Fall in Chicago is\"\n",
        "gpt2_output1 = generate_text(gpt2_model, gpt2_tokenizer, prompt, temperature=0.7)\n",
        "gpt2_output2 = generate_text(gpt2_model, gpt2_tokenizer, prompt, temperature=1.5, max_new_tokens=100)\n",
        "\n",
        "print(\"Output with temperature 0.7:\")\n",
        "print(gpt2_output1)\n",
        "print(\"Output with temperature 1.5:\")\n",
        "print(gpt2_output2)"
      ],
      "id": "l4bT6vkcf2rm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7LKzkeVf2rm"
      },
      "source": [
        "Now, we'll generate text using the LLaMA-2 model with different parameters."
      ],
      "id": "J7LKzkeVf2rm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_eOHZf-f2rm"
      },
      "source": [
        "## Part 2: Attention Mechanism Analysis with BertViz\n",
        "We'll use BertViz to analyze how the attention mechanisms differ between a smaller model (GPT-2) and a larger model (LLaMA-2-7B)."
      ],
      "id": "F_eOHZf-f2rm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01mDDfzNf2rm"
      },
      "outputs": [],
      "source": [
        "!pip install bertviz"
      ],
      "id": "01mDDfzNf2rm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_us5Eg0ff2rm"
      },
      "source": [
        "### Load the Models\n",
        "We'll load GPT-2 and LLaMA-2 for the attention visualization task."
      ],
      "id": "_us5Eg0ff2rm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuUwnTLXf2rm"
      },
      "outputs": [],
      "source": [
        "# Load the smaller model (GPT-2)\n",
        "small_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "small_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_attentions=True)\n",
        "\n",
        "# Load the larger model (LLaMA-2-7B)\n",
        "large_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "large_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", output_attentions=True)"
      ],
      "id": "iuUwnTLXf2rm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lloAJJmf2rm"
      },
      "source": [
        "### Tokenize Input for Both Models"
      ],
      "id": "_lloAJJmf2rm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z26Di0otf2rm"
      },
      "outputs": [],
      "source": [
        "text = \"The future of AI holds immense potential.\"\n",
        "\n",
        "# Tokenize for GPT-2\n",
        "small_inputs = small_tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize for LLaMA-2\n",
        "large_inputs = large_tokenizer(text, return_tensors=\"pt\")"
      ],
      "id": "z26Di0otf2rm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHP5Fhenf2rm"
      },
      "source": [
        "### Get the Outputs with Attention"
      ],
      "id": "EHP5Fhenf2rm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te49ZZQhf2rm"
      },
      "outputs": [],
      "source": [
        "# Get attention from GPT-2\n",
        "small_outputs = small_model(**small_inputs)\n",
        "\n",
        "# Get attention from LLaMA-2\n",
        "large_outputs = large_model(**large_inputs)"
      ],
      "id": "Te49ZZQhf2rm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti_YB-umf2rm"
      },
      "source": [
        "### Visualize Attention using BertViz"
      ],
      "id": "Ti_YB-umf2rm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5C2_r_Ef2rm"
      },
      "outputs": [],
      "source": [
        "from bertviz import head_view\n",
        "\n",
        "# GPT-2 visualization\n",
        "head_view(small_outputs.attentions, small_inputs.input_ids, small_tokenizer)\n",
        "\n",
        "# LLaMA-2 visualization\n",
        "head_view(large_outputs.attentions, large_inputs.input_ids, large_tokenizer)"
      ],
      "id": "a5C2_r_Ef2rm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHtRk6Ijf2rn"
      },
      "source": [
        "## Analysis of Attention Mechanisms\n",
        "Now that we have visualized the attention heads of both models, let's analyze the differences:\n",
        "\n",
        "- **GPT-2 (Small model)**: With fewer parameters, GPT-2 has fewer attention heads. These heads tend to focus on a limited context, making the model more likely to attend to recent tokens or single-word relationships.\n",
        "- **LLaMA-2-7B (Large model)**: With a larger number of parameters, LLaMA-2 has more attention heads, which can focus on both local context and long-range dependencies in the text. This leads to richer attention distributions."
      ],
      "id": "sHtRk6Ijf2rn"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}