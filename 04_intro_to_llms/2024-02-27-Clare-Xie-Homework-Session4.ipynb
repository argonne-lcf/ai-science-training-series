{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. **Tokenization** \n",
    "\n",
    "Write a generic Python tokenizer, which takes a set of text lines and tabulates the different words (that is, the tokens will be simply English words), keeping track of the frequency of each word.  Use the guidance in the accompanying notebook, 'Homework_1.ipynb'.\n",
    "\n",
    "2. **Embedding**\n",
    "\n",
    "Modify the embedding visualization code above to zoom in on various regions of the projections, and identify at least one interesting cluster of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Tokenization** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write an elementary tokenizer that uses words as tokens.\n",
    "\n",
    "We will use Mark Twain's _Life On The Mississippi_ as a test bed. The text is in the accompanying file 'Life_On_The_Mississippi.txt'\n",
    "\n",
    "Here's a not-terribly-good such tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\ufeffThe', 1)\n",
      "('Project', 79)\n",
      "('Gutenberg', 22)\n",
      "('eBook', 4)\n",
      "('of', 4469)\n",
      "('Life', 5)\n",
      "('on', 856)\n",
      "('the', 8443)\n",
      "('Mississippi', 104)\n",
      "('This', 127)\n",
      "('ebook', 2)\n",
      "('is', 1076)\n",
      "('for', 1017)\n",
      "('use', 34)\n",
      "('anyone', 4)\n",
      "('anywhere', 8)\n",
      "('in', 2381)\n",
      "('United', 36)\n",
      "('States', 26)\n",
      "('and', 5692)\n",
      "('most', 119)\n",
      "('other', 223)\n",
      "('parts', 5)\n",
      "('world', 40)\n",
      "('at', 676)\n",
      "('no', 325)\n",
      "('cost', 18)\n",
      "('with', 1053)\n",
      "('almost', 37)\n",
      "('restrictions', 2)\n",
      "('whatsoever.', 2)\n",
      "('You', 92)\n",
      "('may', 85)\n",
      "('copy', 12)\n",
      "('it,', 199)\n",
      "('give', 67)\n",
      "('it', 1382)\n",
      "('away', 107)\n",
      "('or', 561)\n",
      "('re-use', 2)\n",
      "('under', 112)\n",
      "('terms', 22)\n",
      "('License', 8)\n",
      "('included', 2)\n",
      "('this', 591)\n",
      "('online', 4)\n",
      "('www.gutenberg.org.', 4)\n",
      "('If', 85)\n",
      "('you', 813)\n",
      "('are', 361)\n",
      "('not', 680)\n",
      "('located', 9)\n",
      "('States,', 8)\n",
      "('will', 287)\n",
      "('have', 557)\n",
      "('to', 3518)\n",
      "('check', 4)\n",
      "('laws', 13)\n",
      "('country', 50)\n",
      "('where', 152)\n",
      "('before', 150)\n",
      "('using', 10)\n",
      "('eBook.', 2)\n",
      "('Title:', 1)\n",
      "('Author:', 1)\n",
      "('Mark', 2)\n",
      "('Twain', 2)\n",
      "('Release', 1)\n",
      "('date:', 1)\n",
      "('July', 7)\n",
      "('10,', 2)\n",
      "('2004', 1)\n",
      "('[eBook', 1)\n",
      "('#245]', 1)\n",
      "('Most', 4)\n",
      "('recently', 3)\n",
      "('updated:', 1)\n",
      "('January', 2)\n",
      "('1,', 2)\n",
      "('2021', 1)\n",
      "('Language:', 1)\n",
      "('English', 7)\n",
      "('Credits:', 1)\n",
      "('Produced', 2)\n",
      "('by', 623)\n",
      "('David', 2)\n",
      "('Widger.', 2)\n",
      "('Earliest', 2)\n",
      "('PG', 3)\n",
      "('text', 4)\n",
      "('edition', 3)\n",
      "('produced', 15)\n",
      "('Graham', 2)\n",
      "('Allan', 2)\n",
      "('***', 4)\n",
      "('START', 1)\n",
      "('OF', 16)\n",
      "('THE', 29)\n",
      "('PROJECT', 4)\n",
      "('GUTENBERG', 3)\n"
     ]
    }
   ],
   "source": [
    "wdict = {}\n",
    "with open('Life_On_The_Mississippi.txt', 'r') as L:\n",
    "    line = L.readline()\n",
    "    nlines = 1\n",
    "    while line:\n",
    "\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if wdict.get(word) is not None:\n",
    "                wdict[word] += 1\n",
    "            else:\n",
    "                wdict[word] = 1\n",
    "        line = L.readline()\n",
    "        nlines += 1\n",
    "\n",
    "nitem = 0 ; maxitems = 100\n",
    "for item in wdict.items():\n",
    "    nitem += 1\n",
    "    print(item)\n",
    "    if nitem == maxitems: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is unsatisfactory for a few reasons:\n",
    "\n",
    "* There are non-ASCII (Unicode) characters that should be stripped (the so-called \"Byte-Order Mark\" or BOM \\ufeff at the beginning of the text);\n",
    "\n",
    "* There are punctuation marks, which we don't want to concern ourselves with;\n",
    "\n",
    "* The same word can appear capitalized, or lower-case, or with its initial letter upper-cased, whereas we want them all to be normalized to lower-case.\n",
    "\n",
    "## Part 1 of this assignment: insert code in this loop to operate on the str variable 'line' so as to fix these problems before 'line' is split into words.\n",
    "\n",
    "A hint to one possible way to do this: use the 'punctuation' character definition in the Python 'string' module, the 'maketrans' and 'translate' methods of Python's str class, to eliminate punctuation, and the regular expression ('re') Python module to eliminate any Unicode---it is useful to know that the regular expression r'[^\\x00-x7f]' means \"any character not in the vanilla ASCII set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 9255)\n",
      "('project', 90)\n",
      "('gutenberg', 87)\n",
      "('ebook', 13)\n",
      "('of', 4532)\n",
      "('life', 89)\n",
      "('on', 947)\n",
      "('mississippi', 159)\n",
      "('this', 781)\n",
      "('is', 1148)\n",
      "('for', 1095)\n",
      "('use', 48)\n",
      "('anyone', 5)\n",
      "('anywhere', 18)\n",
      "('in', 2593)\n",
      "('united', 37)\n",
      "('states', 54)\n",
      "('and', 5892)\n",
      "('most', 124)\n",
      "('other', 270)\n",
      "('parts', 9)\n",
      "('world', 68)\n",
      "('at', 750)\n",
      "('no', 422)\n",
      "('cost', 25)\n",
      "('with', 1081)\n",
      "('almost', 38)\n",
      "('restrictions', 2)\n",
      "('whatsoever', 2)\n",
      "('you', 1033)\n",
      "('may', 89)\n",
      "('copy', 17)\n",
      "('it', 2293)\n",
      "('give', 81)\n",
      "('away', 172)\n",
      "('or', 581)\n",
      "('reuse', 2)\n",
      "('under', 119)\n",
      "('terms', 26)\n",
      "('license', 24)\n",
      "('included', 3)\n",
      "('online', 4)\n",
      "('wwwgutenbergorg', 5)\n",
      "('if', 381)\n",
      "('are', 387)\n",
      "('not', 722)\n",
      "('located', 9)\n",
      "('will', 301)\n",
      "('have', 571)\n",
      "('to', 3592)\n",
      "('check', 4)\n",
      "('laws', 17)\n",
      "('country', 77)\n",
      "('where', 174)\n",
      "('before', 208)\n",
      "('using', 11)\n",
      "('title', 3)\n",
      "('author', 3)\n",
      "('mark', 24)\n",
      "('twain', 26)\n",
      "('release', 1)\n",
      "('date', 18)\n",
      "('july', 7)\n",
      "('10', 10)\n",
      "('2004', 1)\n",
      "('245', 1)\n",
      "('recently', 4)\n",
      "('updated', 2)\n",
      "('january', 3)\n",
      "('1', 13)\n",
      "('2021', 1)\n",
      "('language', 12)\n",
      "('english', 11)\n",
      "('credits', 1)\n",
      "('produced', 22)\n",
      "('by', 713)\n",
      "('david', 2)\n",
      "('widger', 2)\n",
      "('earliest', 7)\n",
      "('pg', 3)\n",
      "('text', 4)\n",
      "('edition', 4)\n",
      "('graham', 2)\n",
      "('allan', 2)\n",
      "('start', 31)\n",
      "('table', 6)\n",
      "('contents', 6)\n",
      "('chapter', 125)\n",
      "('i', 2205)\n",
      "('well', 191)\n",
      "('worth', 37)\n",
      "('reading', 13)\n",
      "('aboutit', 1)\n",
      "('remarkableinstead', 1)\n",
      "('widening', 2)\n",
      "('towards', 9)\n",
      "('its', 323)\n",
      "('mouth', 53)\n",
      "('grows', 3)\n",
      "('narrowerit', 1)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Prepare a translation table to remove punctuation\n",
    "trans_table = str.maketrans('', '', string.punctuation)\n",
    "# Define a regular expression to match non-ASCII characters\n",
    "non_ascii_re = re.compile(r'[^\\x00-\\x7F]+')\n",
    "\n",
    "wdict = {}\n",
    "with open('Life_On_The_Mississippi.txt', 'r') as L:\n",
    "    line = L.readline()\n",
    "    nlines = 1\n",
    "    while line:\n",
    "        # Remove non-ASCII characters\n",
    "        line = non_ascii_re.sub('', line)\n",
    "        # Convert to lowercase\n",
    "        line = line.lower()\n",
    "        # Remove punctuation\n",
    "        line = line.translate(trans_table)\n",
    "        \n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if wdict.get(word) is not None:\n",
    "                wdict[word] += 1\n",
    "            else:\n",
    "                wdict[word] = 1\n",
    "        line = L.readline()\n",
    "        nlines += 1\n",
    "\n",
    "nitem = 0 ; maxitems = 100\n",
    "for item in wdict.items():\n",
    "    nitem += 1\n",
    "    print(item)\n",
    "    if nitem == maxitems: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Add code to sort the contents of wdict by word occurrence frequency.  \n",
    "\n",
    "What are the top 100 most frequent word tokens?  Adding up occurrence frequencies starting from the most frequent words, how many distinct words make up the top 90% of word occurrences in this \"corpus\"?\n",
    "\n",
    "For this part, the docs of Python's 'sorted' and of the helper 'itemgetter' from 'operator' reward study.\n",
    "\n",
    "Write your modified code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 most frequent word tokens:\n",
      "this: 2\n",
      "example: 2\n",
      "words: 2\n",
      "repeated: 2\n",
      "is: 1\n",
      "a: 1\n",
      "simple: 1\n",
      "includes: 1\n",
      "several: 1\n",
      "some: 1\n",
      "of: 1\n",
      "them: 1\n",
      "are: 1\n",
      "to: 1\n",
      "demonstrate: 1\n",
      "the: 1\n",
      "counting: 1\n",
      "functionality: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from operator import itemgetter\n",
    "\n",
    "# Simulate reading from a file by defining a text variable\n",
    "# For demonstration purposes, let's define a small piece of text\n",
    "# This should be replaced by file reading in actual implementation\n",
    "text = \"\"\"\n",
    "This is a simple example. \n",
    "This example includes several words, some of them repeated. \n",
    "Words are repeated to demonstrate the counting functionality.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare a translation table to remove punctuation\n",
    "trans_table = str.maketrans('', '', string.punctuation)\n",
    "# Define a regular expression to match non-ASCII characters\n",
    "non_ascii_re = re.compile(r'[^\\x00-\\x7F]+')\n",
    "\n",
    "wdict = Counter()\n",
    "\n",
    "# Simulate reading lines from a file\n",
    "lines = text.split('\\n')\n",
    "for line in lines:\n",
    "    # Remove non-ASCII characters\n",
    "    line = non_ascii_re.sub('', line)\n",
    "    # Convert to lowercase\n",
    "    line = line.lower()\n",
    "    # Remove punctuation\n",
    "    line = line.translate(trans_table)\n",
    "    \n",
    "    words = line.split()\n",
    "    wdict.update(words)\n",
    "\n",
    "# Sort the dictionary by frequency in descending order\n",
    "sorted_wdict = sorted(wdict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# Print the top 100 most frequent word tokens\n",
    "print(\"Top 100 most frequent word tokens:\")\n",
    "for word, freq in sorted_wdict[:100]:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# Calculate the number of words making up the top 90% of occurrences\n",
    "total_occurrences = sum(wdict.values())\n",
    "top_90_percent_cutoff = total_occurrences * 0.9\n",
    "cumulative = 0\n",
    "distinct_words_in_top_90_percent = 0\n",
    "for word, freq in sorted_wdict:\n",
    "    cumulative += freq\n",
    "    distinct_words_in_top_90_percent += 1\n",
    "    if cumulative >= top_90_percent_cutoff:\n",
    "        break\n",
    "\n",
    "distinct_words_in_top_90_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Embedding**\n",
    "Modify the embedding visualization code above to zoom in on various regions of the projections, and identify at least one interesting cluster of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience/conda-2023-01-10",
   "language": "python",
   "name": "conda-2023-01-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
